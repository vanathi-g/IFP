{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "COD prediction",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sDHDc_oyux8P"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4pTrEZ-ux8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "b5655f49-128e-4fb1-d882-448a1501c5e6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#reading only the columns required from dataset\n",
        "df = pd.read_csv('drive/My Drive/water-treatment.data',usecols = ['date','Q-E','PH-E','ZN-E','DQO-E','DQO-S'])\n",
        "\n",
        "print(df.head())\n",
        "#print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e03cfdf899bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#reading only the columns required from dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive/My Drive/water-treatment.data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Q-E'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'PH-E'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DBO-E'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DQO-E'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'DQO-S'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/water-treatment.data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jp7jeNzuux8d",
        "outputId": "24bc4bc1-0adc-4771-d6ba-09039e3a9ea3"
      },
      "source": [
        "#converting non-numeric columns to numeric\n",
        "temp = df.drop('date', axis = 1)\n",
        "temp = temp.apply(pd.to_numeric, errors = 'coerce')\n",
        "\n",
        "df = pd.concat([df['date'],temp],axis=1,sort=False)\n",
        "print(df.head())\n",
        "print(df.dtypes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       date      Q-E  PH-E  DBO-E  DQO-E  DQO-S\n",
            "0  D-1/3/90  44101.0   7.8    NaN  407.0   84.0\n",
            "1  D-2/3/90  39024.0   7.7    NaN  443.0   91.0\n",
            "2  D-4/3/90  32229.0   7.6    NaN  528.0  128.0\n",
            "3  D-5/3/90  35023.0   7.9  205.0  588.0  104.0\n",
            "4  D-6/3/90  36924.0   8.0  242.0  496.0  108.0\n",
            "date      object\n",
            "Q-E      float64\n",
            "PH-E     float64\n",
            "DBO-E    float64\n",
            "DQO-E    float64\n",
            "DQO-S    float64\n",
            "dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6as8IftUux8e",
        "outputId": "08cccaab-4137-45fa-8490-1b4da71fe1d9"
      },
      "source": [
        "#handling NaN values - replacing by median\n",
        "print(df.isnull().sum())\n",
        "\n",
        "cols = ['Q-E', 'PH-E', 'ZN-E', 'DQO-E', 'DQO-S']\n",
        "for col in cols:\n",
        "    m = df[col].mean()\n",
        "    df[col].fillna(m, inplace=True)\n",
        "\n",
        "print(df.isnull().sum()) #no more null values\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "date      0\n",
            "Q-E      18\n",
            "PH-E      0\n",
            "DBO-E    23\n",
            "DQO-E     6\n",
            "DQO-S    18\n",
            "dtype: int64\n",
            "date     0\n",
            "Q-E      0\n",
            "PH-E     0\n",
            "DBO-E    0\n",
            "DQO-E    0\n",
            "DQO-S    0\n",
            "dtype: int64\n",
            "       date      Q-E  PH-E       DBO-E  DQO-E  DQO-S\n",
            "0  D-1/3/90  44101.0   7.8  188.714286  407.0   84.0\n",
            "1  D-2/3/90  39024.0   7.7  188.714286  443.0   91.0\n",
            "2  D-4/3/90  32229.0   7.6  188.714286  528.0  128.0\n",
            "3  D-5/3/90  35023.0   7.9  205.000000  588.0  104.0\n",
            "4  D-6/3/90  36924.0   8.0  242.000000  496.0  108.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FhL_aCWjux8f",
        "outputId": "72c06d86-1a37-4f53-de1b-db69e97a9db3"
      },
      "source": [
        "#visualizing data\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "ax1.scatter(df['Q-E'],df['DQO-S'])\n",
        "ax1.set_title('Water treatment data')\n",
        "ax1.set_xlabel('Input flow')\n",
        "ax1.set_ylabel('Output COD')\n",
        "\n",
        "ax2.scatter(df['PH-E'],df['DQO-S'])\n",
        "ax2.set_title('Water treatment data')\n",
        "ax2.set_xlabel('Input pH')\n",
        "ax2.set_ylabel('Output COD')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/home/hduser/anaconda3/lib/python3.7/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
            "  % get_backend())\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "topcq8rHux8g"
      },
      "source": [
        "#X-y split\n",
        "data = df.drop(['date'], axis=1)\n",
        "X = data.drop(['DQO-S'], axis=1).values\n",
        "y = data['DQO-S'].values\n",
        "\n",
        "#Train-Test-Split\n",
        "X_train, X_test, y_train, y_test,  = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peFAjtrwux8g"
      },
      "source": [
        "#Scaling the data\n",
        "X_train = MinMaxScaler().fit_transform(X_train)\n",
        "X_test = MinMaxScaler().fit_transform(X_test)\n",
        "X_val = MinMaxScaler().fit_transform(X_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieRyFa0Eux8g",
        "outputId": "7e3dd6c9-27ed-4282-cfd4-29b97514876d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(353, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeInzkzwux8h"
      },
      "source": [
        "#Model-Building\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1znGN0dKux8h",
        "outputId": "b9a9d68d-a0ea-4304-c9fc-5a28b5fb7669"
      },
      "source": [
        "xmodel.fit(x=X_train, y=y_train, validation_data=(X_val, y_val), batch_size=128, epochs=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 353 samples, validate on 87 samples\n",
            "Epoch 1/5000\n",
            "353/353 [==============================] - 0s 662us/sample - loss: 8334.7494 - val_loss: 9998.5859\n",
            "Epoch 2/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 8331.6465 - val_loss: 9995.0752\n",
            "Epoch 3/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8328.5533 - val_loss: 9991.5098\n",
            "Epoch 4/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8325.4157 - val_loss: 9987.8994\n",
            "Epoch 5/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8322.1799 - val_loss: 9984.2402\n",
            "Epoch 6/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 8318.8593 - val_loss: 9980.5156\n",
            "Epoch 7/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8315.5292 - val_loss: 9976.7041\n",
            "Epoch 8/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8312.0142 - val_loss: 9972.8193\n",
            "Epoch 9/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8308.5283 - val_loss: 9968.8379\n",
            "Epoch 10/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8304.8866 - val_loss: 9964.7998\n",
            "Epoch 11/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 8301.1328 - val_loss: 9960.6914\n",
            "Epoch 12/5000\n",
            "353/353 [==============================] - 0s 46us/sample - loss: 8297.3338 - val_loss: 9956.4902\n",
            "Epoch 13/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 8293.3684 - val_loss: 9952.1982\n",
            "Epoch 14/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 8289.3295 - val_loss: 9947.8193\n",
            "Epoch 15/5000\n",
            "353/353 [==============================] - 0s 42us/sample - loss: 8285.0797 - val_loss: 9943.2998\n",
            "Epoch 16/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 8280.7463 - val_loss: 9938.6045\n",
            "Epoch 17/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 8276.2170 - val_loss: 9933.7422\n",
            "Epoch 18/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8271.6267 - val_loss: 9928.7197\n",
            "Epoch 19/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8266.9202 - val_loss: 9923.5566\n",
            "Epoch 20/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 8262.0932 - val_loss: 9918.2822\n",
            "Epoch 21/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 8257.1843 - val_loss: 9912.8936\n",
            "Epoch 22/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8252.0539 - val_loss: 9907.4004\n",
            "Epoch 23/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8246.9607 - val_loss: 9901.7715\n",
            "Epoch 24/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 8241.7077 - val_loss: 9896.0371\n",
            "Epoch 25/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 8236.3352 - val_loss: 9890.1045\n",
            "Epoch 26/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8230.9196 - val_loss: 9884.0283\n",
            "Epoch 27/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8225.4973 - val_loss: 9877.8301\n",
            "Epoch 28/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8219.8166 - val_loss: 9871.4775\n",
            "Epoch 29/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 8214.2034 - val_loss: 9865.0107\n",
            "Epoch 30/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 8208.4986 - val_loss: 9858.4150\n",
            "Epoch 31/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 8202.6917 - val_loss: 9851.7168\n",
            "Epoch 32/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8196.8774 - val_loss: 9844.8877\n",
            "Epoch 33/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8190.9398 - val_loss: 9837.9766\n",
            "Epoch 34/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8184.9068 - val_loss: 9830.9688\n",
            "Epoch 35/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 8178.8716 - val_loss: 9823.8262\n",
            "Epoch 36/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 8172.5759 - val_loss: 9816.6094\n",
            "Epoch 37/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8166.3328 - val_loss: 9809.2871\n",
            "Epoch 38/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 8159.9742 - val_loss: 9801.8721\n",
            "Epoch 39/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8153.5822 - val_loss: 9794.3594\n",
            "Epoch 40/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8146.9294 - val_loss: 9786.7949\n",
            "Epoch 41/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8140.3763 - val_loss: 9779.1152\n",
            "Epoch 42/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8133.7302 - val_loss: 9771.3271\n",
            "Epoch 43/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8126.9211 - val_loss: 9763.4453\n",
            "Epoch 44/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 8120.0709 - val_loss: 9755.4473\n",
            "Epoch 45/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8113.2165 - val_loss: 9747.3311\n",
            "Epoch 46/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8106.0944 - val_loss: 9739.1543\n",
            "Epoch 47/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 8099.0260 - val_loss: 9730.8623\n",
            "Epoch 48/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 8091.7879 - val_loss: 9722.4834\n",
            "Epoch 49/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8084.5667 - val_loss: 9713.9854\n",
            "Epoch 50/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8077.1606 - val_loss: 9705.3926\n",
            "Epoch 51/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8069.7614 - val_loss: 9696.6777\n",
            "Epoch 52/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 8062.2205 - val_loss: 9687.8682\n",
            "Epoch 53/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8054.5136 - val_loss: 9678.9785\n",
            "Epoch 54/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8046.8706 - val_loss: 9669.9600\n",
            "Epoch 55/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8039.1004 - val_loss: 9660.8359\n",
            "Epoch 56/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 8031.1997 - val_loss: 9651.6211\n",
            "Epoch 57/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 8023.2133 - val_loss: 9642.3027\n",
            "Epoch 58/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 8015.1162 - val_loss: 9632.8818\n",
            "Epoch 59/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 8006.9676 - val_loss: 9623.3438\n",
            "Epoch 60/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 7998.7214 - val_loss: 9613.6885\n",
            "Epoch 61/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7990.3958 - val_loss: 9603.9082\n",
            "Epoch 62/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7981.8965 - val_loss: 9594.0312\n",
            "Epoch 63/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 7973.3349 - val_loss: 9584.0459\n",
            "Epoch 64/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 7964.6452 - val_loss: 9573.9541\n",
            "Epoch 65/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 7955.8830 - val_loss: 9563.7393\n",
            "Epoch 66/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7947.0697 - val_loss: 9553.3848\n",
            "Epoch 67/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7938.0607 - val_loss: 9542.9199\n",
            "Epoch 68/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 7929.1018 - val_loss: 9532.3066\n",
            "Epoch 69/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 7919.9038 - val_loss: 9521.5889\n",
            "Epoch 70/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7910.5997 - val_loss: 9510.7627\n",
            "Epoch 71/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7901.2145 - val_loss: 9499.8193\n",
            "Epoch 72/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7891.8470 - val_loss: 9488.7236\n",
            "Epoch 73/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7882.2138 - val_loss: 9477.5244\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 74/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 7872.4814 - val_loss: 9466.2061\n",
            "Epoch 75/5000\n",
            "353/353 [==============================] - 0s 41us/sample - loss: 7862.7404 - val_loss: 9454.7432\n",
            "Epoch 76/5000\n",
            "353/353 [==============================] - 0s 41us/sample - loss: 7852.8002 - val_loss: 9443.1758\n",
            "Epoch 77/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 7842.8390 - val_loss: 9431.4697\n",
            "Epoch 78/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7832.7444 - val_loss: 9419.6377\n",
            "Epoch 79/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7822.5302 - val_loss: 9407.6729\n",
            "Epoch 80/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7812.1229 - val_loss: 9395.5977\n",
            "Epoch 81/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7801.7490 - val_loss: 9383.3682\n",
            "Epoch 82/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 7791.1960 - val_loss: 9371.0068\n",
            "Epoch 83/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7780.5142 - val_loss: 9358.5166\n",
            "Epoch 84/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7769.7224 - val_loss: 9345.9014\n",
            "Epoch 85/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7758.7924 - val_loss: 9333.1729\n",
            "Epoch 86/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 7747.8369 - val_loss: 9320.2988\n",
            "Epoch 87/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 7736.8249 - val_loss: 9307.2676\n",
            "Epoch 88/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7725.5253 - val_loss: 9294.1299\n",
            "Epoch 89/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7714.2641 - val_loss: 9280.8418\n",
            "Epoch 90/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7702.7647 - val_loss: 9267.4512\n",
            "Epoch 91/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7691.1938 - val_loss: 9253.9355\n",
            "Epoch 92/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7679.6328 - val_loss: 9240.2598\n",
            "Epoch 93/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7667.8173 - val_loss: 9226.4775\n",
            "Epoch 94/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7655.8976 - val_loss: 9212.5762\n",
            "Epoch 95/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7643.9399 - val_loss: 9198.5205\n",
            "Epoch 96/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7631.8529 - val_loss: 9184.3154\n",
            "Epoch 97/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7619.5857 - val_loss: 9169.9814\n",
            "Epoch 98/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7607.1535 - val_loss: 9155.5332\n",
            "Epoch 99/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7594.7156 - val_loss: 9140.9268\n",
            "Epoch 100/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7582.2748 - val_loss: 9126.1377\n",
            "Epoch 101/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7569.4983 - val_loss: 9111.2344\n",
            "Epoch 102/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7556.6048 - val_loss: 9096.2139\n",
            "Epoch 103/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 7543.8484 - val_loss: 9081.0098\n",
            "Epoch 104/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7530.4252 - val_loss: 9065.7793\n",
            "Epoch 105/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7517.4846 - val_loss: 9050.3135\n",
            "Epoch 106/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 7504.2330 - val_loss: 9034.7012\n",
            "Epoch 107/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7490.7826 - val_loss: 9018.9668\n",
            "Epoch 108/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7477.1972 - val_loss: 9003.1113\n",
            "Epoch 109/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7463.4445 - val_loss: 8987.1318\n",
            "Epoch 110/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7449.8527 - val_loss: 8970.9443\n",
            "Epoch 111/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7435.8821 - val_loss: 8954.6445\n",
            "Epoch 112/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7421.8674 - val_loss: 8938.2061\n",
            "Epoch 113/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7407.6658 - val_loss: 8921.6484\n",
            "Epoch 114/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 7393.4566 - val_loss: 8904.9277\n",
            "Epoch 115/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 7379.1243 - val_loss: 8888.0576\n",
            "Epoch 116/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7364.4409 - val_loss: 8871.0938\n",
            "Epoch 117/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7350.0430 - val_loss: 8853.9121\n",
            "Epoch 118/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7335.1402 - val_loss: 8836.6484\n",
            "Epoch 119/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7320.3916 - val_loss: 8819.2041\n",
            "Epoch 120/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 7305.3481 - val_loss: 8801.6396\n",
            "Epoch 121/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7290.2906 - val_loss: 8783.9121\n",
            "Epoch 122/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7274.8927 - val_loss: 8766.0801\n",
            "Epoch 123/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7259.6445 - val_loss: 8748.0557\n",
            "Epoch 124/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7244.1757 - val_loss: 8729.8740\n",
            "Epoch 125/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 7228.6370 - val_loss: 8711.5264\n",
            "Epoch 126/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7212.6979 - val_loss: 8693.1055\n",
            "Epoch 127/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 7196.9260 - val_loss: 8674.5029\n",
            "Epoch 128/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 7181.1641 - val_loss: 8655.7061\n",
            "Epoch 129/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7164.8216 - val_loss: 8636.8604\n",
            "Epoch 130/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 7148.8099 - val_loss: 8617.8057\n",
            "Epoch 131/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 7132.3837 - val_loss: 8598.6523\n",
            "Epoch 132/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 7115.6628 - val_loss: 8579.4385\n",
            "Epoch 133/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 7099.3658 - val_loss: 8559.9756\n",
            "Epoch 134/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 7082.6302 - val_loss: 8540.3750\n",
            "Epoch 135/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 7065.7276 - val_loss: 8520.6465\n",
            "Epoch 136/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 7048.8924 - val_loss: 8500.7324\n",
            "Epoch 137/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 7031.8991 - val_loss: 8480.6533\n",
            "Epoch 138/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 7014.6128 - val_loss: 8460.4902\n",
            "Epoch 139/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 6997.2915 - val_loss: 8440.1973\n",
            "Epoch 140/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 6980.0001 - val_loss: 8419.7344\n",
            "Epoch 141/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6962.2587 - val_loss: 8399.2021\n",
            "Epoch 142/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6944.7505 - val_loss: 8378.4854\n",
            "Epoch 143/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 6926.8536 - val_loss: 8357.6602\n",
            "Epoch 144/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6909.2162 - val_loss: 8336.6133\n",
            "Epoch 145/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 6891.0713 - val_loss: 8315.4873\n",
            "Epoch 146/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 6872.8426 - val_loss: 8294.2666\n",
            "Epoch 147/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 6854.5530 - val_loss: 8272.9199\n",
            "Epoch 148/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6836.3642 - val_loss: 8251.3691\n",
            "Epoch 149/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6817.7539 - val_loss: 8229.7080\n",
            "Epoch 150/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 6799.0769 - val_loss: 8207.8955\n",
            "Epoch 151/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6780.5107 - val_loss: 8185.8672\n",
            "Epoch 152/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6761.7390 - val_loss: 8163.6909\n",
            "Epoch 153/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6742.7941 - val_loss: 8141.3965\n",
            "Epoch 154/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6723.4355 - val_loss: 8119.0645\n",
            "Epoch 155/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 6704.3359 - val_loss: 8096.5703\n",
            "Epoch 156/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6684.9730 - val_loss: 8073.9463\n",
            "Epoch 157/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6665.6843 - val_loss: 8051.1191\n",
            "Epoch 158/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6646.0177 - val_loss: 8028.2031\n",
            "Epoch 159/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 6626.6609 - val_loss: 8005.0767\n",
            "Epoch 160/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6606.7513 - val_loss: 7981.8823\n",
            "Epoch 161/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6586.9793 - val_loss: 7958.5430\n",
            "Epoch 162/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6566.9736 - val_loss: 7935.1216\n",
            "Epoch 163/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6546.9276 - val_loss: 7911.5811\n",
            "Epoch 164/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6526.7088 - val_loss: 7887.9468\n",
            "Epoch 165/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 6506.3908 - val_loss: 7864.2061\n",
            "Epoch 166/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6486.0049 - val_loss: 7840.3198\n",
            "Epoch 167/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6465.6755 - val_loss: 7816.2451\n",
            "Epoch 168/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6444.9585 - val_loss: 7792.0903\n",
            "Epoch 169/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 6424.1747 - val_loss: 7767.8232\n",
            "Epoch 170/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6403.4522 - val_loss: 7743.3892\n",
            "Epoch 171/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 6382.3920 - val_loss: 7718.8599\n",
            "Epoch 172/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6361.5399 - val_loss: 7694.1436\n",
            "Epoch 173/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 6340.3038 - val_loss: 7669.3608\n",
            "Epoch 174/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6319.1223 - val_loss: 7644.4375\n",
            "Epoch 175/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 6297.7188 - val_loss: 7619.4165\n",
            "Epoch 176/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6276.6445 - val_loss: 7594.1938\n",
            "Epoch 177/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6255.0221 - val_loss: 7568.9209\n",
            "Epoch 178/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 6233.3545 - val_loss: 7543.5703\n",
            "Epoch 179/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 6211.2672 - val_loss: 7518.2378\n",
            "Epoch 180/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6189.9337 - val_loss: 7492.6250\n",
            "Epoch 181/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 6167.7172 - val_loss: 7467.0088\n",
            "Epoch 182/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 6145.7608 - val_loss: 7441.2334\n",
            "Epoch 183/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 6123.6848 - val_loss: 7415.2988\n",
            "Epoch 184/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 6101.7484 - val_loss: 7389.1631\n",
            "Epoch 185/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 6079.2073 - val_loss: 7363.0044\n",
            "Epoch 186/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 6056.9241 - val_loss: 7336.7119\n",
            "Epoch 187/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 6034.5682 - val_loss: 7310.2983\n",
            "Epoch 188/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 6011.9896 - val_loss: 7283.8091\n",
            "Epoch 189/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 5989.2482 - val_loss: 7257.2617\n",
            "Epoch 190/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 5966.4917 - val_loss: 7230.6099\n",
            "Epoch 191/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5943.7010 - val_loss: 7203.8354\n",
            "Epoch 192/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 5921.0091 - val_loss: 7176.9087\n",
            "Epoch 193/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 5897.7583 - val_loss: 7150.0020\n",
            "Epoch 194/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 5874.7302 - val_loss: 7123.0020\n",
            "Epoch 195/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5851.6353 - val_loss: 7095.9009\n",
            "Epoch 196/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5828.5351 - val_loss: 7068.6611\n",
            "Epoch 197/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5805.1635 - val_loss: 7041.3418\n",
            "Epoch 198/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5781.6553 - val_loss: 7013.9707\n",
            "Epoch 199/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 5758.3150 - val_loss: 6986.4390\n",
            "Epoch 200/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5734.7106 - val_loss: 6958.8228\n",
            "Epoch 201/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 5711.3646 - val_loss: 6931.0332\n",
            "Epoch 202/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 5687.6302 - val_loss: 6903.1924\n",
            "Epoch 203/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5663.9225 - val_loss: 6875.2837\n",
            "Epoch 204/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 5639.9343 - val_loss: 6847.3647\n",
            "Epoch 205/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 5616.1702 - val_loss: 6819.3242\n",
            "Epoch 206/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 5592.2221 - val_loss: 6791.2256\n",
            "Epoch 207/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5567.9186 - val_loss: 6763.1475\n",
            "Epoch 208/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 5544.2321 - val_loss: 6734.8633\n",
            "Epoch 209/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 5520.0821 - val_loss: 6706.5386\n",
            "Epoch 210/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 5495.9251 - val_loss: 6678.1436\n",
            "Epoch 211/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 5471.5225 - val_loss: 6649.7393\n",
            "Epoch 212/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 5447.1523 - val_loss: 6621.2837\n",
            "Epoch 213/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 5422.7640 - val_loss: 6592.7451\n",
            "Epoch 214/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 5398.6163 - val_loss: 6564.0474\n",
            "Epoch 215/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 5374.3735 - val_loss: 6535.2466\n",
            "Epoch 216/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 5349.4625 - val_loss: 6506.5425\n",
            "Epoch 217/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 5325.1872 - val_loss: 6477.7056\n",
            "Epoch 218/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 41us/sample - loss: 5300.4647 - val_loss: 6448.8921\n",
            "Epoch 219/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5275.8489 - val_loss: 6420.0425\n",
            "Epoch 220/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 5251.2658 - val_loss: 6391.1172\n",
            "Epoch 221/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 5226.6718 - val_loss: 6362.1147\n",
            "Epoch 222/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 5201.8178 - val_loss: 6333.1099\n",
            "Epoch 223/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 5176.8594 - val_loss: 6304.0947\n",
            "Epoch 224/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 5152.1309 - val_loss: 6274.9863\n",
            "Epoch 225/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 5127.2130 - val_loss: 6245.8320\n",
            "Epoch 226/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5102.3596 - val_loss: 6216.5913\n",
            "Epoch 227/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 5077.1265 - val_loss: 6187.3354\n",
            "Epoch 228/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 5052.5275 - val_loss: 6157.8945\n",
            "Epoch 229/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5027.3298 - val_loss: 6128.5103\n",
            "Epoch 230/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 5002.2257 - val_loss: 6099.1123\n",
            "Epoch 231/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4977.3498 - val_loss: 6069.6279\n",
            "Epoch 232/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 4952.1184 - val_loss: 6040.1748\n",
            "Epoch 233/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 4926.9389 - val_loss: 6010.7070\n",
            "Epoch 234/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4901.7665 - val_loss: 5981.2192\n",
            "Epoch 235/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 4876.6587 - val_loss: 5951.6880\n",
            "Epoch 236/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 4851.7270 - val_loss: 5922.0845\n",
            "Epoch 237/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4826.2442 - val_loss: 5892.5713\n",
            "Epoch 238/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4801.3040 - val_loss: 5862.9561\n",
            "Epoch 239/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 4775.7771 - val_loss: 5833.4585\n",
            "Epoch 240/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 4750.4120 - val_loss: 5803.9805\n",
            "Epoch 241/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4725.6805 - val_loss: 5774.2998\n",
            "Epoch 242/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4700.2810 - val_loss: 5744.6748\n",
            "Epoch 243/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4674.8203 - val_loss: 5715.1099\n",
            "Epoch 244/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4649.6264 - val_loss: 5685.4814\n",
            "Epoch 245/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4624.4924 - val_loss: 5655.7983\n",
            "Epoch 246/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 4599.2096 - val_loss: 5626.1470\n",
            "Epoch 247/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4573.8972 - val_loss: 5596.5098\n",
            "Epoch 248/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 4548.6279 - val_loss: 5566.8706\n",
            "Epoch 249/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 4523.3837 - val_loss: 5537.2646\n",
            "Epoch 250/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4498.1138 - val_loss: 5507.7139\n",
            "Epoch 251/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4473.2351 - val_loss: 5478.1108\n",
            "Epoch 252/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4447.6504 - val_loss: 5448.6616\n",
            "Epoch 253/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4422.7393 - val_loss: 5419.1274\n",
            "Epoch 254/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4397.6996 - val_loss: 5389.5518\n",
            "Epoch 255/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4372.7088 - val_loss: 5359.9648\n",
            "Epoch 256/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4346.9745 - val_loss: 5330.6152\n",
            "Epoch 257/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4322.1820 - val_loss: 5301.1787\n",
            "Epoch 258/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4296.7589 - val_loss: 5271.8501\n",
            "Epoch 259/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4271.8656 - val_loss: 5242.4492\n",
            "Epoch 260/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 4247.2681 - val_loss: 5212.9478\n",
            "Epoch 261/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 4221.9376 - val_loss: 5183.6113\n",
            "Epoch 262/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 4196.7589 - val_loss: 5154.3540\n",
            "Epoch 263/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 4171.7703 - val_loss: 5125.1021\n",
            "Epoch 264/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 4146.6716 - val_loss: 5095.8892\n",
            "Epoch 265/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 4122.1863 - val_loss: 5066.5371\n",
            "Epoch 266/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 4097.1133 - val_loss: 5037.2651\n",
            "Epoch 267/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 4072.0011 - val_loss: 5008.1006\n",
            "Epoch 268/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 4047.2302 - val_loss: 4978.9204\n",
            "Epoch 269/5000\n",
            "353/353 [==============================] - 0s 42us/sample - loss: 4022.5448 - val_loss: 4949.7749\n",
            "Epoch 270/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 3997.6742 - val_loss: 4920.7520\n",
            "Epoch 271/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 3973.1196 - val_loss: 4891.7334\n",
            "Epoch 272/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 3948.4326 - val_loss: 4862.8037\n",
            "Epoch 273/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3923.6689 - val_loss: 4834.0186\n",
            "Epoch 274/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3899.7561 - val_loss: 4805.1445\n",
            "Epoch 275/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3874.5722 - val_loss: 4776.5776\n",
            "Epoch 276/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 3850.4832 - val_loss: 4747.9453\n",
            "Epoch 277/5000\n",
            "353/353 [==============================] - 0s 48us/sample - loss: 3826.1417 - val_loss: 4719.4058\n",
            "Epoch 278/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 3801.9064 - val_loss: 4690.9448\n",
            "Epoch 279/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 3777.4012 - val_loss: 4662.6279\n",
            "Epoch 280/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 3752.8932 - val_loss: 4634.4253\n",
            "Epoch 281/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 3729.1659 - val_loss: 4606.0771\n",
            "Epoch 282/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 3705.7017 - val_loss: 4577.6299\n",
            "Epoch 283/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 3680.9858 - val_loss: 4549.5049\n",
            "Epoch 284/5000\n",
            "353/353 [==============================] - 0s 42us/sample - loss: 3656.8399 - val_loss: 4521.4614\n",
            "Epoch 285/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 3633.3833 - val_loss: 4493.3335\n",
            "Epoch 286/5000\n",
            "353/353 [==============================] - 0s 41us/sample - loss: 3609.1687 - val_loss: 4465.4019\n",
            "Epoch 287/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3585.7241 - val_loss: 4437.4502\n",
            "Epoch 288/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 3561.7895 - val_loss: 4409.6733\n",
            "Epoch 289/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3538.1335 - val_loss: 4381.9678\n",
            "Epoch 290/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 34us/sample - loss: 3513.9859 - val_loss: 4354.4912\n",
            "Epoch 291/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 3491.1414 - val_loss: 4326.8521\n",
            "Epoch 292/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 3467.4731 - val_loss: 4299.3906\n",
            "Epoch 293/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 3444.2742 - val_loss: 4271.9780\n",
            "Epoch 294/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 3421.1758 - val_loss: 4244.6411\n",
            "Epoch 295/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 3398.0111 - val_loss: 4217.4565\n",
            "Epoch 296/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 3374.8543 - val_loss: 4190.4268\n",
            "Epoch 297/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 3351.9824 - val_loss: 4163.5093\n",
            "Epoch 298/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 3328.7759 - val_loss: 4136.8433\n",
            "Epoch 299/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 3306.2114 - val_loss: 4110.1909\n",
            "Epoch 300/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3283.5983 - val_loss: 4083.6016\n",
            "Epoch 301/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 3261.1224 - val_loss: 4057.0874\n",
            "Epoch 302/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 3238.7844 - val_loss: 4030.6750\n",
            "Epoch 303/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 3216.0448 - val_loss: 4004.5107\n",
            "Epoch 304/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 3194.0509 - val_loss: 3978.3977\n",
            "Epoch 305/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 3171.6971 - val_loss: 3952.5076\n",
            "Epoch 306/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 3149.4155 - val_loss: 3926.8005\n",
            "Epoch 307/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 3127.5543 - val_loss: 3901.1121\n",
            "Epoch 308/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 3105.8182 - val_loss: 3875.4741\n",
            "Epoch 309/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 3084.4194 - val_loss: 3849.8613\n",
            "Epoch 310/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 3062.8192 - val_loss: 3824.4399\n",
            "Epoch 311/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 3040.9249 - val_loss: 3799.3240\n",
            "Epoch 312/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 3019.2836 - val_loss: 3774.3757\n",
            "Epoch 313/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 2998.3272 - val_loss: 3749.4148\n",
            "Epoch 314/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2976.8267 - val_loss: 3724.6685\n",
            "Epoch 315/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2956.1051 - val_loss: 3699.9065\n",
            "Epoch 316/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 2935.2853 - val_loss: 3675.2485\n",
            "Epoch 317/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2914.5862 - val_loss: 3650.7249\n",
            "Epoch 318/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2893.2215 - val_loss: 3626.5684\n",
            "Epoch 319/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2872.6065 - val_loss: 3602.5093\n",
            "Epoch 320/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2852.4492 - val_loss: 3578.4543\n",
            "Epoch 321/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2832.2574 - val_loss: 3554.5100\n",
            "Epoch 322/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2811.8274 - val_loss: 3530.8005\n",
            "Epoch 323/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2791.7205 - val_loss: 3507.2300\n",
            "Epoch 324/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2771.7214 - val_loss: 3483.8379\n",
            "Epoch 325/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 2751.9528 - val_loss: 3460.6016\n",
            "Epoch 326/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 2732.4267 - val_loss: 3437.4880\n",
            "Epoch 327/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 2712.5257 - val_loss: 3414.6477\n",
            "Epoch 328/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2693.2298 - val_loss: 3391.9270\n",
            "Epoch 329/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2673.8186 - val_loss: 3369.3767\n",
            "Epoch 330/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2654.8695 - val_loss: 3346.8879\n",
            "Epoch 331/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 2635.6337 - val_loss: 3324.5906\n",
            "Epoch 332/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 2616.6119 - val_loss: 3302.4688\n",
            "Epoch 333/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2597.7469 - val_loss: 3280.5017\n",
            "Epoch 334/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2578.7918 - val_loss: 3258.7075\n",
            "Epoch 335/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2560.8885 - val_loss: 3236.7742\n",
            "Epoch 336/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2542.2612 - val_loss: 3215.0959\n",
            "Epoch 337/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 2523.5122 - val_loss: 3193.7273\n",
            "Epoch 338/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2505.7136 - val_loss: 3172.3889\n",
            "Epoch 339/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2487.7603 - val_loss: 3151.2080\n",
            "Epoch 340/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 2469.7564 - val_loss: 3130.2395\n",
            "Epoch 341/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 2452.0387 - val_loss: 3109.4429\n",
            "Epoch 342/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2434.1914 - val_loss: 3088.9109\n",
            "Epoch 343/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2416.9835 - val_loss: 3068.4749\n",
            "Epoch 344/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2399.1790 - val_loss: 3048.3298\n",
            "Epoch 345/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2382.3225 - val_loss: 3028.2090\n",
            "Epoch 346/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2365.0732 - val_loss: 3008.2822\n",
            "Epoch 347/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2348.6745 - val_loss: 2988.3352\n",
            "Epoch 348/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2331.3502 - val_loss: 2968.7190\n",
            "Epoch 349/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2314.4202 - val_loss: 2949.2847\n",
            "Epoch 350/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 2297.9133 - val_loss: 2929.9331\n",
            "Epoch 351/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2282.3254 - val_loss: 2910.4854\n",
            "Epoch 352/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 2265.6091 - val_loss: 2891.4229\n",
            "Epoch 353/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2249.6718 - val_loss: 2872.5457\n",
            "Epoch 354/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2233.7812 - val_loss: 2853.8806\n",
            "Epoch 355/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 2218.1325 - val_loss: 2835.3882\n",
            "Epoch 356/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 2201.8898 - val_loss: 2817.2793\n",
            "Epoch 357/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2186.7154 - val_loss: 2799.2070\n",
            "Epoch 358/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 2171.2343 - val_loss: 2781.3442\n",
            "Epoch 359/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2155.8552 - val_loss: 2763.6511\n",
            "Epoch 360/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2141.2990 - val_loss: 2745.9021\n",
            "Epoch 361/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2126.3856 - val_loss: 2728.3379\n",
            "Epoch 362/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 32us/sample - loss: 2111.3686 - val_loss: 2711.0449\n",
            "Epoch 363/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2096.7076 - val_loss: 2693.9678\n",
            "Epoch 364/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 2082.2844 - val_loss: 2677.0437\n",
            "Epoch 365/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 2068.1045 - val_loss: 2660.2666\n",
            "Epoch 366/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 2053.9574 - val_loss: 2643.7009\n",
            "Epoch 367/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2039.8145 - val_loss: 2627.3738\n",
            "Epoch 368/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 2026.2531 - val_loss: 2611.1404\n",
            "Epoch 369/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 2012.4909 - val_loss: 2595.1599\n",
            "Epoch 370/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1998.7494 - val_loss: 2579.4185\n",
            "Epoch 371/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1985.4442 - val_loss: 2563.7759\n",
            "Epoch 372/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1972.0285 - val_loss: 2548.3342\n",
            "Epoch 373/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1959.0201 - val_loss: 2532.9951\n",
            "Epoch 374/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1945.8793 - val_loss: 2517.8311\n",
            "Epoch 375/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1933.1461 - val_loss: 2502.7571\n",
            "Epoch 376/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1920.3614 - val_loss: 2487.8557\n",
            "Epoch 377/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1907.5993 - val_loss: 2473.1987\n",
            "Epoch 378/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1895.6929 - val_loss: 2458.5442\n",
            "Epoch 379/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1883.1697 - val_loss: 2444.1763\n",
            "Epoch 380/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1871.0744 - val_loss: 2429.9951\n",
            "Epoch 381/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1859.0891 - val_loss: 2416.0017\n",
            "Epoch 382/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1847.4472 - val_loss: 2402.1428\n",
            "Epoch 383/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1835.6576 - val_loss: 2388.5337\n",
            "Epoch 384/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1824.3181 - val_loss: 2375.0798\n",
            "Epoch 385/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1812.7246 - val_loss: 2361.8857\n",
            "Epoch 386/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 2151.46 - 0s 30us/sample - loss: 1801.6743 - val_loss: 2348.7830\n",
            "Epoch 387/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1790.5704 - val_loss: 2335.8577\n",
            "Epoch 388/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1780.1901 - val_loss: 2322.9690\n",
            "Epoch 389/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1768.7961 - val_loss: 2310.4829\n",
            "Epoch 390/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1758.4053 - val_loss: 2298.1016\n",
            "Epoch 391/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1748.0506 - val_loss: 2285.8616\n",
            "Epoch 392/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1737.4664 - val_loss: 2273.8691\n",
            "Epoch 393/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1727.4624 - val_loss: 2261.9673\n",
            "Epoch 394/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1717.2298 - val_loss: 2250.2793\n",
            "Epoch 395/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1707.4382 - val_loss: 2238.7190\n",
            "Epoch 396/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1697.8994 - val_loss: 2227.2495\n",
            "Epoch 397/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1687.8700 - val_loss: 2216.0608\n",
            "Epoch 398/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1678.4224 - val_loss: 2204.9841\n",
            "Epoch 399/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1669.3019 - val_loss: 2193.9773\n",
            "Epoch 400/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1660.0516 - val_loss: 2183.1428\n",
            "Epoch 401/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1650.6366 - val_loss: 2172.5715\n",
            "Epoch 402/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1641.7389 - val_loss: 2162.0959\n",
            "Epoch 403/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1633.1682 - val_loss: 2151.6472\n",
            "Epoch 404/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1624.3136 - val_loss: 2141.4121\n",
            "Epoch 405/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1615.9338 - val_loss: 2131.2900\n",
            "Epoch 406/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1607.0760 - val_loss: 2121.4321\n",
            "Epoch 407/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1598.8629 - val_loss: 2111.6675\n",
            "Epoch 408/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1590.6363 - val_loss: 2102.0662\n",
            "Epoch 409/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1582.4280 - val_loss: 2092.6240\n",
            "Epoch 410/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1574.4540 - val_loss: 2083.3010\n",
            "Epoch 411/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1566.5410 - val_loss: 2074.1323\n",
            "Epoch 412/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1559.3476 - val_loss: 2064.9465\n",
            "Epoch 413/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1551.0661 - val_loss: 2056.1216\n",
            "Epoch 414/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1543.8332 - val_loss: 2047.3008\n",
            "Epoch 415/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1536.4924 - val_loss: 2038.5873\n",
            "Epoch 416/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1529.3354 - val_loss: 2030.0031\n",
            "Epoch 417/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1521.9262 - val_loss: 2021.6552\n",
            "Epoch 418/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1515.2857 - val_loss: 2013.3065\n",
            "Epoch 419/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1507.9432 - val_loss: 2005.2247\n",
            "Epoch 420/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1501.0865 - val_loss: 1997.2421\n",
            "Epoch 421/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1494.6019 - val_loss: 1989.2753\n",
            "Epoch 422/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1487.7333 - val_loss: 1981.4753\n",
            "Epoch 423/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1481.6070 - val_loss: 1973.6235\n",
            "Epoch 424/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1474.8139 - val_loss: 1965.9896\n",
            "Epoch 425/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1468.3079 - val_loss: 1958.4630\n",
            "Epoch 426/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1461.9737 - val_loss: 1950.9650\n",
            "Epoch 427/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1455.7947 - val_loss: 1943.4711\n",
            "Epoch 428/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1449.4502 - val_loss: 1935.9839\n",
            "Epoch 429/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1443.3483 - val_loss: 1928.4436\n",
            "Epoch 430/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1436.9685 - val_loss: 1920.9626\n",
            "Epoch 431/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1431.0072 - val_loss: 1913.4055\n",
            "Epoch 432/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1424.6404 - val_loss: 1905.9695\n",
            "Epoch 433/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1418.3627 - val_loss: 1898.6108\n",
            "Epoch 434/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1412.2995 - val_loss: 1891.2465\n",
            "Epoch 435/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1406.1487 - val_loss: 1883.9668\n",
            "Epoch 436/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1400.1445 - val_loss: 1876.7020\n",
            "Epoch 437/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1394.0144 - val_loss: 1869.5221\n",
            "Epoch 438/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1388.1056 - val_loss: 1862.3751\n",
            "Epoch 439/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1381.9583 - val_loss: 1855.3461\n",
            "Epoch 440/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1376.2306 - val_loss: 1848.3605\n",
            "Epoch 441/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1370.5057 - val_loss: 1841.4159\n",
            "Epoch 442/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1365.2063 - val_loss: 1834.4760\n",
            "Epoch 443/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1359.0998 - val_loss: 1827.8103\n",
            "Epoch 444/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1353.6117 - val_loss: 1821.2214\n",
            "Epoch 445/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1348.1928 - val_loss: 1814.7177\n",
            "Epoch 446/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1343.2044 - val_loss: 1808.2599\n",
            "Epoch 447/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1337.3903 - val_loss: 1802.1215\n",
            "Epoch 448/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1332.6684 - val_loss: 1795.9781\n",
            "Epoch 449/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1327.2910 - val_loss: 1790.0895\n",
            "Epoch 450/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1322.6941 - val_loss: 1784.2363\n",
            "Epoch 451/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1318.1448 - val_loss: 1778.4698\n",
            "Epoch 452/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1313.1911 - val_loss: 1772.9445\n",
            "Epoch 453/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1308.4477 - val_loss: 1767.5956\n",
            "Epoch 454/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1304.2453 - val_loss: 1762.2958\n",
            "Epoch 455/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1299.7880 - val_loss: 1757.1636\n",
            "Epoch 456/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1295.4871 - val_loss: 1752.1781\n",
            "Epoch 457/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1291.6205 - val_loss: 1747.2482\n",
            "Epoch 458/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1287.7824 - val_loss: 1742.4512\n",
            "Epoch 459/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1283.7243 - val_loss: 1737.8899\n",
            "Epoch 460/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1280.0914 - val_loss: 1733.4594\n",
            "Epoch 461/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1276.6958 - val_loss: 1729.1353\n",
            "Epoch 462/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1273.0079 - val_loss: 1725.0282\n",
            "Epoch 463/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1269.6005 - val_loss: 1721.0498\n",
            "Epoch 464/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1266.4364 - val_loss: 1717.1768\n",
            "Epoch 465/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1263.1558 - val_loss: 1713.4851\n",
            "Epoch 466/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1260.4240 - val_loss: 1709.8057\n",
            "Epoch 467/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1257.3770 - val_loss: 1706.2906\n",
            "Epoch 468/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1254.5813 - val_loss: 1702.9075\n",
            "Epoch 469/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1251.8104 - val_loss: 1699.6659\n",
            "Epoch 470/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1249.1097 - val_loss: 1696.5389\n",
            "Epoch 471/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1246.5953 - val_loss: 1693.4845\n",
            "Epoch 472/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1243.9708 - val_loss: 1690.5702\n",
            "Epoch 473/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1241.7764 - val_loss: 1687.6722\n",
            "Epoch 474/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1239.3780 - val_loss: 1684.9098\n",
            "Epoch 475/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1237.4340 - val_loss: 1682.1741\n",
            "Epoch 476/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1235.4579 - val_loss: 1679.5519\n",
            "Epoch 477/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1233.2996 - val_loss: 1677.1281\n",
            "Epoch 478/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1231.2174 - val_loss: 1674.8644\n",
            "Epoch 479/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1229.6417 - val_loss: 1672.6334\n",
            "Epoch 480/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1227.7385 - val_loss: 1670.5609\n",
            "Epoch 481/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1226.0494 - val_loss: 1668.5857\n",
            "Epoch 482/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1224.6118 - val_loss: 1666.6536\n",
            "Epoch 483/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1223.1116 - val_loss: 1664.8018\n",
            "Epoch 484/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1221.7204 - val_loss: 1663.0244\n",
            "Epoch 485/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1220.0466 - val_loss: 1661.4246\n",
            "Epoch 486/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1218.9869 - val_loss: 1659.7911\n",
            "Epoch 487/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1217.6628 - val_loss: 1658.2516\n",
            "Epoch 488/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1216.3942 - val_loss: 1656.7847\n",
            "Epoch 489/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1215.3143 - val_loss: 1655.3562\n",
            "Epoch 490/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1214.4857 - val_loss: 1653.9288\n",
            "Epoch 491/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1213.3580 - val_loss: 1652.6135\n",
            "Epoch 492/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1212.1525 - val_loss: 1651.4231\n",
            "Epoch 493/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1211.4758 - val_loss: 1650.2177\n",
            "Epoch 494/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1210.5049 - val_loss: 1649.1156\n",
            "Epoch 495/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1209.6619 - val_loss: 1648.0878\n",
            "Epoch 496/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1208.9047 - val_loss: 1647.1105\n",
            "Epoch 497/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1207.9450 - val_loss: 1646.2439\n",
            "Epoch 498/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1207.4312 - val_loss: 1645.3170\n",
            "Epoch 499/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1206.7046 - val_loss: 1644.4368\n",
            "Epoch 500/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1206.1537 - val_loss: 1643.5624\n",
            "Epoch 501/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1205.4708 - val_loss: 1642.7576\n",
            "Epoch 502/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1205.0221 - val_loss: 1641.9614\n",
            "Epoch 503/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1204.2389 - val_loss: 1641.2750\n",
            "Epoch 504/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1203.7936 - val_loss: 1640.5881\n",
            "Epoch 505/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1203.3141 - val_loss: 1639.9257\n",
            "Epoch 506/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1202.7526 - val_loss: 1639.3159\n",
            "Epoch 507/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1202.3679 - val_loss: 1638.7040\n",
            "Epoch 508/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1201.8883 - val_loss: 1638.1383\n",
            "Epoch 509/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1201.5613 - val_loss: 1637.5670\n",
            "Epoch 510/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1201.1585 - val_loss: 1637.0248\n",
            "Epoch 511/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1200.8320 - val_loss: 1636.4913\n",
            "Epoch 512/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1200.3157 - val_loss: 1636.0344\n",
            "Epoch 513/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1200.0418 - val_loss: 1635.5607\n",
            "Epoch 514/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1199.7017 - val_loss: 1635.1097\n",
            "Epoch 515/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1199.4495 - val_loss: 1634.6573\n",
            "Epoch 516/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1199.1368 - val_loss: 1634.2349\n",
            "Epoch 517/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1198.7877 - val_loss: 1633.8583\n",
            "Epoch 518/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1198.6751 - val_loss: 1633.4431\n",
            "Epoch 519/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1198.4207 - val_loss: 1633.0613\n",
            "Epoch 520/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1198.0898 - val_loss: 1632.7333\n",
            "Epoch 521/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1197.9395 - val_loss: 1632.3986\n",
            "Epoch 522/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1197.6805 - val_loss: 1632.0956\n",
            "Epoch 523/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1197.4980 - val_loss: 1631.7922\n",
            "Epoch 524/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1197.4505 - val_loss: 1631.4733\n",
            "Epoch 525/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1197.0940 - val_loss: 1631.2145\n",
            "Epoch 526/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1196.9275 - val_loss: 1630.9550\n",
            "Epoch 527/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1196.7829 - val_loss: 1630.6904\n",
            "Epoch 528/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1196.6577 - val_loss: 1630.4241\n",
            "Epoch 529/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1196.4607 - val_loss: 1630.1895\n",
            "Epoch 530/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1196.3101 - val_loss: 1629.9574\n",
            "Epoch 531/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1196.2553 - val_loss: 1629.7004\n",
            "Epoch 532/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1196.0922 - val_loss: 1629.4686\n",
            "Epoch 533/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1195.9203 - val_loss: 1629.2653\n",
            "Epoch 534/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1195.8521 - val_loss: 1629.0507\n",
            "Epoch 535/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1195.6849 - val_loss: 1628.8629\n",
            "Epoch 536/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1195.5730 - val_loss: 1628.6753\n",
            "Epoch 537/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1195.4867 - val_loss: 1628.4779\n",
            "Epoch 538/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1195.3877 - val_loss: 1628.2913\n",
            "Epoch 539/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1195.2982 - val_loss: 1628.1063\n",
            "Epoch 540/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1195.1885 - val_loss: 1627.9275\n",
            "Epoch 541/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1195.1368 - val_loss: 1627.7432\n",
            "Epoch 542/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1194.9769 - val_loss: 1627.5959\n",
            "Epoch 543/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1194.9089 - val_loss: 1627.4271\n",
            "Epoch 544/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1194.8322 - val_loss: 1627.2478\n",
            "Epoch 545/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.7705 - val_loss: 1627.0695\n",
            "Epoch 546/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.6292 - val_loss: 1626.9250\n",
            "Epoch 547/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.5624 - val_loss: 1626.7594\n",
            "Epoch 548/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1194.5150 - val_loss: 1626.5876\n",
            "Epoch 549/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.4089 - val_loss: 1626.4325\n",
            "Epoch 550/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.3148 - val_loss: 1626.2913\n",
            "Epoch 551/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1194.2643 - val_loss: 1626.1394\n",
            "Epoch 552/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1194.2142 - val_loss: 1625.9825\n",
            "Epoch 553/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1194.1758 - val_loss: 1625.8292\n",
            "Epoch 554/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1194.0297 - val_loss: 1625.6975\n",
            "Epoch 555/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1193.9631 - val_loss: 1625.5634\n",
            "Epoch 556/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1193.9080 - val_loss: 1625.4387\n",
            "Epoch 557/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1193.9026 - val_loss: 1625.2847\n",
            "Epoch 558/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1193.7808 - val_loss: 1625.1558\n",
            "Epoch 559/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1193.7063 - val_loss: 1625.0281\n",
            "Epoch 560/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1193.6340 - val_loss: 1624.9037\n",
            "Epoch 561/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1193.5778 - val_loss: 1624.7653\n",
            "Epoch 562/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1193.5251 - val_loss: 1624.6371\n",
            "Epoch 563/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1193.4784 - val_loss: 1624.4943\n",
            "Epoch 564/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1193.3880 - val_loss: 1624.3616\n",
            "Epoch 565/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1193.3414 - val_loss: 1624.2261\n",
            "Epoch 566/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1193.2589 - val_loss: 1624.1052\n",
            "Epoch 567/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 977.927 - 0s 28us/sample - loss: 1193.1990 - val_loss: 1623.9783\n",
            "Epoch 568/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1193.1641 - val_loss: 1623.8450\n",
            "Epoch 569/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1193.0780 - val_loss: 1623.7236\n",
            "Epoch 570/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1193.0190 - val_loss: 1623.5963\n",
            "Epoch 571/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1192.9564 - val_loss: 1623.4718\n",
            "Epoch 572/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1192.9363 - val_loss: 1623.3328\n",
            "Epoch 573/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1192.8355 - val_loss: 1623.2112\n",
            "Epoch 574/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1192.7693 - val_loss: 1623.0869\n",
            "Epoch 575/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1192.7286 - val_loss: 1622.9486\n",
            "Epoch 576/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1192.6731 - val_loss: 1622.8147\n",
            "Epoch 577/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1192.5838 - val_loss: 1622.6925\n",
            "Epoch 578/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1192.5491 - val_loss: 1622.5740\n",
            "Epoch 579/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1192.4628 - val_loss: 1622.4456\n",
            "Epoch 580/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1192.4209 - val_loss: 1622.3110\n",
            "Epoch 581/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1192.3690 - val_loss: 1622.1772\n",
            "Epoch 582/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1192.3362 - val_loss: 1622.0437\n",
            "Epoch 583/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1192.2358 - val_loss: 1621.9283\n",
            "Epoch 584/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1192.1779 - val_loss: 1621.8015\n",
            "Epoch 585/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1192.1025 - val_loss: 1621.6815\n",
            "Epoch 586/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1192.0680 - val_loss: 1621.5565\n",
            "Epoch 587/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1191.9970 - val_loss: 1621.4355\n",
            "Epoch 588/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1191.9441 - val_loss: 1621.3201\n",
            "Epoch 589/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1191.8951 - val_loss: 1621.1886\n",
            "Epoch 590/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1191.8369 - val_loss: 1621.0715\n",
            "Epoch 591/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1191.7658 - val_loss: 1620.9398\n",
            "Epoch 592/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1191.7007 - val_loss: 1620.8151\n",
            "Epoch 593/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1191.6296 - val_loss: 1620.6859\n",
            "Epoch 594/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1191.6037 - val_loss: 1620.5519\n",
            "Epoch 595/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1191.5284 - val_loss: 1620.4204\n",
            "Epoch 596/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1191.4520 - val_loss: 1620.2980\n",
            "Epoch 597/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1191.3961 - val_loss: 1620.1726\n",
            "Epoch 598/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1191.3398 - val_loss: 1620.0472\n",
            "Epoch 599/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1191.2919 - val_loss: 1619.9246\n",
            "Epoch 600/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1191.2432 - val_loss: 1619.7953\n",
            "Epoch 601/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1191.1704 - val_loss: 1619.6685\n",
            "Epoch 602/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1191.1058 - val_loss: 1619.5461\n",
            "Epoch 603/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1191.0750 - val_loss: 1619.4204\n",
            "Epoch 604/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1191.0104 - val_loss: 1619.3003\n",
            "Epoch 605/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1190.9422 - val_loss: 1619.1832\n",
            "Epoch 606/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1190.8795 - val_loss: 1619.0609\n",
            "Epoch 607/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1190.8361 - val_loss: 1618.9425\n",
            "Epoch 608/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1190.7666 - val_loss: 1618.8219\n",
            "Epoch 609/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1190.7239 - val_loss: 1618.6952\n",
            "Epoch 610/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1190.6421 - val_loss: 1618.5792\n",
            "Epoch 611/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1190.5939 - val_loss: 1618.4531\n",
            "Epoch 612/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1190.5284 - val_loss: 1618.3289\n",
            "Epoch 613/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1190.4878 - val_loss: 1618.2067\n",
            "Epoch 614/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1190.4167 - val_loss: 1618.0751\n",
            "Epoch 615/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1190.3495 - val_loss: 1617.9490\n",
            "Epoch 616/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1190.2940 - val_loss: 1617.8231\n",
            "Epoch 617/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1190.2394 - val_loss: 1617.6935\n",
            "Epoch 618/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1190.1995 - val_loss: 1617.5664\n",
            "Epoch 619/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1190.1290 - val_loss: 1617.4406\n",
            "Epoch 620/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1190.0603 - val_loss: 1617.3141\n",
            "Epoch 621/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1190.0100 - val_loss: 1617.1876\n",
            "Epoch 622/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1189.9489 - val_loss: 1617.0645\n",
            "Epoch 623/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1189.9028 - val_loss: 1616.9380\n",
            "Epoch 624/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1189.8267 - val_loss: 1616.8140\n",
            "Epoch 625/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1189.7694 - val_loss: 1616.6940\n",
            "Epoch 626/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1189.7259 - val_loss: 1616.5697\n",
            "Epoch 627/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1189.6585 - val_loss: 1616.4478\n",
            "Epoch 628/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1189.6081 - val_loss: 1616.3300\n",
            "Epoch 629/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1189.5363 - val_loss: 1616.2078\n",
            "Epoch 630/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1189.4756 - val_loss: 1616.0906\n",
            "Epoch 631/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1189.4212 - val_loss: 1615.9673\n",
            "Epoch 632/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1189.3624 - val_loss: 1615.8475\n",
            "Epoch 633/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1189.3096 - val_loss: 1615.7256\n",
            "Epoch 634/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1189.2817 - val_loss: 1615.5963\n",
            "Epoch 635/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1189.2020 - val_loss: 1615.4725\n",
            "Epoch 636/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1189.1372 - val_loss: 1615.3514\n",
            "Epoch 637/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1189.0713 - val_loss: 1615.2284\n",
            "Epoch 638/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1189.0412 - val_loss: 1615.1011\n",
            "Epoch 639/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1188.9532 - val_loss: 1614.9840\n",
            "Epoch 640/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1188.9409 - val_loss: 1614.8629\n",
            "Epoch 641/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1188.8541 - val_loss: 1614.7339\n",
            "Epoch 642/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1188.7739 - val_loss: 1614.6102\n",
            "Epoch 643/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1188.7156 - val_loss: 1614.4829\n",
            "Epoch 644/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1188.6972 - val_loss: 1614.3560\n",
            "Epoch 645/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1188.6027 - val_loss: 1614.2299\n",
            "Epoch 646/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1188.5404 - val_loss: 1614.1086\n",
            "Epoch 647/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1188.4820 - val_loss: 1613.9847\n",
            "Epoch 648/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1188.4294 - val_loss: 1613.8623\n",
            "Epoch 649/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1188.3748 - val_loss: 1613.7410\n",
            "Epoch 650/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1188.3238 - val_loss: 1613.6193\n",
            "Epoch 651/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1188.2758 - val_loss: 1613.4988\n",
            "Epoch 652/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1188.2537 - val_loss: 1613.3690\n",
            "Epoch 653/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1188.1378 - val_loss: 1613.2454\n",
            "Epoch 654/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1188.0736 - val_loss: 1613.1241\n",
            "Epoch 655/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1188.0121 - val_loss: 1613.0079\n",
            "Epoch 656/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1187.9575 - val_loss: 1612.8868\n",
            "Epoch 657/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1187.9012 - val_loss: 1612.7671\n",
            "Epoch 658/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1187.8434 - val_loss: 1612.6433\n",
            "Epoch 659/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1187.8298 - val_loss: 1612.5121\n",
            "Epoch 660/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1187.7274 - val_loss: 1612.3929\n",
            "Epoch 661/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1187.6820 - val_loss: 1612.2637\n",
            "Epoch 662/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1187.6260 - val_loss: 1612.1353\n",
            "Epoch 663/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1187.5557 - val_loss: 1612.0088\n",
            "Epoch 664/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1187.5068 - val_loss: 1611.8840\n",
            "Epoch 665/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1187.4362 - val_loss: 1611.7565\n",
            "Epoch 666/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 713.389 - 0s 31us/sample - loss: 1187.3768 - val_loss: 1611.6388\n",
            "Epoch 667/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1187.3205 - val_loss: 1611.5142\n",
            "Epoch 668/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1187.2561 - val_loss: 1611.3937\n",
            "Epoch 669/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1187.1921 - val_loss: 1611.2714\n",
            "Epoch 670/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1187.1437 - val_loss: 1611.1469\n",
            "Epoch 671/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1187.0915 - val_loss: 1611.0205\n",
            "Epoch 672/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1187.0162 - val_loss: 1610.8964\n",
            "Epoch 673/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.9761 - val_loss: 1610.7690\n",
            "Epoch 674/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.9166 - val_loss: 1610.6516\n",
            "Epoch 675/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1186.8476 - val_loss: 1610.5248\n",
            "Epoch 676/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1186.8271 - val_loss: 1610.3942\n",
            "Epoch 677/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.7185 - val_loss: 1610.2747\n",
            "Epoch 678/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1186.6732 - val_loss: 1610.1490\n",
            "Epoch 679/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.6100 - val_loss: 1610.0250\n",
            "Epoch 680/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.5675 - val_loss: 1609.8983\n",
            "Epoch 681/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1186.4923 - val_loss: 1609.7726\n",
            "Epoch 682/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1186.4643 - val_loss: 1609.6537\n",
            "Epoch 683/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1186.3642 - val_loss: 1609.5312\n",
            "Epoch 684/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1186.3064 - val_loss: 1609.4037\n",
            "Epoch 685/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1186.2436 - val_loss: 1609.2759\n",
            "Epoch 686/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1186.1893 - val_loss: 1609.1453\n",
            "Epoch 687/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1186.1358 - val_loss: 1609.0129\n",
            "Epoch 688/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1186.0689 - val_loss: 1608.8822\n",
            "Epoch 689/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1186.0040 - val_loss: 1608.7543\n",
            "Epoch 690/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1185.9400 - val_loss: 1608.6311\n",
            "Epoch 691/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1185.8880 - val_loss: 1608.5035\n",
            "Epoch 692/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1185.8202 - val_loss: 1608.3743\n",
            "Epoch 693/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1185.7801 - val_loss: 1608.2439\n",
            "Epoch 694/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.7139 - val_loss: 1608.1135\n",
            "Epoch 695/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.6619 - val_loss: 1607.9821\n",
            "Epoch 696/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1185.5705 - val_loss: 1607.8601\n",
            "Epoch 697/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.5156 - val_loss: 1607.7295\n",
            "Epoch 698/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1185.4615 - val_loss: 1607.5995\n",
            "Epoch 699/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.4037 - val_loss: 1607.4684\n",
            "Epoch 700/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1185.3449 - val_loss: 1607.3416\n",
            "Epoch 701/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1185.2906 - val_loss: 1607.2161\n",
            "Epoch 702/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.2126 - val_loss: 1607.0857\n",
            "Epoch 703/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1185.1725 - val_loss: 1606.9532\n",
            "Epoch 704/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1185.1016 - val_loss: 1606.8239\n",
            "Epoch 705/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1185.0630 - val_loss: 1606.6947\n",
            "Epoch 706/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1184.9999 - val_loss: 1606.5685\n",
            "Epoch 707/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.9137 - val_loss: 1606.4454\n",
            "Epoch 708/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1184.8486 - val_loss: 1606.3168\n",
            "Epoch 709/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1184.8068 - val_loss: 1606.1898\n",
            "Epoch 710/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.7338 - val_loss: 1606.0618\n",
            "Epoch 711/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.6643 - val_loss: 1605.9352\n",
            "Epoch 712/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1184.6101 - val_loss: 1605.8055\n",
            "Epoch 713/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1184.5595 - val_loss: 1605.6805\n",
            "Epoch 714/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.4898 - val_loss: 1605.5560\n",
            "Epoch 715/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1184.4253 - val_loss: 1605.4261\n",
            "Epoch 716/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1184.3675 - val_loss: 1605.2965\n",
            "Epoch 717/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1184.2992 - val_loss: 1605.1665\n",
            "Epoch 718/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1184.2527 - val_loss: 1605.0359\n",
            "Epoch 719/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.1764 - val_loss: 1604.9059\n",
            "Epoch 720/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1184.1178 - val_loss: 1604.7743\n",
            "Epoch 721/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1184.1254 - val_loss: 1604.6428\n",
            "Epoch 722/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1183.9934 - val_loss: 1604.5077\n",
            "Epoch 723/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1183.9276 - val_loss: 1604.3793\n",
            "Epoch 724/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1183.8798 - val_loss: 1604.2478\n",
            "Epoch 725/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1183.8214 - val_loss: 1604.1156\n",
            "Epoch 726/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1183.7538 - val_loss: 1603.9866\n",
            "Epoch 727/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1183.6884 - val_loss: 1603.8484\n",
            "Epoch 728/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1183.6234 - val_loss: 1603.7148\n",
            "Epoch 729/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1183.5617 - val_loss: 1603.5864\n",
            "Epoch 730/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1183.5078 - val_loss: 1603.4526\n",
            "Epoch 731/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1183.4291 - val_loss: 1603.3219\n",
            "Epoch 732/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1183.3672 - val_loss: 1603.1917\n",
            "Epoch 733/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1183.3099 - val_loss: 1603.0565\n",
            "Epoch 734/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1183.2440 - val_loss: 1602.9227\n",
            "Epoch 735/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1183.1956 - val_loss: 1602.7872\n",
            "Epoch 736/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1183.1266 - val_loss: 1602.6517\n",
            "Epoch 737/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1183.0603 - val_loss: 1602.5179\n",
            "Epoch 738/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1183.0047 - val_loss: 1602.3864\n",
            "Epoch 739/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1182.9541 - val_loss: 1602.2584\n",
            "Epoch 740/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1182.8674 - val_loss: 1602.1266\n",
            "Epoch 741/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1182.8341 - val_loss: 1601.9917\n",
            "Epoch 742/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1182.7487 - val_loss: 1601.8555\n",
            "Epoch 743/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1182.6834 - val_loss: 1601.7261\n",
            "Epoch 744/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1182.6163 - val_loss: 1601.5959\n",
            "Epoch 745/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1182.5678 - val_loss: 1601.4585\n",
            "Epoch 746/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1182.4933 - val_loss: 1601.3281\n",
            "Epoch 747/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1182.4427 - val_loss: 1601.1921\n",
            "Epoch 748/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1182.3666 - val_loss: 1601.0603\n",
            "Epoch 749/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1182.3047 - val_loss: 1600.9314\n",
            "Epoch 750/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1182.2355 - val_loss: 1600.7987\n",
            "Epoch 751/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1182.1736 - val_loss: 1600.6666\n",
            "Epoch 752/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1182.1157 - val_loss: 1600.5311\n",
            "Epoch 753/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1182.0470 - val_loss: 1600.3938\n",
            "Epoch 754/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1182.0122 - val_loss: 1600.2584\n",
            "Epoch 755/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1181.9121 - val_loss: 1600.1261\n",
            "Epoch 756/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1181.8937 - val_loss: 1599.9900\n",
            "Epoch 757/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1181.7967 - val_loss: 1599.8521\n",
            "Epoch 758/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1181.7339 - val_loss: 1599.7184\n",
            "Epoch 759/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1181.6659 - val_loss: 1599.5808\n",
            "Epoch 760/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1181.5900 - val_loss: 1599.4479\n",
            "Epoch 761/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1181.5264 - val_loss: 1599.3103\n",
            "Epoch 762/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1181.4787 - val_loss: 1599.1687\n",
            "Epoch 763/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1181.3956 - val_loss: 1599.0311\n",
            "Epoch 764/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1181.3296 - val_loss: 1598.8953\n",
            "Epoch 765/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1181.2791 - val_loss: 1598.7556\n",
            "Epoch 766/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1181.2172 - val_loss: 1598.6174\n",
            "Epoch 767/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1181.1371 - val_loss: 1598.4808\n",
            "Epoch 768/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1181.0969 - val_loss: 1598.3439\n",
            "Epoch 769/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1181.0081 - val_loss: 1598.2097\n",
            "Epoch 770/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1180.9553 - val_loss: 1598.0747\n",
            "Epoch 771/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1180.8797 - val_loss: 1597.9410\n",
            "Epoch 772/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1180.8154 - val_loss: 1597.8082\n",
            "Epoch 773/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1180.7631 - val_loss: 1597.6753\n",
            "Epoch 774/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1180.6854 - val_loss: 1597.5359\n",
            "Epoch 775/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1180.6257 - val_loss: 1597.3962\n",
            "Epoch 776/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1180.5588 - val_loss: 1597.2596\n",
            "Epoch 777/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1180.5168 - val_loss: 1597.1201\n",
            "Epoch 778/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1180.4539 - val_loss: 1596.9791\n",
            "Epoch 779/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1180.3683 - val_loss: 1596.8463\n",
            "Epoch 780/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1180.3126 - val_loss: 1596.7057\n",
            "Epoch 781/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1180.2398 - val_loss: 1596.5664\n",
            "Epoch 782/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1180.1730 - val_loss: 1596.4307\n",
            "Epoch 783/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1180.1041 - val_loss: 1596.2985\n",
            "Epoch 784/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1180.0531 - val_loss: 1596.1632\n",
            "Epoch 785/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.9713 - val_loss: 1596.0305\n",
            "Epoch 786/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1179.9392 - val_loss: 1595.8903\n",
            "Epoch 787/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1179.8514 - val_loss: 1595.7562\n",
            "Epoch 788/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.7777 - val_loss: 1595.6216\n",
            "Epoch 789/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1179.7201 - val_loss: 1595.4844\n",
            "Epoch 790/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.6403 - val_loss: 1595.3529\n",
            "Epoch 791/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1179.5845 - val_loss: 1595.2135\n",
            "Epoch 792/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1179.5383 - val_loss: 1595.0765\n",
            "Epoch 793/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.4563 - val_loss: 1594.9391\n",
            "Epoch 794/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 33us/sample - loss: 1179.4397 - val_loss: 1594.7897\n",
            "Epoch 795/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.3248 - val_loss: 1594.6462\n",
            "Epoch 796/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1179.2511 - val_loss: 1594.5001\n",
            "Epoch 797/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1179.1812 - val_loss: 1594.3583\n",
            "Epoch 798/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1195.64 - 0s 30us/sample - loss: 1179.1121 - val_loss: 1594.2158\n",
            "Epoch 799/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1179.0655 - val_loss: 1594.0706\n",
            "Epoch 800/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1178.9811 - val_loss: 1593.9280\n",
            "Epoch 801/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1178.9148 - val_loss: 1593.7888\n",
            "Epoch 802/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1178.8424 - val_loss: 1593.6497\n",
            "Epoch 803/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1178.7984 - val_loss: 1593.5087\n",
            "Epoch 804/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1178.7472 - val_loss: 1593.3650\n",
            "Epoch 805/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1178.6463 - val_loss: 1593.2198\n",
            "Epoch 806/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1178.5737 - val_loss: 1593.0801\n",
            "Epoch 807/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1178.5248 - val_loss: 1592.9326\n",
            "Epoch 808/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1178.4494 - val_loss: 1592.7842\n",
            "Epoch 809/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1178.3856 - val_loss: 1592.6405\n",
            "Epoch 810/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1178.3174 - val_loss: 1592.5020\n",
            "Epoch 811/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1178.2478 - val_loss: 1592.3650\n",
            "Epoch 812/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1178.1827 - val_loss: 1592.2281\n",
            "Epoch 813/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1178.1195 - val_loss: 1592.0857\n",
            "Epoch 814/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1178.0816 - val_loss: 1591.9436\n",
            "Epoch 815/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1177.9697 - val_loss: 1591.8025\n",
            "Epoch 816/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1177.9564 - val_loss: 1591.6539\n",
            "Epoch 817/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1177.8458 - val_loss: 1591.5129\n",
            "Epoch 818/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1177.7753 - val_loss: 1591.3743\n",
            "Epoch 819/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1177.7276 - val_loss: 1591.2358\n",
            "Epoch 820/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1177.6438 - val_loss: 1591.0946\n",
            "Epoch 821/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1177.5999 - val_loss: 1590.9603\n",
            "Epoch 822/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1177.5389 - val_loss: 1590.8124\n",
            "Epoch 823/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1177.4357 - val_loss: 1590.6700\n",
            "Epoch 824/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1177.3806 - val_loss: 1590.5266\n",
            "Epoch 825/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1177.3089 - val_loss: 1590.3811\n",
            "Epoch 826/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1177.2382 - val_loss: 1590.2448\n",
            "Epoch 827/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1177.1834 - val_loss: 1590.1006\n",
            "Epoch 828/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1177.1007 - val_loss: 1589.9597\n",
            "Epoch 829/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1177.0358 - val_loss: 1589.8141\n",
            "Epoch 830/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1176.9470 - val_loss: 1589.6720\n",
            "Epoch 831/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1176.9061 - val_loss: 1589.5178\n",
            "Epoch 832/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1176.8376 - val_loss: 1589.3700\n",
            "Epoch 833/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1176.7622 - val_loss: 1589.2253\n",
            "Epoch 834/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1176.6987 - val_loss: 1589.0846\n",
            "Epoch 835/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1176.6239 - val_loss: 1588.9370\n",
            "Epoch 836/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1176.5665 - val_loss: 1588.7930\n",
            "Epoch 837/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1176.5137 - val_loss: 1588.6456\n",
            "Epoch 838/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1176.4758 - val_loss: 1588.5094\n",
            "Epoch 839/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1176.3425 - val_loss: 1588.3704\n",
            "Epoch 840/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1176.2871 - val_loss: 1588.2196\n",
            "Epoch 841/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1176.2098 - val_loss: 1588.0773\n",
            "Epoch 842/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1176.1600 - val_loss: 1587.9357\n",
            "Epoch 843/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1176.1145 - val_loss: 1587.7983\n",
            "Epoch 844/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1176.0131 - val_loss: 1587.6479\n",
            "Epoch 845/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1175.9604 - val_loss: 1587.4950\n",
            "Epoch 846/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1175.8802 - val_loss: 1587.3490\n",
            "Epoch 847/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1175.8094 - val_loss: 1587.2112\n",
            "Epoch 848/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1175.7493 - val_loss: 1587.0681\n",
            "Epoch 849/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1175.7006 - val_loss: 1586.9294\n",
            "Epoch 850/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1175.5979 - val_loss: 1586.7859\n",
            "Epoch 851/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1175.5718 - val_loss: 1586.6415\n",
            "Epoch 852/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1175.4623 - val_loss: 1586.4950\n",
            "Epoch 853/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1175.4077 - val_loss: 1586.3488\n",
            "Epoch 854/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1175.3491 - val_loss: 1586.1993\n",
            "Epoch 855/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1175.2866 - val_loss: 1586.0535\n",
            "Epoch 856/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1175.2363 - val_loss: 1585.9125\n",
            "Epoch 857/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1175.1573 - val_loss: 1585.7750\n",
            "Epoch 858/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1175.0714 - val_loss: 1585.6216\n",
            "Epoch 859/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1174.9995 - val_loss: 1585.4750\n",
            "Epoch 860/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1174.9324 - val_loss: 1585.3239\n",
            "Epoch 861/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1174.8601 - val_loss: 1585.1749\n",
            "Epoch 862/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1174.7995 - val_loss: 1585.0271\n",
            "Epoch 863/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1174.7014 - val_loss: 1584.8922\n",
            "Epoch 864/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1174.6816 - val_loss: 1584.7560\n",
            "Epoch 865/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1174.5652 - val_loss: 1584.6188\n",
            "Epoch 866/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1174.5024 - val_loss: 1584.4738\n",
            "Epoch 867/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1174.4608 - val_loss: 1584.3243\n",
            "Epoch 868/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1174.3783 - val_loss: 1584.1761\n",
            "Epoch 869/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1174.2965 - val_loss: 1584.0372\n",
            "Epoch 870/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1174.2125 - val_loss: 1583.9055\n",
            "Epoch 871/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1174.1636 - val_loss: 1583.7719\n",
            "Epoch 872/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1174.0738 - val_loss: 1583.6318\n",
            "Epoch 873/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1174.0079 - val_loss: 1583.4904\n",
            "Epoch 874/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1173.9539 - val_loss: 1583.3430\n",
            "Epoch 875/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1173.8774 - val_loss: 1583.1940\n",
            "Epoch 876/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1173.8036 - val_loss: 1583.0477\n",
            "Epoch 877/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1173.7467 - val_loss: 1582.9041\n",
            "Epoch 878/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1173.6581 - val_loss: 1582.7629\n",
            "Epoch 879/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1173.5951 - val_loss: 1582.6150\n",
            "Epoch 880/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1173.5281 - val_loss: 1582.4662\n",
            "Epoch 881/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1173.4694 - val_loss: 1582.3080\n",
            "Epoch 882/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1173.3793 - val_loss: 1582.1650\n",
            "Epoch 883/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1173.3112 - val_loss: 1582.0160\n",
            "Epoch 884/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1173.2515 - val_loss: 1581.8651\n",
            "Epoch 885/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1173.1955 - val_loss: 1581.6970\n",
            "Epoch 886/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1173.0824 - val_loss: 1581.5481\n",
            "Epoch 887/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1173.0890 - val_loss: 1581.3840\n",
            "Epoch 888/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1172.9525 - val_loss: 1581.2419\n",
            "Epoch 889/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1172.8746 - val_loss: 1581.0907\n",
            "Epoch 890/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1172.8144 - val_loss: 1580.9467\n",
            "Epoch 891/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1172.7301 - val_loss: 1580.7980\n",
            "Epoch 892/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1172.6754 - val_loss: 1580.6531\n",
            "Epoch 893/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1172.5933 - val_loss: 1580.4908\n",
            "Epoch 894/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1172.5259 - val_loss: 1580.3398\n",
            "Epoch 895/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1172.4419 - val_loss: 1580.1837\n",
            "Epoch 896/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1172.3942 - val_loss: 1580.0354\n",
            "Epoch 897/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1172.2952 - val_loss: 1579.8796\n",
            "Epoch 898/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1172.2218 - val_loss: 1579.7264\n",
            "Epoch 899/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1172.1556 - val_loss: 1579.5629\n",
            "Epoch 900/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 1172.1097 - val_loss: 1579.4113\n",
            "Epoch 901/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1172.0184 - val_loss: 1579.2438\n",
            "Epoch 902/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1171.9631 - val_loss: 1579.0790\n",
            "Epoch 903/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1171.8732 - val_loss: 1578.9319\n",
            "Epoch 904/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1171.8137 - val_loss: 1578.7843\n",
            "Epoch 905/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1171.7158 - val_loss: 1578.6357\n",
            "Epoch 906/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1171.6795 - val_loss: 1578.4718\n",
            "Epoch 907/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1171.5968 - val_loss: 1578.3239\n",
            "Epoch 908/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1171.5296 - val_loss: 1578.1781\n",
            "Epoch 909/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1171.4625 - val_loss: 1578.0156\n",
            "Epoch 910/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1171.3751 - val_loss: 1577.8644\n",
            "Epoch 911/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1171.3178 - val_loss: 1577.7153\n",
            "Epoch 912/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1171.2312 - val_loss: 1577.5691\n",
            "Epoch 913/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1171.1554 - val_loss: 1577.4203\n",
            "Epoch 914/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1171.0904 - val_loss: 1577.2716\n",
            "Epoch 915/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1170.9936 - val_loss: 1577.1360\n",
            "Epoch 916/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1170.9356 - val_loss: 1576.9889\n",
            "Epoch 917/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1170.8663 - val_loss: 1576.8361\n",
            "Epoch 918/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1170.8371 - val_loss: 1576.7036\n",
            "Epoch 919/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.7089 - val_loss: 1576.5542\n",
            "Epoch 920/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1170.6759 - val_loss: 1576.4100\n",
            "Epoch 921/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1170.5978 - val_loss: 1576.2568\n",
            "Epoch 922/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1170.5098 - val_loss: 1576.0880\n",
            "Epoch 923/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.4219 - val_loss: 1575.9142\n",
            "Epoch 924/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1170.3516 - val_loss: 1575.7478\n",
            "Epoch 925/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.2825 - val_loss: 1575.5817\n",
            "Epoch 926/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.2027 - val_loss: 1575.4108\n",
            "Epoch 927/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.1181 - val_loss: 1575.2527\n",
            "Epoch 928/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1170.0457 - val_loss: 1575.0880\n",
            "Epoch 929/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1169.9878 - val_loss: 1574.9177\n",
            "Epoch 930/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1169.9060 - val_loss: 1574.7546\n",
            "Epoch 931/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1169.8553 - val_loss: 1574.6067\n",
            "Epoch 932/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1169.7736 - val_loss: 1574.4376\n",
            "Epoch 933/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1169.6843 - val_loss: 1574.2777\n",
            "Epoch 934/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1169.6034 - val_loss: 1574.1218\n",
            "Epoch 935/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1169.5523 - val_loss: 1573.9691\n",
            "Epoch 936/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1169.4584 - val_loss: 1573.8151\n",
            "Epoch 937/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1169.3780 - val_loss: 1573.6627\n",
            "Epoch 938/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1169.3174 - val_loss: 1573.5022\n",
            "Epoch 939/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1169.2567 - val_loss: 1573.3518\n",
            "Epoch 940/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1169.1580 - val_loss: 1573.1993\n",
            "Epoch 941/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1169.1096 - val_loss: 1573.0392\n",
            "Epoch 942/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1169.0274 - val_loss: 1572.8895\n",
            "Epoch 943/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1168.9392 - val_loss: 1572.7461\n",
            "Epoch 944/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1168.8660 - val_loss: 1572.5925\n",
            "Epoch 945/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1168.7977 - val_loss: 1572.4417\n",
            "Epoch 946/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1168.7260 - val_loss: 1572.2855\n",
            "Epoch 947/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1168.6693 - val_loss: 1572.1251\n",
            "Epoch 948/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1168.5752 - val_loss: 1571.9760\n",
            "Epoch 949/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1168.5148 - val_loss: 1571.8276\n",
            "Epoch 950/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1168.4442 - val_loss: 1571.6829\n",
            "Epoch 951/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1168.3547 - val_loss: 1571.5148\n",
            "Epoch 952/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1168.2882 - val_loss: 1571.3556\n",
            "Epoch 953/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1168.2038 - val_loss: 1571.1891\n",
            "Epoch 954/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1168.1253 - val_loss: 1571.0251\n",
            "Epoch 955/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1168.0636 - val_loss: 1570.8574\n",
            "Epoch 956/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1168.0367 - val_loss: 1570.6898\n",
            "Epoch 957/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1167.9358 - val_loss: 1570.5498\n",
            "Epoch 958/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1167.8380 - val_loss: 1570.3911\n",
            "Epoch 959/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1167.7590 - val_loss: 1570.2529\n",
            "Epoch 960/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1167.6695 - val_loss: 1570.0977\n",
            "Epoch 961/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1167.6072 - val_loss: 1569.9384\n",
            "Epoch 962/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1167.5283 - val_loss: 1569.7980\n",
            "Epoch 963/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1167.4461 - val_loss: 1569.6471\n",
            "Epoch 964/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1167.3690 - val_loss: 1569.5007\n",
            "Epoch 965/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1167.2948 - val_loss: 1569.3488\n",
            "Epoch 966/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1167.2252 - val_loss: 1569.1895\n",
            "Epoch 967/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1167.1612 - val_loss: 1569.0396\n",
            "Epoch 968/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1167.0913 - val_loss: 1568.8718\n",
            "Epoch 969/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1167.0142 - val_loss: 1568.7157\n",
            "Epoch 970/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1166.9153 - val_loss: 1568.5613\n",
            "Epoch 971/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1166.8730 - val_loss: 1568.3829\n",
            "Epoch 972/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1166.7760 - val_loss: 1568.2152\n",
            "Epoch 973/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1166.7072 - val_loss: 1568.0573\n",
            "Epoch 974/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1166.6188 - val_loss: 1567.9097\n",
            "Epoch 975/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1166.5474 - val_loss: 1567.7404\n",
            "Epoch 976/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1166.4594 - val_loss: 1567.5820\n",
            "Epoch 977/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1166.4958 - val_loss: 1567.4440\n",
            "Epoch 978/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1166.3172 - val_loss: 1567.2710\n",
            "Epoch 979/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1166.2719 - val_loss: 1567.1108\n",
            "Epoch 980/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1166.1608 - val_loss: 1566.9292\n",
            "Epoch 981/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1166.1115 - val_loss: 1566.7500\n",
            "Epoch 982/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1165.9971 - val_loss: 1566.5942\n",
            "Epoch 983/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1165.9228 - val_loss: 1566.4288\n",
            "Epoch 984/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1165.8589 - val_loss: 1566.2531\n",
            "Epoch 985/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1165.7820 - val_loss: 1566.0830\n",
            "Epoch 986/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1165.7079 - val_loss: 1565.9120\n",
            "Epoch 987/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1165.6694 - val_loss: 1565.7321\n",
            "Epoch 988/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1165.5543 - val_loss: 1565.5815\n",
            "Epoch 989/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1165.4844 - val_loss: 1565.4318\n",
            "Epoch 990/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1165.4063 - val_loss: 1565.2729\n",
            "Epoch 991/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1163.08 - 0s 25us/sample - loss: 1165.3268 - val_loss: 1565.1041\n",
            "Epoch 992/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1165.2232 - val_loss: 1564.9536\n",
            "Epoch 993/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1165.1830 - val_loss: 1564.7998\n",
            "Epoch 994/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1165.0901 - val_loss: 1564.6398\n",
            "Epoch 995/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1165.0037 - val_loss: 1564.4630\n",
            "Epoch 996/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1164.9412 - val_loss: 1564.2865\n",
            "Epoch 997/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1164.8897 - val_loss: 1564.1038\n",
            "Epoch 998/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1164.7909 - val_loss: 1563.9603\n",
            "Epoch 999/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1164.7205 - val_loss: 1563.7949\n",
            "Epoch 1000/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1164.6387 - val_loss: 1563.6379\n",
            "Epoch 1001/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1164.5350 - val_loss: 1563.4653\n",
            "Epoch 1002/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1164.4542 - val_loss: 1563.2919\n",
            "Epoch 1003/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1164.3940 - val_loss: 1563.1102\n",
            "Epoch 1004/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1164.3253 - val_loss: 1562.9492\n",
            "Epoch 1005/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1164.2251 - val_loss: 1562.7764\n",
            "Epoch 1006/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1164.1613 - val_loss: 1562.6067\n",
            "Epoch 1007/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1164.1323 - val_loss: 1562.4248\n",
            "Epoch 1008/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1164.0257 - val_loss: 1562.2614\n",
            "Epoch 1009/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1164.0205 - val_loss: 1562.1334\n",
            "Epoch 1010/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 33us/sample - loss: 1163.8559 - val_loss: 1561.9740\n",
            "Epoch 1011/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1163.7656 - val_loss: 1561.8252\n",
            "Epoch 1012/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1163.7022 - val_loss: 1561.6760\n",
            "Epoch 1013/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1163.6320 - val_loss: 1561.5228\n",
            "Epoch 1014/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1163.5493 - val_loss: 1561.3678\n",
            "Epoch 1015/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1163.4766 - val_loss: 1561.2069\n",
            "Epoch 1016/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1163.4144 - val_loss: 1561.0751\n",
            "Epoch 1017/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1163.3135 - val_loss: 1560.9281\n",
            "Epoch 1018/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1163.2275 - val_loss: 1560.7698\n",
            "Epoch 1019/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1163.1600 - val_loss: 1560.6212\n",
            "Epoch 1020/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1163.0840 - val_loss: 1560.4661\n",
            "Epoch 1021/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1163.0015 - val_loss: 1560.3231\n",
            "Epoch 1022/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1162.9303 - val_loss: 1560.1636\n",
            "Epoch 1023/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1162.8387 - val_loss: 1560.0179\n",
            "Epoch 1024/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1162.7715 - val_loss: 1559.8741\n",
            "Epoch 1025/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1162.7201 - val_loss: 1559.7299\n",
            "Epoch 1026/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1162.6156 - val_loss: 1559.5599\n",
            "Epoch 1027/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1162.5303 - val_loss: 1559.3990\n",
            "Epoch 1028/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1162.4645 - val_loss: 1559.2290\n",
            "Epoch 1029/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1162.4165 - val_loss: 1559.0709\n",
            "Epoch 1030/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1162.2985 - val_loss: 1558.8964\n",
            "Epoch 1031/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1162.2246 - val_loss: 1558.7328\n",
            "Epoch 1032/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1162.1578 - val_loss: 1558.5469\n",
            "Epoch 1033/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1162.0780 - val_loss: 1558.3766\n",
            "Epoch 1034/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1162.0204 - val_loss: 1558.1996\n",
            "Epoch 1035/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1161.9388 - val_loss: 1558.0582\n",
            "Epoch 1036/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1161.8370 - val_loss: 1557.9170\n",
            "Epoch 1037/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1161.7965 - val_loss: 1557.7777\n",
            "Epoch 1038/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1161.6725 - val_loss: 1557.6044\n",
            "Epoch 1039/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1161.6155 - val_loss: 1557.4382\n",
            "Epoch 1040/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1161.5418 - val_loss: 1557.2515\n",
            "Epoch 1041/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1161.4501 - val_loss: 1557.0796\n",
            "Epoch 1042/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1161.3834 - val_loss: 1556.8888\n",
            "Epoch 1043/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1161.2835 - val_loss: 1556.7284\n",
            "Epoch 1044/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1161.2030 - val_loss: 1556.5549\n",
            "Epoch 1045/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1161.1522 - val_loss: 1556.3704\n",
            "Epoch 1046/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1161.0485 - val_loss: 1556.2053\n",
            "Epoch 1047/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1160.9927 - val_loss: 1556.0565\n",
            "Epoch 1048/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1160.8994 - val_loss: 1555.8776\n",
            "Epoch 1049/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1160.8339 - val_loss: 1555.6974\n",
            "Epoch 1050/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1160.7419 - val_loss: 1555.5317\n",
            "Epoch 1051/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1160.6603 - val_loss: 1555.3662\n",
            "Epoch 1052/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1160.5915 - val_loss: 1555.2111\n",
            "Epoch 1053/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1160.5461 - val_loss: 1555.0339\n",
            "Epoch 1054/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1160.4189 - val_loss: 1554.9005\n",
            "Epoch 1055/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1160.3527 - val_loss: 1554.7725\n",
            "Epoch 1056/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1160.2845 - val_loss: 1554.5999\n",
            "Epoch 1057/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1160.2128 - val_loss: 1554.4696\n",
            "Epoch 1058/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1160.1212 - val_loss: 1554.3185\n",
            "Epoch 1059/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1160.0273 - val_loss: 1554.1620\n",
            "Epoch 1060/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1159.9530 - val_loss: 1553.9845\n",
            "Epoch 1061/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1159.8627 - val_loss: 1553.8220\n",
            "Epoch 1062/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1159.8099 - val_loss: 1553.6403\n",
            "Epoch 1063/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1159.7321 - val_loss: 1553.4688\n",
            "Epoch 1064/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1159.6274 - val_loss: 1553.3202\n",
            "Epoch 1065/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1159.5544 - val_loss: 1553.1875\n",
            "Epoch 1066/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1159.4877 - val_loss: 1553.0464\n",
            "Epoch 1067/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1159.4194 - val_loss: 1552.8888\n",
            "Epoch 1068/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1159.3264 - val_loss: 1552.7091\n",
            "Epoch 1069/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1159.2596 - val_loss: 1552.5256\n",
            "Epoch 1070/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1159.1689 - val_loss: 1552.3617\n",
            "Epoch 1071/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1159.0960 - val_loss: 1552.1898\n",
            "Epoch 1072/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1159.0364 - val_loss: 1552.0575\n",
            "Epoch 1073/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1158.9660 - val_loss: 1551.9036\n",
            "Epoch 1074/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1158.8611 - val_loss: 1551.6996\n",
            "Epoch 1075/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1158.7735 - val_loss: 1551.5221\n",
            "Epoch 1076/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1158.6980 - val_loss: 1551.3297\n",
            "Epoch 1077/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1158.6167 - val_loss: 1551.1597\n",
            "Epoch 1078/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1158.5237 - val_loss: 1550.9784\n",
            "Epoch 1079/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1158.4523 - val_loss: 1550.7848\n",
            "Epoch 1080/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1158.3771 - val_loss: 1550.6125\n",
            "Epoch 1081/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1158.3044 - val_loss: 1550.4393\n",
            "Epoch 1082/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1158.1988 - val_loss: 1550.2450\n",
            "Epoch 1083/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1158.1810 - val_loss: 1550.0719\n",
            "Epoch 1084/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1158.0437 - val_loss: 1549.8544\n",
            "Epoch 1085/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1158.0032 - val_loss: 1549.6372\n",
            "Epoch 1086/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1157.9150 - val_loss: 1549.4653\n",
            "Epoch 1087/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1157.8090 - val_loss: 1549.2739\n",
            "Epoch 1088/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1157.8099 - val_loss: 1549.0636\n",
            "Epoch 1089/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1157.6683 - val_loss: 1548.8804\n",
            "Epoch 1090/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1157.5960 - val_loss: 1548.7145\n",
            "Epoch 1091/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1157.5199 - val_loss: 1548.5427\n",
            "Epoch 1092/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1157.4393 - val_loss: 1548.3734\n",
            "Epoch 1093/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1157.3793 - val_loss: 1548.2227\n",
            "Epoch 1094/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1157.2670 - val_loss: 1548.0752\n",
            "Epoch 1095/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1157.2277 - val_loss: 1547.9276\n",
            "Epoch 1096/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1157.1087 - val_loss: 1547.7358\n",
            "Epoch 1097/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1157.0149 - val_loss: 1547.5568\n",
            "Epoch 1098/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.9698 - val_loss: 1547.3846\n",
            "Epoch 1099/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.8519 - val_loss: 1547.2000\n",
            "Epoch 1100/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.8232 - val_loss: 1547.0397\n",
            "Epoch 1101/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1156.6993 - val_loss: 1546.8461\n",
            "Epoch 1102/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1156.6192 - val_loss: 1546.6625\n",
            "Epoch 1103/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.5597 - val_loss: 1546.4717\n",
            "Epoch 1104/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1156.4526 - val_loss: 1546.3076\n",
            "Epoch 1105/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.3921 - val_loss: 1546.1379\n",
            "Epoch 1106/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1156.3111 - val_loss: 1546.0032\n",
            "Epoch 1107/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1156.2243 - val_loss: 1545.8677\n",
            "Epoch 1108/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1156.1138 - val_loss: 1545.7196\n",
            "Epoch 1109/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1156.0427 - val_loss: 1545.5752\n",
            "Epoch 1110/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1156.0073 - val_loss: 1545.4393\n",
            "Epoch 1111/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1155.9031 - val_loss: 1545.2722\n",
            "Epoch 1112/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1155.8238 - val_loss: 1545.1078\n",
            "Epoch 1113/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1155.7297 - val_loss: 1544.9158\n",
            "Epoch 1114/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1155.6363 - val_loss: 1544.7310\n",
            "Epoch 1115/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1155.5656 - val_loss: 1544.5304\n",
            "Epoch 1116/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1155.4969 - val_loss: 1544.3361\n",
            "Epoch 1117/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1155.4114 - val_loss: 1544.1704\n",
            "Epoch 1118/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1155.3488 - val_loss: 1544.0026\n",
            "Epoch 1119/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1155.2653 - val_loss: 1543.8585\n",
            "Epoch 1120/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1155.1591 - val_loss: 1543.6815\n",
            "Epoch 1121/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1155.0856 - val_loss: 1543.5155\n",
            "Epoch 1122/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1155.0061 - val_loss: 1543.3468\n",
            "Epoch 1123/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1154.9248 - val_loss: 1543.1897\n",
            "Epoch 1124/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.8619 - val_loss: 1543.0023\n",
            "Epoch 1125/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1154.7976 - val_loss: 1542.8521\n",
            "Epoch 1126/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1154.7115 - val_loss: 1542.6527\n",
            "Epoch 1127/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.6247 - val_loss: 1542.4740\n",
            "Epoch 1128/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1154.5501 - val_loss: 1542.3362\n",
            "Epoch 1129/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.4575 - val_loss: 1542.1573\n",
            "Epoch 1130/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.3704 - val_loss: 1542.0029\n",
            "Epoch 1131/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1154.2803 - val_loss: 1541.8688\n",
            "Epoch 1132/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1154.1937 - val_loss: 1541.7345\n",
            "Epoch 1133/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.1177 - val_loss: 1541.5995\n",
            "Epoch 1134/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1154.0440 - val_loss: 1541.4492\n",
            "Epoch 1135/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1153.9593 - val_loss: 1541.3049\n",
            "Epoch 1136/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1153.9118 - val_loss: 1541.1523\n",
            "Epoch 1137/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1153.7949 - val_loss: 1540.9835\n",
            "Epoch 1138/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1153.7198 - val_loss: 1540.8263\n",
            "Epoch 1139/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1153.6308 - val_loss: 1540.6487\n",
            "Epoch 1140/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1153.5610 - val_loss: 1540.4835\n",
            "Epoch 1141/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1153.4845 - val_loss: 1540.3096\n",
            "Epoch 1142/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1153.4217 - val_loss: 1540.1082\n",
            "Epoch 1143/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1153.3042 - val_loss: 1539.9215\n",
            "Epoch 1144/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1153.2631 - val_loss: 1539.7207\n",
            "Epoch 1145/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1153.1560 - val_loss: 1539.5660\n",
            "Epoch 1146/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1153.0706 - val_loss: 1539.3928\n",
            "Epoch 1147/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1153.0312 - val_loss: 1539.2552\n",
            "Epoch 1148/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1152.9122 - val_loss: 1539.0763\n",
            "Epoch 1149/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1152.8280 - val_loss: 1538.8971\n",
            "Epoch 1150/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1152.7509 - val_loss: 1538.7115\n",
            "Epoch 1151/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1152.6713 - val_loss: 1538.5237\n",
            "Epoch 1152/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1152.5800 - val_loss: 1538.3544\n",
            "Epoch 1153/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1152.5119 - val_loss: 1538.1700\n",
            "Epoch 1154/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 27us/sample - loss: 1152.4532 - val_loss: 1537.9692\n",
            "Epoch 1155/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1152.3609 - val_loss: 1537.7847\n",
            "Epoch 1156/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1152.2836 - val_loss: 1537.6381\n",
            "Epoch 1157/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1152.1796 - val_loss: 1537.4825\n",
            "Epoch 1158/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1152.1132 - val_loss: 1537.3181\n",
            "Epoch 1159/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1152.0301 - val_loss: 1537.1174\n",
            "Epoch 1160/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1151.9481 - val_loss: 1536.9313\n",
            "Epoch 1161/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1151.8637 - val_loss: 1536.7703\n",
            "Epoch 1162/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1151.7778 - val_loss: 1536.5862\n",
            "Epoch 1163/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1151.8671 - val_loss: 1536.4702\n",
            "Epoch 1164/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1151.6163 - val_loss: 1536.2588\n",
            "Epoch 1165/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1151.5275 - val_loss: 1536.0643\n",
            "Epoch 1166/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1151.4455 - val_loss: 1535.8834\n",
            "Epoch 1167/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1151.3828 - val_loss: 1535.6688\n",
            "Epoch 1168/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1151.2767 - val_loss: 1535.4939\n",
            "Epoch 1169/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1151.2363 - val_loss: 1535.2861\n",
            "Epoch 1170/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1151.1573 - val_loss: 1535.0844\n",
            "Epoch 1171/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1151.0339 - val_loss: 1534.9231\n",
            "Epoch 1172/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1150.9442 - val_loss: 1534.7748\n",
            "Epoch 1173/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1150.8608 - val_loss: 1534.6467\n",
            "Epoch 1174/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1150.8204 - val_loss: 1534.5206\n",
            "Epoch 1175/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1150.7062 - val_loss: 1534.3723\n",
            "Epoch 1176/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1150.6284 - val_loss: 1534.2085\n",
            "Epoch 1177/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1150.6614 - val_loss: 1534.1277\n",
            "Epoch 1178/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1150.4504 - val_loss: 1533.9768\n",
            "Epoch 1179/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1150.3631 - val_loss: 1533.8140\n",
            "Epoch 1180/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1150.3098 - val_loss: 1533.6426\n",
            "Epoch 1181/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1150.2152 - val_loss: 1533.4709\n",
            "Epoch 1182/5000\n",
            "353/353 [==============================] - 0s 24us/sample - loss: 1150.1323 - val_loss: 1533.2627\n",
            "Epoch 1183/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1150.0779 - val_loss: 1533.0952\n",
            "Epoch 1184/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1149.9568 - val_loss: 1532.9043\n",
            "Epoch 1185/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1149.8758 - val_loss: 1532.6948\n",
            "Epoch 1186/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1149.8091 - val_loss: 1532.4950\n",
            "Epoch 1187/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1149.7086 - val_loss: 1532.3143\n",
            "Epoch 1188/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1149.6421 - val_loss: 1532.1162\n",
            "Epoch 1189/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1149.5643 - val_loss: 1531.8937\n",
            "Epoch 1190/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1149.4834 - val_loss: 1531.7032\n",
            "Epoch 1191/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1149.4124 - val_loss: 1531.5029\n",
            "Epoch 1192/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1149.3666 - val_loss: 1531.3500\n",
            "Epoch 1193/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1149.2310 - val_loss: 1531.1533\n",
            "Epoch 1194/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1149.1508 - val_loss: 1530.9585\n",
            "Epoch 1195/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1149.0703 - val_loss: 1530.7771\n",
            "Epoch 1196/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1148.9980 - val_loss: 1530.5906\n",
            "Epoch 1197/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1148.9141 - val_loss: 1530.4321\n",
            "Epoch 1198/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1148.8349 - val_loss: 1530.2767\n",
            "Epoch 1199/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1148.7706 - val_loss: 1530.0797\n",
            "Epoch 1200/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1148.6721 - val_loss: 1529.8987\n",
            "Epoch 1201/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1148.5819 - val_loss: 1529.7417\n",
            "Epoch 1202/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1148.5043 - val_loss: 1529.5813\n",
            "Epoch 1203/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1148.4638 - val_loss: 1529.4451\n",
            "Epoch 1204/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1148.3399 - val_loss: 1529.2592\n",
            "Epoch 1205/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1148.2514 - val_loss: 1529.0780\n",
            "Epoch 1206/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1148.1716 - val_loss: 1528.8949\n",
            "Epoch 1207/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1148.0812 - val_loss: 1528.7200\n",
            "Epoch 1208/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1148.0683 - val_loss: 1528.5243\n",
            "Epoch 1209/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1147.9762 - val_loss: 1528.4283\n",
            "Epoch 1210/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1147.8493 - val_loss: 1528.2748\n",
            "Epoch 1211/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1147.7733 - val_loss: 1528.1276\n",
            "Epoch 1212/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1147.6629 - val_loss: 1527.9607\n",
            "Epoch 1213/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1147.6415 - val_loss: 1527.8220\n",
            "Epoch 1214/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1147.5029 - val_loss: 1527.6456\n",
            "Epoch 1215/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1147.4206 - val_loss: 1527.4424\n",
            "Epoch 1216/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1147.3593 - val_loss: 1527.2375\n",
            "Epoch 1217/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1147.2522 - val_loss: 1527.0657\n",
            "Epoch 1218/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1147.1762 - val_loss: 1526.8811\n",
            "Epoch 1219/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1147.1213 - val_loss: 1526.7485\n",
            "Epoch 1220/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1147.0634 - val_loss: 1526.5240\n",
            "Epoch 1221/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1146.9253 - val_loss: 1526.3564\n",
            "Epoch 1222/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1146.8426 - val_loss: 1526.1971\n",
            "Epoch 1223/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1146.7505 - val_loss: 1526.0447\n",
            "Epoch 1224/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1146.6699 - val_loss: 1525.8944\n",
            "Epoch 1225/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1146.5895 - val_loss: 1525.7263\n",
            "Epoch 1226/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1146.4909 - val_loss: 1525.5781\n",
            "Epoch 1227/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1146.4568 - val_loss: 1525.4691\n",
            "Epoch 1228/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1146.3422 - val_loss: 1525.3206\n",
            "Epoch 1229/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1146.2522 - val_loss: 1525.1753\n",
            "Epoch 1230/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1146.1682 - val_loss: 1525.0144\n",
            "Epoch 1231/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1146.1018 - val_loss: 1524.8667\n",
            "Epoch 1232/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1146.0240 - val_loss: 1524.6681\n",
            "Epoch 1233/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1145.9185 - val_loss: 1524.5240\n",
            "Epoch 1234/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1145.8376 - val_loss: 1524.3881\n",
            "Epoch 1235/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1145.7915 - val_loss: 1524.1781\n",
            "Epoch 1236/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1145.6988 - val_loss: 1524.0647\n",
            "Epoch 1237/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1145.6068 - val_loss: 1523.8837\n",
            "Epoch 1238/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1145.5039 - val_loss: 1523.7047\n",
            "Epoch 1239/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1145.4180 - val_loss: 1523.5061\n",
            "Epoch 1240/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1145.3635 - val_loss: 1523.3469\n",
            "Epoch 1241/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1145.2558 - val_loss: 1523.1509\n",
            "Epoch 1242/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1145.1852 - val_loss: 1522.9711\n",
            "Epoch 1243/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1145.0982 - val_loss: 1522.8073\n",
            "Epoch 1244/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1145.0008 - val_loss: 1522.6067\n",
            "Epoch 1245/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1144.9463 - val_loss: 1522.3734\n",
            "Epoch 1246/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1144.8879 - val_loss: 1522.2245\n",
            "Epoch 1247/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1144.9153 - val_loss: 1521.9414\n",
            "Epoch 1248/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1144.6635 - val_loss: 1521.7861\n",
            "Epoch 1249/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1144.6055 - val_loss: 1521.6514\n",
            "Epoch 1250/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1144.5479 - val_loss: 1521.5022\n",
            "Epoch 1251/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1144.4434 - val_loss: 1521.3049\n",
            "Epoch 1252/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1144.3494 - val_loss: 1521.1517\n",
            "Epoch 1253/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1144.2648 - val_loss: 1520.9417\n",
            "Epoch 1254/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1144.1761 - val_loss: 1520.7504\n",
            "Epoch 1255/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1144.0776 - val_loss: 1520.5941\n",
            "Epoch 1256/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1143.9975 - val_loss: 1520.4238\n",
            "Epoch 1257/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1143.9112 - val_loss: 1520.2764\n",
            "Epoch 1258/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1143.8404 - val_loss: 1520.1320\n",
            "Epoch 1259/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1143.7504 - val_loss: 1519.9691\n",
            "Epoch 1260/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1143.6840 - val_loss: 1519.7797\n",
            "Epoch 1261/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1143.6227 - val_loss: 1519.5620\n",
            "Epoch 1262/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1143.5226 - val_loss: 1519.3757\n",
            "Epoch 1263/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1143.4107 - val_loss: 1519.2271\n",
            "Epoch 1264/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1143.3491 - val_loss: 1519.1117\n",
            "Epoch 1265/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1143.2768 - val_loss: 1518.9875\n",
            "Epoch 1266/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1143.1713 - val_loss: 1518.7920\n",
            "Epoch 1267/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1143.1323 - val_loss: 1518.6462\n",
            "Epoch 1268/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1143.0197 - val_loss: 1518.4176\n",
            "Epoch 1269/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1142.9333 - val_loss: 1518.2249\n",
            "Epoch 1270/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1142.8304 - val_loss: 1518.0514\n",
            "Epoch 1271/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1142.7364 - val_loss: 1517.9088\n",
            "Epoch 1272/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1142.6712 - val_loss: 1517.7473\n",
            "Epoch 1273/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1142.5886 - val_loss: 1517.5656\n",
            "Epoch 1274/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1142.5039 - val_loss: 1517.3639\n",
            "Epoch 1275/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1142.4071 - val_loss: 1517.1383\n",
            "Epoch 1276/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1142.3235 - val_loss: 1516.9376\n",
            "Epoch 1277/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1142.2330 - val_loss: 1516.7339\n",
            "Epoch 1278/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1142.1393 - val_loss: 1516.5176\n",
            "Epoch 1279/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1142.0759 - val_loss: 1516.3151\n",
            "Epoch 1280/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.9811 - val_loss: 1516.0977\n",
            "Epoch 1281/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1141.8939 - val_loss: 1515.8779\n",
            "Epoch 1282/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1141.8198 - val_loss: 1515.6693\n",
            "Epoch 1283/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.7348 - val_loss: 1515.4805\n",
            "Epoch 1284/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1141.6503 - val_loss: 1515.3125\n",
            "Epoch 1285/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.5692 - val_loss: 1515.1304\n",
            "Epoch 1286/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.5354 - val_loss: 1514.9102\n",
            "Epoch 1287/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1141.4130 - val_loss: 1514.7661\n",
            "Epoch 1288/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1141.3505 - val_loss: 1514.6456\n",
            "Epoch 1289/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1141.2231 - val_loss: 1514.4587\n",
            "Epoch 1290/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1141.1531 - val_loss: 1514.3041\n",
            "Epoch 1291/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.0770 - val_loss: 1514.1078\n",
            "Epoch 1292/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1141.0159 - val_loss: 1513.9530\n",
            "Epoch 1293/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1140.8865 - val_loss: 1513.7465\n",
            "Epoch 1294/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1140.8172 - val_loss: 1513.5535\n",
            "Epoch 1295/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1140.7153 - val_loss: 1513.3890\n",
            "Epoch 1296/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1140.6703 - val_loss: 1513.2825\n",
            "Epoch 1297/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1140.5946 - val_loss: 1513.1611\n",
            "Epoch 1298/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1140.4701 - val_loss: 1512.9784\n",
            "Epoch 1299/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1140.4212 - val_loss: 1512.8046\n",
            "Epoch 1300/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1140.3281 - val_loss: 1512.5361\n",
            "Epoch 1301/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1140.2369 - val_loss: 1512.3765\n",
            "Epoch 1302/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1140.1480 - val_loss: 1512.1255\n",
            "Epoch 1303/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1140.0820 - val_loss: 1511.8997\n",
            "Epoch 1304/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1139.9648 - val_loss: 1511.7294\n",
            "Epoch 1305/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1139.9068 - val_loss: 1511.5896\n",
            "Epoch 1306/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1139.8130 - val_loss: 1511.4093\n",
            "Epoch 1307/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1139.7115 - val_loss: 1511.2644\n",
            "Epoch 1308/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1139.6288 - val_loss: 1511.1034\n",
            "Epoch 1309/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1139.5415 - val_loss: 1510.9666\n",
            "Epoch 1310/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1139.4586 - val_loss: 1510.8636\n",
            "Epoch 1311/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1139.3635 - val_loss: 1510.7495\n",
            "Epoch 1312/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1139.3540 - val_loss: 1510.6758\n",
            "Epoch 1313/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1139.2030 - val_loss: 1510.5267\n",
            "Epoch 1314/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1139.1284 - val_loss: 1510.3436\n",
            "Epoch 1315/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1139.0650 - val_loss: 1510.2096\n",
            "Epoch 1316/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1138.9690 - val_loss: 1510.0018\n",
            "Epoch 1317/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1138.8656 - val_loss: 1509.8416\n",
            "Epoch 1318/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1138.8706 - val_loss: 1509.7507\n",
            "Epoch 1319/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1138.6976 - val_loss: 1509.5214\n",
            "Epoch 1320/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1138.6618 - val_loss: 1509.3730\n",
            "Epoch 1321/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1138.5604 - val_loss: 1509.0884\n",
            "Epoch 1322/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1138.4400 - val_loss: 1508.8705\n",
            "Epoch 1323/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1138.4110 - val_loss: 1508.6135\n",
            "Epoch 1324/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1138.2834 - val_loss: 1508.4279\n",
            "Epoch 1325/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1138.1909 - val_loss: 1508.2546\n",
            "Epoch 1326/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1138.1199 - val_loss: 1508.0908\n",
            "Epoch 1327/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1138.0303 - val_loss: 1507.8743\n",
            "Epoch 1328/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1137.9775 - val_loss: 1507.7379\n",
            "Epoch 1329/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1137.8643 - val_loss: 1507.5668\n",
            "Epoch 1330/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1137.8078 - val_loss: 1507.3281\n",
            "Epoch 1331/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1137.6959 - val_loss: 1507.1843\n",
            "Epoch 1332/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1137.6174 - val_loss: 1507.0073\n",
            "Epoch 1333/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1137.5108 - val_loss: 1506.7897\n",
            "Epoch 1334/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1137.4362 - val_loss: 1506.5869\n",
            "Epoch 1335/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1137.3489 - val_loss: 1506.3947\n",
            "Epoch 1336/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1137.3093 - val_loss: 1506.2421\n",
            "Epoch 1337/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1137.1881 - val_loss: 1506.0221\n",
            "Epoch 1338/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1137.1013 - val_loss: 1505.8252\n",
            "Epoch 1339/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1137.0273 - val_loss: 1505.6327\n",
            "Epoch 1340/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.9339 - val_loss: 1505.4635\n",
            "Epoch 1341/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1136.8726 - val_loss: 1505.3246\n",
            "Epoch 1342/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1136.8091 - val_loss: 1505.1908\n",
            "Epoch 1343/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.6756 - val_loss: 1505.0049\n",
            "Epoch 1344/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1136.6000 - val_loss: 1504.8215\n",
            "Epoch 1345/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.5223 - val_loss: 1504.6260\n",
            "Epoch 1346/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.4505 - val_loss: 1504.3772\n",
            "Epoch 1347/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.3610 - val_loss: 1504.1578\n",
            "Epoch 1348/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1136.2663 - val_loss: 1503.9421\n",
            "Epoch 1349/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1136.2376 - val_loss: 1503.6768\n",
            "Epoch 1350/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1136.1304 - val_loss: 1503.4771\n",
            "Epoch 1351/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1136.0442 - val_loss: 1503.3185\n",
            "Epoch 1352/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1135.9479 - val_loss: 1503.1569\n",
            "Epoch 1353/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1135.8688 - val_loss: 1503.0221\n",
            "Epoch 1354/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1135.8006 - val_loss: 1502.8417\n",
            "Epoch 1355/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1135.7087 - val_loss: 1502.6830\n",
            "Epoch 1356/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1135.6423 - val_loss: 1502.6085\n",
            "Epoch 1357/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1135.5176 - val_loss: 1502.4856\n",
            "Epoch 1358/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1135.4584 - val_loss: 1502.3887\n",
            "Epoch 1359/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1135.3885 - val_loss: 1502.2467\n",
            "Epoch 1360/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1135.2741 - val_loss: 1502.0701\n",
            "Epoch 1361/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1135.1924 - val_loss: 1501.8456\n",
            "Epoch 1362/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1135.1298 - val_loss: 1501.6326\n",
            "Epoch 1363/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1135.0616 - val_loss: 1501.5160\n",
            "Epoch 1364/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1134.9557 - val_loss: 1501.3667\n",
            "Epoch 1365/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1134.8810 - val_loss: 1501.1274\n",
            "Epoch 1366/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1134.7767 - val_loss: 1500.9471\n",
            "Epoch 1367/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1134.6929 - val_loss: 1500.8031\n",
            "Epoch 1368/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1134.5966 - val_loss: 1500.6118\n",
            "Epoch 1369/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1134.5214 - val_loss: 1500.4469\n",
            "Epoch 1370/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 32us/sample - loss: 1134.4402 - val_loss: 1500.2998\n",
            "Epoch 1371/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1134.3791 - val_loss: 1500.0964\n",
            "Epoch 1372/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1134.2659 - val_loss: 1499.9119\n",
            "Epoch 1373/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1134.1772 - val_loss: 1499.7560\n",
            "Epoch 1374/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1134.2143 - val_loss: 1499.6746\n",
            "Epoch 1375/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1134.0067 - val_loss: 1499.4978\n",
            "Epoch 1376/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1133.9336 - val_loss: 1499.2780\n",
            "Epoch 1377/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1133.8648 - val_loss: 1499.0947\n",
            "Epoch 1378/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1133.7777 - val_loss: 1498.8933\n",
            "Epoch 1379/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1133.7148 - val_loss: 1498.6775\n",
            "Epoch 1380/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1133.6055 - val_loss: 1498.4923\n",
            "Epoch 1381/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1133.5271 - val_loss: 1498.3232\n",
            "Epoch 1382/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1133.4940 - val_loss: 1498.1954\n",
            "Epoch 1383/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1133.3860 - val_loss: 1497.9843\n",
            "Epoch 1384/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1133.2946 - val_loss: 1497.8656\n",
            "Epoch 1385/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1133.1990 - val_loss: 1497.7137\n",
            "Epoch 1386/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1133.1728 - val_loss: 1497.5490\n",
            "Epoch 1387/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 872.368 - 0s 29us/sample - loss: 1133.0622 - val_loss: 1497.3901\n",
            "Epoch 1388/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1132.9742 - val_loss: 1497.2152\n",
            "Epoch 1389/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.8888 - val_loss: 1497.0216\n",
            "Epoch 1390/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1132.8153 - val_loss: 1496.8787\n",
            "Epoch 1391/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1132.7084 - val_loss: 1496.6881\n",
            "Epoch 1392/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1132.6571 - val_loss: 1496.4509\n",
            "Epoch 1393/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.5548 - val_loss: 1496.2742\n",
            "Epoch 1394/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1132.4675 - val_loss: 1496.0942\n",
            "Epoch 1395/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.4088 - val_loss: 1495.9189\n",
            "Epoch 1396/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1132.3349 - val_loss: 1495.8013\n",
            "Epoch 1397/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1132.2062 - val_loss: 1495.6315\n",
            "Epoch 1398/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.1484 - val_loss: 1495.4884\n",
            "Epoch 1399/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.0538 - val_loss: 1495.2734\n",
            "Epoch 1400/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1132.0619 - val_loss: 1495.0239\n",
            "Epoch 1401/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1131.8916 - val_loss: 1494.9192\n",
            "Epoch 1402/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1131.8100 - val_loss: 1494.7811\n",
            "Epoch 1403/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1131.7792 - val_loss: 1494.7063\n",
            "Epoch 1404/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1131.6640 - val_loss: 1494.5110\n",
            "Epoch 1405/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1131.5482 - val_loss: 1494.3478\n",
            "Epoch 1406/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1131.4950 - val_loss: 1494.2615\n",
            "Epoch 1407/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1131.3935 - val_loss: 1494.1274\n",
            "Epoch 1408/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1131.3206 - val_loss: 1493.9440\n",
            "Epoch 1409/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1131.2247 - val_loss: 1493.7787\n",
            "Epoch 1410/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1131.1728 - val_loss: 1493.6602\n",
            "Epoch 1411/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1131.0784 - val_loss: 1493.4532\n",
            "Epoch 1412/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1130.9957 - val_loss: 1493.2808\n",
            "Epoch 1413/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1130.9066 - val_loss: 1493.0933\n",
            "Epoch 1414/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1130.8875 - val_loss: 1492.9951\n",
            "Epoch 1415/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1130.7449 - val_loss: 1492.7637\n",
            "Epoch 1416/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1130.6819 - val_loss: 1492.6224\n",
            "Epoch 1417/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1130.6388 - val_loss: 1492.3594\n",
            "Epoch 1418/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1130.5039 - val_loss: 1492.1893\n",
            "Epoch 1419/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1130.4289 - val_loss: 1492.0837\n",
            "Epoch 1420/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1130.3262 - val_loss: 1491.9701\n",
            "Epoch 1421/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1130.2500 - val_loss: 1491.7666\n",
            "Epoch 1422/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1130.1759 - val_loss: 1491.6141\n",
            "Epoch 1423/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1130.0865 - val_loss: 1491.4451\n",
            "Epoch 1424/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1129.9976 - val_loss: 1491.3011\n",
            "Epoch 1425/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1129.9459 - val_loss: 1491.1097\n",
            "Epoch 1426/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1129.8694 - val_loss: 1491.0360\n",
            "Epoch 1427/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1129.8179 - val_loss: 1490.9517\n",
            "Epoch 1428/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1129.6684 - val_loss: 1490.7610\n",
            "Epoch 1429/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1129.5978 - val_loss: 1490.5793\n",
            "Epoch 1430/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1129.5490 - val_loss: 1490.4307\n",
            "Epoch 1431/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1129.4342 - val_loss: 1490.1895\n",
            "Epoch 1432/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1129.3461 - val_loss: 1489.9856\n",
            "Epoch 1433/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1129.2658 - val_loss: 1489.7825\n",
            "Epoch 1434/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1129.2229 - val_loss: 1489.6116\n",
            "Epoch 1435/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1129.1207 - val_loss: 1489.4310\n",
            "Epoch 1436/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1129.0775 - val_loss: 1489.1631\n",
            "Epoch 1437/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1128.9639 - val_loss: 1488.9755\n",
            "Epoch 1438/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1128.8816 - val_loss: 1488.7258\n",
            "Epoch 1439/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1128.8051 - val_loss: 1488.5543\n",
            "Epoch 1440/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1128.7059 - val_loss: 1488.3611\n",
            "Epoch 1441/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1128.6551 - val_loss: 1488.1251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1442/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1128.5583 - val_loss: 1487.9645\n",
            "Epoch 1443/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1128.4774 - val_loss: 1487.7944\n",
            "Epoch 1444/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1128.4046 - val_loss: 1487.5990\n",
            "Epoch 1445/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1128.3079 - val_loss: 1487.4437\n",
            "Epoch 1446/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1128.2361 - val_loss: 1487.2621\n",
            "Epoch 1447/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1128.1718 - val_loss: 1487.0868\n",
            "Epoch 1448/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1128.0994 - val_loss: 1487.0236\n",
            "Epoch 1449/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1127.9827 - val_loss: 1486.9167\n",
            "Epoch 1450/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1127.9056 - val_loss: 1486.8082\n",
            "Epoch 1451/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1127.8157 - val_loss: 1486.6506\n",
            "Epoch 1452/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1127.7748 - val_loss: 1486.5066\n",
            "Epoch 1453/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1127.6722 - val_loss: 1486.3497\n",
            "Epoch 1454/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1127.5844 - val_loss: 1486.1680\n",
            "Epoch 1455/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1127.5293 - val_loss: 1485.9540\n",
            "Epoch 1456/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1127.4214 - val_loss: 1485.8091\n",
            "Epoch 1457/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1127.3496 - val_loss: 1485.7004\n",
            "Epoch 1458/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1127.2796 - val_loss: 1485.5552\n",
            "Epoch 1459/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1127.2060 - val_loss: 1485.4243\n",
            "Epoch 1460/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1127.1145 - val_loss: 1485.2312\n",
            "Epoch 1461/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1127.0734 - val_loss: 1485.1156\n",
            "Epoch 1462/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1126.9483 - val_loss: 1484.8943\n",
            "Epoch 1463/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1126.9081 - val_loss: 1484.7649\n",
            "Epoch 1464/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1126.8000 - val_loss: 1484.5421\n",
            "Epoch 1465/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1126.8156 - val_loss: 1484.2488\n",
            "Epoch 1466/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1126.6853 - val_loss: 1484.0894\n",
            "Epoch 1467/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1126.5742 - val_loss: 1484.0004\n",
            "Epoch 1468/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1126.6197 - val_loss: 1484.0325\n",
            "Epoch 1469/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1126.4062 - val_loss: 1483.8802\n",
            "Epoch 1470/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1126.3469 - val_loss: 1483.7759\n",
            "Epoch 1471/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1126.2622 - val_loss: 1483.5529\n",
            "Epoch 1472/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1126.1719 - val_loss: 1483.4027\n",
            "Epoch 1473/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1126.0863 - val_loss: 1483.2573\n",
            "Epoch 1474/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1126.0620 - val_loss: 1483.1819\n",
            "Epoch 1475/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1125.9490 - val_loss: 1483.0408\n",
            "Epoch 1476/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1125.8425 - val_loss: 1482.8636\n",
            "Epoch 1477/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1125.7809 - val_loss: 1482.7097\n",
            "Epoch 1478/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1125.7266 - val_loss: 1482.4535\n",
            "Epoch 1479/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1125.6462 - val_loss: 1482.2356\n",
            "Epoch 1480/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1125.5757 - val_loss: 1482.0344\n",
            "Epoch 1481/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1125.4539 - val_loss: 1481.9633\n",
            "Epoch 1482/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1125.3993 - val_loss: 1481.8226\n",
            "Epoch 1483/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1125.3500 - val_loss: 1481.7968\n",
            "Epoch 1484/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1125.2099 - val_loss: 1481.6708\n",
            "Epoch 1485/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1125.1404 - val_loss: 1481.5187\n",
            "Epoch 1486/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1125.0632 - val_loss: 1481.3850\n",
            "Epoch 1487/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1125.0093 - val_loss: 1481.2616\n",
            "Epoch 1488/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1124.9807 - val_loss: 1480.9673\n",
            "Epoch 1489/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1124.8325 - val_loss: 1480.7676\n",
            "Epoch 1490/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1124.7314 - val_loss: 1480.6440\n",
            "Epoch 1491/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1124.7928 - val_loss: 1480.6111\n",
            "Epoch 1492/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1124.5792 - val_loss: 1480.3768\n",
            "Epoch 1493/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1124.4997 - val_loss: 1480.1434\n",
            "Epoch 1494/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1124.4254 - val_loss: 1479.9242\n",
            "Epoch 1495/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1124.3763 - val_loss: 1479.6722\n",
            "Epoch 1496/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1124.2739 - val_loss: 1479.4294\n",
            "Epoch 1497/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1124.2830 - val_loss: 1479.1350\n",
            "Epoch 1498/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1124.1807 - val_loss: 1479.0674\n",
            "Epoch 1499/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1124.0442 - val_loss: 1478.8513\n",
            "Epoch 1500/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1123.9786 - val_loss: 1478.7209\n",
            "Epoch 1501/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1123.8969 - val_loss: 1478.5247\n",
            "Epoch 1502/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1123.8293 - val_loss: 1478.4039\n",
            "Epoch 1503/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1123.7442 - val_loss: 1478.2212\n",
            "Epoch 1504/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1123.6856 - val_loss: 1478.1490\n",
            "Epoch 1505/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1123.5947 - val_loss: 1477.9631\n",
            "Epoch 1506/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1123.5101 - val_loss: 1477.8536\n",
            "Epoch 1507/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1123.4207 - val_loss: 1477.7678\n",
            "Epoch 1508/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1123.3435 - val_loss: 1477.6172\n",
            "Epoch 1509/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1123.2654 - val_loss: 1477.4816\n",
            "Epoch 1510/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1123.1953 - val_loss: 1477.3041\n",
            "Epoch 1511/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1123.1172 - val_loss: 1477.1323\n",
            "Epoch 1512/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1123.0431 - val_loss: 1477.0264\n",
            "Epoch 1513/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1122.9909 - val_loss: 1476.9237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1514/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.8808 - val_loss: 1476.7422\n",
            "Epoch 1515/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.8004 - val_loss: 1476.5443\n",
            "Epoch 1516/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.7185 - val_loss: 1476.3198\n",
            "Epoch 1517/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1122.6881 - val_loss: 1476.1592\n",
            "Epoch 1518/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.5793 - val_loss: 1475.8805\n",
            "Epoch 1519/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1122.5508 - val_loss: 1475.5942\n",
            "Epoch 1520/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1122.4992 - val_loss: 1475.5347\n",
            "Epoch 1521/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1122.3342 - val_loss: 1475.3612\n",
            "Epoch 1522/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1122.2716 - val_loss: 1475.1555\n",
            "Epoch 1523/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.1813 - val_loss: 1474.9658\n",
            "Epoch 1524/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1122.1071 - val_loss: 1474.8280\n",
            "Epoch 1525/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1122.0319 - val_loss: 1474.6643\n",
            "Epoch 1526/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1121.9844 - val_loss: 1474.5906\n",
            "Epoch 1527/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.8914 - val_loss: 1474.4456\n",
            "Epoch 1528/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1121.8160 - val_loss: 1474.2964\n",
            "Epoch 1529/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1121.8325 - val_loss: 1473.9730\n",
            "Epoch 1530/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1121.6312 - val_loss: 1473.8429\n",
            "Epoch 1531/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1121.5579 - val_loss: 1473.7063\n",
            "Epoch 1532/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.5314 - val_loss: 1473.6486\n",
            "Epoch 1533/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1121.4188 - val_loss: 1473.5573\n",
            "Epoch 1534/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.3653 - val_loss: 1473.3362\n",
            "Epoch 1535/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.2701 - val_loss: 1473.2629\n",
            "Epoch 1536/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.2175 - val_loss: 1473.2030\n",
            "Epoch 1537/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.1046 - val_loss: 1473.0304\n",
            "Epoch 1538/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1121.0197 - val_loss: 1472.9089\n",
            "Epoch 1539/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1120.9702 - val_loss: 1472.7195\n",
            "Epoch 1540/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1120.9363 - val_loss: 1472.7173\n",
            "Epoch 1541/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1120.7987 - val_loss: 1472.6093\n",
            "Epoch 1542/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1120.7239 - val_loss: 1472.4111\n",
            "Epoch 1543/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1120.6451 - val_loss: 1472.2539\n",
            "Epoch 1544/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1120.5702 - val_loss: 1472.0569\n",
            "Epoch 1545/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1120.4885 - val_loss: 1471.8673\n",
            "Epoch 1546/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1120.4158 - val_loss: 1471.7505\n",
            "Epoch 1547/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1120.3181 - val_loss: 1471.6144\n",
            "Epoch 1548/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1120.2666 - val_loss: 1471.4301\n",
            "Epoch 1549/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1120.2134 - val_loss: 1471.3254\n",
            "Epoch 1550/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1120.0885 - val_loss: 1471.1583\n",
            "Epoch 1551/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1120.0720 - val_loss: 1471.0120\n",
            "Epoch 1552/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1119.9498 - val_loss: 1470.8457\n",
            "Epoch 1553/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1119.9165 - val_loss: 1470.5717\n",
            "Epoch 1554/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1119.8598 - val_loss: 1470.4753\n",
            "Epoch 1555/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1119.7345 - val_loss: 1470.2942\n",
            "Epoch 1556/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1119.6496 - val_loss: 1470.0490\n",
            "Epoch 1557/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1167.21 - 0s 29us/sample - loss: 1119.5503 - val_loss: 1469.8848\n",
            "Epoch 1558/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1119.4984 - val_loss: 1469.7488\n",
            "Epoch 1559/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1119.4103 - val_loss: 1469.5144\n",
            "Epoch 1560/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1119.3464 - val_loss: 1469.3043\n",
            "Epoch 1561/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1119.2811 - val_loss: 1469.1823\n",
            "Epoch 1562/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1119.2279 - val_loss: 1468.8781\n",
            "Epoch 1563/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1119.1213 - val_loss: 1468.7174\n",
            "Epoch 1564/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1119.0352 - val_loss: 1468.5417\n",
            "Epoch 1565/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1119.0508 - val_loss: 1468.4993\n",
            "Epoch 1566/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1118.8985 - val_loss: 1468.2108\n",
            "Epoch 1567/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1118.7987 - val_loss: 1467.9996\n",
            "Epoch 1568/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1118.7359 - val_loss: 1467.8595\n",
            "Epoch 1569/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1118.6508 - val_loss: 1467.6808\n",
            "Epoch 1570/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1118.5883 - val_loss: 1467.5563\n",
            "Epoch 1571/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1118.5056 - val_loss: 1467.3783\n",
            "Epoch 1572/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1118.4254 - val_loss: 1467.1971\n",
            "Epoch 1573/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1118.3436 - val_loss: 1467.0580\n",
            "Epoch 1574/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1118.3099 - val_loss: 1466.9634\n",
            "Epoch 1575/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1118.2269 - val_loss: 1466.7339\n",
            "Epoch 1576/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1118.1550 - val_loss: 1466.6517\n",
            "Epoch 1577/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1118.0585 - val_loss: 1466.4325\n",
            "Epoch 1578/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1117.9802 - val_loss: 1466.2610\n",
            "Epoch 1579/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1117.9231 - val_loss: 1466.1104\n",
            "Epoch 1580/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1117.8347 - val_loss: 1466.0046\n",
            "Epoch 1581/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1117.7982 - val_loss: 1465.9493\n",
            "Epoch 1582/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1117.6834 - val_loss: 1465.7433\n",
            "Epoch 1583/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1117.6021 - val_loss: 1465.5416\n",
            "Epoch 1584/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1117.5822 - val_loss: 1465.4673\n",
            "Epoch 1585/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 27us/sample - loss: 1117.4432 - val_loss: 1465.2710\n",
            "Epoch 1586/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1117.4663 - val_loss: 1465.1720\n",
            "Epoch 1587/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1117.2970 - val_loss: 1464.9425\n",
            "Epoch 1588/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1117.2758 - val_loss: 1464.6378\n",
            "Epoch 1589/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1117.1797 - val_loss: 1464.3566\n",
            "Epoch 1590/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1117.1046 - val_loss: 1464.1177\n",
            "Epoch 1591/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1117.0366 - val_loss: 1463.9659\n",
            "Epoch 1592/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1116.9685 - val_loss: 1463.8074\n",
            "Epoch 1593/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1116.8862 - val_loss: 1463.6884\n",
            "Epoch 1594/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1116.8271 - val_loss: 1463.5437\n",
            "Epoch 1595/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1116.8213 - val_loss: 1463.5548\n",
            "Epoch 1596/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1116.6591 - val_loss: 1463.4081\n",
            "Epoch 1597/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1116.5887 - val_loss: 1463.3270\n",
            "Epoch 1598/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1116.5078 - val_loss: 1463.2590\n",
            "Epoch 1599/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1116.4441 - val_loss: 1463.1615\n",
            "Epoch 1600/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1116.3647 - val_loss: 1463.0499\n",
            "Epoch 1601/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1116.2886 - val_loss: 1462.9513\n",
            "Epoch 1602/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1116.2304 - val_loss: 1462.8263\n",
            "Epoch 1603/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1116.1517 - val_loss: 1462.6272\n",
            "Epoch 1604/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1116.1228 - val_loss: 1462.4427\n",
            "Epoch 1605/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1115.9806 - val_loss: 1462.3785\n",
            "Epoch 1606/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1115.9304 - val_loss: 1462.2283\n",
            "Epoch 1607/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1115.8587 - val_loss: 1462.1733\n",
            "Epoch 1608/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1115.7621 - val_loss: 1462.0708\n",
            "Epoch 1609/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1115.7044 - val_loss: 1461.9795\n",
            "Epoch 1610/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1115.6910 - val_loss: 1461.7146\n",
            "Epoch 1611/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1115.5403 - val_loss: 1461.5829\n",
            "Epoch 1612/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1115.4604 - val_loss: 1461.4666\n",
            "Epoch 1613/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1115.4483 - val_loss: 1461.4669\n",
            "Epoch 1614/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1115.3304 - val_loss: 1461.2761\n",
            "Epoch 1615/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1115.2540 - val_loss: 1461.1157\n",
            "Epoch 1616/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1115.1541 - val_loss: 1461.0459\n",
            "Epoch 1617/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1115.1158 - val_loss: 1461.0057\n",
            "Epoch 1618/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1115.0687 - val_loss: 1460.7756\n",
            "Epoch 1619/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1114.9464 - val_loss: 1460.6514\n",
            "Epoch 1620/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1114.9036 - val_loss: 1460.6041\n",
            "Epoch 1621/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1114.8137 - val_loss: 1460.3776\n",
            "Epoch 1622/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1114.8097 - val_loss: 1460.3542\n",
            "Epoch 1623/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1114.6562 - val_loss: 1460.1598\n",
            "Epoch 1624/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1114.6266 - val_loss: 1459.8320\n",
            "Epoch 1625/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1114.5092 - val_loss: 1459.5953\n",
            "Epoch 1626/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1114.4730 - val_loss: 1459.3232\n",
            "Epoch 1627/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1114.3917 - val_loss: 1459.2074\n",
            "Epoch 1628/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1114.2811 - val_loss: 1459.0056\n",
            "Epoch 1629/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1114.2358 - val_loss: 1458.7834\n",
            "Epoch 1630/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1114.1612 - val_loss: 1458.6836\n",
            "Epoch 1631/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1114.0861 - val_loss: 1458.5302\n",
            "Epoch 1632/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1114.0437 - val_loss: 1458.3075\n",
            "Epoch 1633/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1113.9514 - val_loss: 1458.2729\n",
            "Epoch 1634/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1113.9152 - val_loss: 1458.2242\n",
            "Epoch 1635/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1113.7854 - val_loss: 1458.0455\n",
            "Epoch 1636/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1113.7329 - val_loss: 1457.8774\n",
            "Epoch 1637/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1113.6740 - val_loss: 1457.5603\n",
            "Epoch 1638/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1113.5929 - val_loss: 1457.2906\n",
            "Epoch 1639/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1113.5490 - val_loss: 1457.0601\n",
            "Epoch 1640/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1113.5319 - val_loss: 1456.8376\n",
            "Epoch 1641/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1113.3523 - val_loss: 1456.8171\n",
            "Epoch 1642/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1113.3090 - val_loss: 1456.8560\n",
            "Epoch 1643/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1113.2136 - val_loss: 1456.8595\n",
            "Epoch 1644/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1113.1374 - val_loss: 1456.7939\n",
            "Epoch 1645/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1113.0890 - val_loss: 1456.7247\n",
            "Epoch 1646/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1113.0134 - val_loss: 1456.6411\n",
            "Epoch 1647/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1112.9270 - val_loss: 1456.5184\n",
            "Epoch 1648/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1112.8642 - val_loss: 1456.3820\n",
            "Epoch 1649/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.7928 - val_loss: 1456.1952\n",
            "Epoch 1650/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.7362 - val_loss: 1456.0780\n",
            "Epoch 1651/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.7105 - val_loss: 1455.7678\n",
            "Epoch 1652/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1112.7630 - val_loss: 1455.4551\n",
            "Epoch 1653/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1112.4987 - val_loss: 1455.4631\n",
            "Epoch 1654/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1112.4606 - val_loss: 1455.5026\n",
            "Epoch 1655/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.3871 - val_loss: 1455.4827\n",
            "Epoch 1656/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.3338 - val_loss: 1455.4413\n",
            "Epoch 1657/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1112.2334 - val_loss: 1455.2196\n",
            "Epoch 1658/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1112.1483 - val_loss: 1455.0791\n",
            "Epoch 1659/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1112.0922 - val_loss: 1454.9241\n",
            "Epoch 1660/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1112.0282 - val_loss: 1454.7573\n",
            "Epoch 1661/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1111.9601 - val_loss: 1454.7408\n",
            "Epoch 1662/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1111.9626 - val_loss: 1454.7296\n",
            "Epoch 1663/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1111.8064 - val_loss: 1454.4672\n",
            "Epoch 1664/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1111.7770 - val_loss: 1454.1628\n",
            "Epoch 1665/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1111.6759 - val_loss: 1453.9729\n",
            "Epoch 1666/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1111.6474 - val_loss: 1453.9381\n",
            "Epoch 1667/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1111.5628 - val_loss: 1453.6937\n",
            "Epoch 1668/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1111.5310 - val_loss: 1453.6924\n",
            "Epoch 1669/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1111.3983 - val_loss: 1453.4866\n",
            "Epoch 1670/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1111.3327 - val_loss: 1453.3168\n",
            "Epoch 1671/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1111.2566 - val_loss: 1453.2504\n",
            "Epoch 1672/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1111.1726 - val_loss: 1453.1372\n",
            "Epoch 1673/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1111.1242 - val_loss: 1452.9628\n",
            "Epoch 1674/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1111.0986 - val_loss: 1452.9661\n",
            "Epoch 1675/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1110.9768 - val_loss: 1452.8146\n",
            "Epoch 1676/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1110.9128 - val_loss: 1452.6235\n",
            "Epoch 1677/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1110.8615 - val_loss: 1452.5538\n",
            "Epoch 1678/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1110.7764 - val_loss: 1452.3521\n",
            "Epoch 1679/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1110.7248 - val_loss: 1452.1792\n",
            "Epoch 1680/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1110.6667 - val_loss: 1451.7981\n",
            "Epoch 1681/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1110.5794 - val_loss: 1451.4713\n",
            "Epoch 1682/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1110.5090 - val_loss: 1451.3082\n",
            "Epoch 1683/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1110.4260 - val_loss: 1451.1101\n",
            "Epoch 1684/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1110.3545 - val_loss: 1450.9989\n",
            "Epoch 1685/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1110.3080 - val_loss: 1450.8282\n",
            "Epoch 1686/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1110.2188 - val_loss: 1450.6632\n",
            "Epoch 1687/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1110.1442 - val_loss: 1450.5361\n",
            "Epoch 1688/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1110.0884 - val_loss: 1450.4540\n",
            "Epoch 1689/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1110.0002 - val_loss: 1450.3810\n",
            "Epoch 1690/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1109.9647 - val_loss: 1450.3307\n",
            "Epoch 1691/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1109.8813 - val_loss: 1450.2314\n",
            "Epoch 1692/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1109.7968 - val_loss: 1450.0785\n",
            "Epoch 1693/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1109.7701 - val_loss: 1449.8184\n",
            "Epoch 1694/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1109.7507 - val_loss: 1449.5408\n",
            "Epoch 1695/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1109.6142 - val_loss: 1449.5283\n",
            "Epoch 1696/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1109.5602 - val_loss: 1449.3789\n",
            "Epoch 1697/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1109.4823 - val_loss: 1449.2922\n",
            "Epoch 1698/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1109.3998 - val_loss: 1449.2048\n",
            "Epoch 1699/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1109.3531 - val_loss: 1449.0890\n",
            "Epoch 1700/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1109.2585 - val_loss: 1448.8983\n",
            "Epoch 1701/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1109.2049 - val_loss: 1448.7490\n",
            "Epoch 1702/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1109.1384 - val_loss: 1448.5887\n",
            "Epoch 1703/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1109.0701 - val_loss: 1448.3448\n",
            "Epoch 1704/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1109.0255 - val_loss: 1448.0645\n",
            "Epoch 1705/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1108.9662 - val_loss: 1447.9694\n",
            "Epoch 1706/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1108.9020 - val_loss: 1447.7070\n",
            "Epoch 1707/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1108.7984 - val_loss: 1447.5820\n",
            "Epoch 1708/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1108.7373 - val_loss: 1447.5328\n",
            "Epoch 1709/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1108.7229 - val_loss: 1447.2968\n",
            "Epoch 1710/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.6177 - val_loss: 1447.1946\n",
            "Epoch 1711/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1108.5391 - val_loss: 1447.1843\n",
            "Epoch 1712/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.4765 - val_loss: 1447.0613\n",
            "Epoch 1713/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.4447 - val_loss: 1447.1564\n",
            "Epoch 1714/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.3376 - val_loss: 1447.0348\n",
            "Epoch 1715/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.2630 - val_loss: 1446.9972\n",
            "Epoch 1716/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1108.1899 - val_loss: 1446.9955\n",
            "Epoch 1717/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1108.1230 - val_loss: 1446.8693\n",
            "Epoch 1718/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1108.1182 - val_loss: 1446.8644\n",
            "Epoch 1719/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1108.0200 - val_loss: 1446.6803\n",
            "Epoch 1720/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1108.0446 - val_loss: 1446.8021\n",
            "Epoch 1721/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1107.9422 - val_loss: 1446.5009\n",
            "Epoch 1722/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1107.8149 - val_loss: 1446.3376\n",
            "Epoch 1723/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1107.7176 - val_loss: 1446.2943\n",
            "Epoch 1724/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1107.6692 - val_loss: 1446.1672\n",
            "Epoch 1725/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1107.5812 - val_loss: 1446.1160\n",
            "Epoch 1726/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1107.5322 - val_loss: 1446.0284\n",
            "Epoch 1727/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1107.4715 - val_loss: 1445.9658\n",
            "Epoch 1728/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1107.4081 - val_loss: 1445.8174\n",
            "Epoch 1729/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1107.3206 - val_loss: 1445.6959\n",
            "Epoch 1730/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1107.2706 - val_loss: 1445.5862\n",
            "Epoch 1731/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1107.2015 - val_loss: 1445.5450\n",
            "Epoch 1732/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1107.1448 - val_loss: 1445.4944\n",
            "Epoch 1733/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1107.1213 - val_loss: 1445.5542\n",
            "Epoch 1734/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1107.0730 - val_loss: 1445.5094\n",
            "Epoch 1735/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1106.9368 - val_loss: 1445.2766\n",
            "Epoch 1736/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1106.8782 - val_loss: 1444.9618\n",
            "Epoch 1737/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1106.8207 - val_loss: 1444.7687\n",
            "Epoch 1738/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1106.7537 - val_loss: 1444.4333\n",
            "Epoch 1739/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1106.6878 - val_loss: 1444.2886\n",
            "Epoch 1740/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1106.6584 - val_loss: 1443.9294\n",
            "Epoch 1741/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1106.5785 - val_loss: 1443.6401\n",
            "Epoch 1742/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1106.4988 - val_loss: 1443.6115\n",
            "Epoch 1743/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1106.4148 - val_loss: 1443.3646\n",
            "Epoch 1744/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1106.4078 - val_loss: 1443.1483\n",
            "Epoch 1745/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1106.2935 - val_loss: 1443.0352\n",
            "Epoch 1746/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1106.2069 - val_loss: 1442.8070\n",
            "Epoch 1747/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1106.1572 - val_loss: 1442.5443\n",
            "Epoch 1748/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1106.1082 - val_loss: 1442.3683\n",
            "Epoch 1749/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1106.0185 - val_loss: 1442.2673\n",
            "Epoch 1750/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1105.9464 - val_loss: 1442.1302\n",
            "Epoch 1751/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1105.9115 - val_loss: 1442.0236\n",
            "Epoch 1752/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1105.8434 - val_loss: 1441.9600\n",
            "Epoch 1753/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1105.7919 - val_loss: 1441.8986\n",
            "Epoch 1754/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1105.6967 - val_loss: 1441.7040\n",
            "Epoch 1755/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1105.6688 - val_loss: 1441.5753\n",
            "Epoch 1756/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1105.6155 - val_loss: 1441.3232\n",
            "Epoch 1757/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1105.5345 - val_loss: 1441.1891\n",
            "Epoch 1758/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1105.4758 - val_loss: 1441.2594\n",
            "Epoch 1759/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1105.4762 - val_loss: 1441.3368\n",
            "Epoch 1760/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1105.3664 - val_loss: 1441.1315\n",
            "Epoch 1761/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1105.4344 - val_loss: 1441.2345\n",
            "Epoch 1762/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1105.2357 - val_loss: 1440.9897\n",
            "Epoch 1763/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1105.1419 - val_loss: 1440.8483\n",
            "Epoch 1764/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1105.0914 - val_loss: 1440.7231\n",
            "Epoch 1765/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1105.0406 - val_loss: 1440.5519\n",
            "Epoch 1766/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1104.9642 - val_loss: 1440.4613\n",
            "Epoch 1767/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1104.9736 - val_loss: 1440.4890\n",
            "Epoch 1768/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.8696 - val_loss: 1440.2540\n",
            "Epoch 1769/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1104.7933 - val_loss: 1440.1085\n",
            "Epoch 1770/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1104.7387 - val_loss: 1440.0275\n",
            "Epoch 1771/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.7659 - val_loss: 1439.8820\n",
            "Epoch 1772/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.6677 - val_loss: 1439.6332\n",
            "Epoch 1773/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1104.5694 - val_loss: 1439.4906\n",
            "Epoch 1774/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.5013 - val_loss: 1439.3970\n",
            "Epoch 1775/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.4754 - val_loss: 1439.3597\n",
            "Epoch 1776/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1104.3912 - val_loss: 1439.2336\n",
            "Epoch 1777/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1104.3287 - val_loss: 1439.0959\n",
            "Epoch 1778/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1104.2976 - val_loss: 1438.8718\n",
            "Epoch 1779/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1104.2483 - val_loss: 1438.8286\n",
            "Epoch 1780/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1104.1527 - val_loss: 1438.7063\n",
            "Epoch 1781/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1104.1473 - val_loss: 1438.6266\n",
            "Epoch 1782/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1104.0383 - val_loss: 1438.4940\n",
            "Epoch 1783/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1103.9764 - val_loss: 1438.3690\n",
            "Epoch 1784/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1103.9800 - val_loss: 1438.3569\n",
            "Epoch 1785/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1103.8674 - val_loss: 1438.1332\n",
            "Epoch 1786/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1103.7959 - val_loss: 1437.9900\n",
            "Epoch 1787/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1103.7605 - val_loss: 1437.7888\n",
            "Epoch 1788/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1103.7634 - val_loss: 1437.6511\n",
            "Epoch 1789/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1103.6358 - val_loss: 1437.3922\n",
            "Epoch 1790/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1103.5636 - val_loss: 1437.1243\n",
            "Epoch 1791/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1103.5704 - val_loss: 1436.7921\n",
            "Epoch 1792/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1103.4925 - val_loss: 1436.7209\n",
            "Epoch 1793/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1103.4528 - val_loss: 1436.6516\n",
            "Epoch 1794/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1103.3523 - val_loss: 1436.4777\n",
            "Epoch 1795/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1103.4876 - val_loss: 1436.0919\n",
            "Epoch 1796/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1103.2477 - val_loss: 1436.0663\n",
            "Epoch 1797/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1103.2359 - val_loss: 1436.1050\n",
            "Epoch 1798/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1103.1247 - val_loss: 1436.0282\n",
            "Epoch 1799/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1103.0523 - val_loss: 1436.0361\n",
            "Epoch 1800/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1103.0167 - val_loss: 1436.0876\n",
            "Epoch 1801/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.9598 - val_loss: 1436.0289\n",
            "Epoch 1802/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.8780 - val_loss: 1436.1129\n",
            "Epoch 1803/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1102.8209 - val_loss: 1436.0927\n",
            "Epoch 1804/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.7812 - val_loss: 1436.1881\n",
            "Epoch 1805/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.7103 - val_loss: 1436.2233\n",
            "Epoch 1806/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1102.6714 - val_loss: 1436.1168\n",
            "Epoch 1807/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1102.6047 - val_loss: 1436.1049\n",
            "Epoch 1808/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1102.5564 - val_loss: 1436.1309\n",
            "Epoch 1809/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1102.4864 - val_loss: 1436.0157\n",
            "Epoch 1810/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1102.4526 - val_loss: 1435.9811\n",
            "Epoch 1811/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.3819 - val_loss: 1435.8430\n",
            "Epoch 1812/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1102.3403 - val_loss: 1435.7439\n",
            "Epoch 1813/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1102.2730 - val_loss: 1435.5597\n",
            "Epoch 1814/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1102.2468 - val_loss: 1435.3439\n",
            "Epoch 1815/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1102.2498 - val_loss: 1435.4087\n",
            "Epoch 1816/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1102.0991 - val_loss: 1435.1429\n",
            "Epoch 1817/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1102.0423 - val_loss: 1434.9254\n",
            "Epoch 1818/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1102.0864 - val_loss: 1434.9563\n",
            "Epoch 1819/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1101.9349 - val_loss: 1434.7400\n",
            "Epoch 1820/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1101.8979 - val_loss: 1434.4447\n",
            "Epoch 1821/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1101.8310 - val_loss: 1434.3127\n",
            "Epoch 1822/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1101.7445 - val_loss: 1434.0983\n",
            "Epoch 1823/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1101.7122 - val_loss: 1433.8578\n",
            "Epoch 1824/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1101.6681 - val_loss: 1433.7805\n",
            "Epoch 1825/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1101.5839 - val_loss: 1433.6523\n",
            "Epoch 1826/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1101.5710 - val_loss: 1433.6598\n",
            "Epoch 1827/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1101.4735 - val_loss: 1433.5024\n",
            "Epoch 1828/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1101.4689 - val_loss: 1433.3092\n",
            "Epoch 1829/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1101.4788 - val_loss: 1433.3552\n",
            "Epoch 1830/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1101.3052 - val_loss: 1433.0890\n",
            "Epoch 1831/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1101.3311 - val_loss: 1433.0167\n",
            "Epoch 1832/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1101.2913 - val_loss: 1432.5841\n",
            "Epoch 1833/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1101.1622 - val_loss: 1432.3873\n",
            "Epoch 1834/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1101.1287 - val_loss: 1432.2194\n",
            "Epoch 1835/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1101.0553 - val_loss: 1432.1714\n",
            "Epoch 1836/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1101.0121 - val_loss: 1432.1058\n",
            "Epoch 1837/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1100.9593 - val_loss: 1431.9264\n",
            "Epoch 1838/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1100.9165 - val_loss: 1431.7294\n",
            "Epoch 1839/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.8552 - val_loss: 1431.6670\n",
            "Epoch 1840/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1100.7862 - val_loss: 1431.6885\n",
            "Epoch 1841/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1100.8530 - val_loss: 1431.9297\n",
            "Epoch 1842/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1100.7275 - val_loss: 1432.0145\n",
            "Epoch 1843/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1100.6260 - val_loss: 1431.8329\n",
            "Epoch 1844/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1100.5785 - val_loss: 1431.6472\n",
            "Epoch 1845/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1100.5420 - val_loss: 1431.4465\n",
            "Epoch 1846/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1100.5259 - val_loss: 1431.2738\n",
            "Epoch 1847/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1100.3946 - val_loss: 1431.3018\n",
            "Epoch 1848/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.3838 - val_loss: 1431.4594\n",
            "Epoch 1849/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.3083 - val_loss: 1431.5297\n",
            "Epoch 1850/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1100.2358 - val_loss: 1431.5049\n",
            "Epoch 1851/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1100.2186 - val_loss: 1431.5099\n",
            "Epoch 1852/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1100.1877 - val_loss: 1431.3027\n",
            "Epoch 1853/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.0795 - val_loss: 1431.2867\n",
            "Epoch 1854/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.0495 - val_loss: 1431.1595\n",
            "Epoch 1855/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1100.0059 - val_loss: 1431.0535\n",
            "Epoch 1856/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1099.9996 - val_loss: 1431.1377\n",
            "Epoch 1857/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1099.8885 - val_loss: 1430.9023\n",
            "Epoch 1858/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1099.8565 - val_loss: 1430.7552\n",
            "Epoch 1859/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1099.7704 - val_loss: 1430.6198\n",
            "Epoch 1860/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.7183 - val_loss: 1430.4033\n",
            "Epoch 1861/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1099.6651 - val_loss: 1430.1921\n",
            "Epoch 1862/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.5999 - val_loss: 1430.0032\n",
            "Epoch 1863/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1099.5599 - val_loss: 1429.8593\n",
            "Epoch 1864/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1099.6435 - val_loss: 1429.8804\n",
            "Epoch 1865/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1099.4713 - val_loss: 1429.5540\n",
            "Epoch 1866/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.4539 - val_loss: 1429.2625\n",
            "Epoch 1867/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1099.3462 - val_loss: 1429.2058\n",
            "Epoch 1868/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1099.2855 - val_loss: 1429.1919\n",
            "Epoch 1869/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.2564 - val_loss: 1429.2358\n",
            "Epoch 1870/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.1806 - val_loss: 1429.1119\n",
            "Epoch 1871/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1099.1193 - val_loss: 1429.0298\n",
            "Epoch 1872/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1099.0882 - val_loss: 1428.9646\n",
            "Epoch 1873/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1099.0702 - val_loss: 1428.7495\n",
            "Epoch 1874/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.9614 - val_loss: 1428.6888\n",
            "Epoch 1875/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1098.9415 - val_loss: 1428.5858\n",
            "Epoch 1876/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1098.8804 - val_loss: 1428.3597\n",
            "Epoch 1877/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1098.8175 - val_loss: 1428.2882\n",
            "Epoch 1878/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1098.7614 - val_loss: 1428.2252\n",
            "Epoch 1879/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.7261 - val_loss: 1428.2648\n",
            "Epoch 1880/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.7201 - val_loss: 1428.2583\n",
            "Epoch 1881/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1098.5826 - val_loss: 1428.0997\n",
            "Epoch 1882/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1098.5567 - val_loss: 1427.9380\n",
            "Epoch 1883/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.4917 - val_loss: 1427.8031\n",
            "Epoch 1884/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1098.4414 - val_loss: 1427.6113\n",
            "Epoch 1885/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1098.4049 - val_loss: 1427.6080\n",
            "Epoch 1886/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.4504 - val_loss: 1427.6405\n",
            "Epoch 1887/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1098.2849 - val_loss: 1427.3564\n",
            "Epoch 1888/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1098.2822 - val_loss: 1426.9894\n",
            "Epoch 1889/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1098.2109 - val_loss: 1426.8142\n",
            "Epoch 1890/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.1514 - val_loss: 1426.8372\n",
            "Epoch 1891/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1098.0808 - val_loss: 1426.7957\n",
            "Epoch 1892/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1098.0439 - val_loss: 1426.6163\n",
            "Epoch 1893/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1097.9758 - val_loss: 1426.5997\n",
            "Epoch 1894/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1097.9366 - val_loss: 1426.6090\n",
            "Epoch 1895/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1097.8992 - val_loss: 1426.4297\n",
            "Epoch 1896/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1097.8235 - val_loss: 1426.3969\n",
            "Epoch 1897/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1097.7736 - val_loss: 1426.3271\n",
            "Epoch 1898/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1097.7145 - val_loss: 1426.3309\n",
            "Epoch 1899/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1097.6612 - val_loss: 1426.2634\n",
            "Epoch 1900/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1097.6052 - val_loss: 1426.2570\n",
            "Epoch 1901/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1097.6049 - val_loss: 1426.0768\n",
            "Epoch 1902/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1097.5296 - val_loss: 1426.0919\n",
            "Epoch 1903/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1097.5119 - val_loss: 1425.8906\n",
            "Epoch 1904/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1097.4995 - val_loss: 1426.0382\n",
            "Epoch 1905/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1097.3537 - val_loss: 1425.9348\n",
            "Epoch 1906/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1097.3061 - val_loss: 1425.8500\n",
            "Epoch 1907/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1097.3336 - val_loss: 1425.5479\n",
            "Epoch 1908/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1097.1986 - val_loss: 1425.4537\n",
            "Epoch 1909/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1097.1831 - val_loss: 1425.2051\n",
            "Epoch 1910/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1097.1091 - val_loss: 1425.1189\n",
            "Epoch 1911/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1097.0566 - val_loss: 1425.0334\n",
            "Epoch 1912/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1097.0147 - val_loss: 1424.8186\n",
            "Epoch 1913/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1096.9596 - val_loss: 1424.6777\n",
            "Epoch 1914/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1096.9318 - val_loss: 1424.5438\n",
            "Epoch 1915/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1096.8637 - val_loss: 1424.6218\n",
            "Epoch 1916/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1096.8109 - val_loss: 1424.7184\n",
            "Epoch 1917/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1096.7843 - val_loss: 1424.7722\n",
            "Epoch 1918/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1096.7281 - val_loss: 1424.7729\n",
            "Epoch 1919/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1096.6494 - val_loss: 1424.6764\n",
            "Epoch 1920/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1096.6203 - val_loss: 1424.4531\n",
            "Epoch 1921/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1096.5544 - val_loss: 1424.2423\n",
            "Epoch 1922/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1096.7054 - val_loss: 1423.9410\n",
            "Epoch 1923/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1096.4900 - val_loss: 1424.1943\n",
            "Epoch 1924/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1096.4250 - val_loss: 1424.1096\n",
            "Epoch 1925/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1096.3876 - val_loss: 1424.2812\n",
            "Epoch 1926/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1096.3273 - val_loss: 1424.1548\n",
            "Epoch 1927/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1096.2585 - val_loss: 1424.2128\n",
            "Epoch 1928/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1096.2483 - val_loss: 1424.2935\n",
            "Epoch 1929/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1096.1869 - val_loss: 1424.2152\n",
            "Epoch 1930/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1096.1210 - val_loss: 1424.0466\n",
            "Epoch 1931/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1096.1058 - val_loss: 1423.9818\n",
            "Epoch 1932/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1096.0429 - val_loss: 1423.6980\n",
            "Epoch 1933/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1096.1126 - val_loss: 1423.1953\n",
            "Epoch 1934/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1096.0048 - val_loss: 1422.8704\n",
            "Epoch 1935/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1095.9248 - val_loss: 1422.7234\n",
            "Epoch 1936/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1095.8463 - val_loss: 1422.8718\n",
            "Epoch 1937/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1095.8882 - val_loss: 1423.0457\n",
            "Epoch 1938/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1095.7404 - val_loss: 1423.0051\n",
            "Epoch 1939/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1095.6801 - val_loss: 1422.8019\n",
            "Epoch 1940/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1095.7633 - val_loss: 1422.8528\n",
            "Epoch 1941/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1095.6025 - val_loss: 1422.5420\n",
            "Epoch 1942/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1095.5407 - val_loss: 1422.3074\n",
            "Epoch 1943/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1095.5270 - val_loss: 1422.0222\n",
            "Epoch 1944/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1095.4492 - val_loss: 1421.9908\n",
            "Epoch 1945/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 25us/sample - loss: 1095.3969 - val_loss: 1421.9053\n",
            "Epoch 1946/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1095.3617 - val_loss: 1421.7875\n",
            "Epoch 1947/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1095.3492 - val_loss: 1421.9116\n",
            "Epoch 1948/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1095.3125 - val_loss: 1421.7108\n",
            "Epoch 1949/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1095.2282 - val_loss: 1421.7269\n",
            "Epoch 1950/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1095.1528 - val_loss: 1421.7332\n",
            "Epoch 1951/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1095.1007 - val_loss: 1421.7484\n",
            "Epoch 1952/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1095.0764 - val_loss: 1421.8766\n",
            "Epoch 1953/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1095.0232 - val_loss: 1421.9401\n",
            "Epoch 1954/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.9948 - val_loss: 1421.9747\n",
            "Epoch 1955/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.9224 - val_loss: 1421.8978\n",
            "Epoch 1956/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1094.8981 - val_loss: 1421.6921\n",
            "Epoch 1957/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.8222 - val_loss: 1421.4991\n",
            "Epoch 1958/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1094.9493 - val_loss: 1421.1891\n",
            "Epoch 1959/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1094.7377 - val_loss: 1421.1899\n",
            "Epoch 1960/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1094.7009 - val_loss: 1421.2600\n",
            "Epoch 1961/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1094.8175 - val_loss: 1421.6423\n",
            "Epoch 1962/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1094.6671 - val_loss: 1421.4520\n",
            "Epoch 1963/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1094.5726 - val_loss: 1421.5648\n",
            "Epoch 1964/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.5185 - val_loss: 1421.4908\n",
            "Epoch 1965/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1094.4901 - val_loss: 1421.3983\n",
            "Epoch 1966/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1094.4293 - val_loss: 1421.3818\n",
            "Epoch 1967/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1094.3971 - val_loss: 1421.3993\n",
            "Epoch 1968/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1094.3338 - val_loss: 1421.3236\n",
            "Epoch 1969/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1094.3326 - val_loss: 1421.3672\n",
            "Epoch 1970/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.2877 - val_loss: 1421.2891\n",
            "Epoch 1971/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1094.2087 - val_loss: 1421.0518\n",
            "Epoch 1972/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.2192 - val_loss: 1420.6355\n",
            "Epoch 1973/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1094.1757 - val_loss: 1420.2817\n",
            "Epoch 1974/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1094.1103 - val_loss: 1420.3195\n",
            "Epoch 1975/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1094.0469 - val_loss: 1420.0439\n",
            "Epoch 1976/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1094.0344 - val_loss: 1419.6714\n",
            "Epoch 1977/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1093.9442 - val_loss: 1419.5521\n",
            "Epoch 1978/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1093.8844 - val_loss: 1419.5510\n",
            "Epoch 1979/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1093.8241 - val_loss: 1419.4679\n",
            "Epoch 1980/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1093.8141 - val_loss: 1419.4631\n",
            "Epoch 1981/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1093.8759 - val_loss: 1419.1075\n",
            "Epoch 1982/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1093.7780 - val_loss: 1419.2861\n",
            "Epoch 1983/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1093.6666 - val_loss: 1419.0942\n",
            "Epoch 1984/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1093.5957 - val_loss: 1419.0148\n",
            "Epoch 1985/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1093.5941 - val_loss: 1419.0107\n",
            "Epoch 1986/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1093.5147 - val_loss: 1418.9008\n",
            "Epoch 1987/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1093.4642 - val_loss: 1418.6870\n",
            "Epoch 1988/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1093.4295 - val_loss: 1418.5443\n",
            "Epoch 1989/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1093.3867 - val_loss: 1418.3716\n",
            "Epoch 1990/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1093.3383 - val_loss: 1418.2275\n",
            "Epoch 1991/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1093.3140 - val_loss: 1418.3167\n",
            "Epoch 1992/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1093.2488 - val_loss: 1418.2013\n",
            "Epoch 1993/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1093.1890 - val_loss: 1418.1364\n",
            "Epoch 1994/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1093.1921 - val_loss: 1418.1984\n",
            "Epoch 1995/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1093.1062 - val_loss: 1418.1539\n",
            "Epoch 1996/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1093.0907 - val_loss: 1417.9020\n",
            "Epoch 1997/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1093.0124 - val_loss: 1417.7991\n",
            "Epoch 1998/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1093.0353 - val_loss: 1417.8625\n",
            "Epoch 1999/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1092.9195 - val_loss: 1417.7256\n",
            "Epoch 2000/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1092.8901 - val_loss: 1417.5526\n",
            "Epoch 2001/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1092.8636 - val_loss: 1417.5260\n",
            "Epoch 2002/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1092.8006 - val_loss: 1417.3435\n",
            "Epoch 2003/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1092.7447 - val_loss: 1417.1621\n",
            "Epoch 2004/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1092.7175 - val_loss: 1417.0299\n",
            "Epoch 2005/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1092.7356 - val_loss: 1417.0071\n",
            "Epoch 2006/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1092.7055 - val_loss: 1416.6770\n",
            "Epoch 2007/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1092.5978 - val_loss: 1416.6001\n",
            "Epoch 2008/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1092.5962 - val_loss: 1416.7260\n",
            "Epoch 2009/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1092.5011 - val_loss: 1416.7327\n",
            "Epoch 2010/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1092.4738 - val_loss: 1416.6024\n",
            "Epoch 2011/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1092.4273 - val_loss: 1416.4984\n",
            "Epoch 2012/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1092.3689 - val_loss: 1416.5011\n",
            "Epoch 2013/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1092.4177 - val_loss: 1416.6475\n",
            "Epoch 2014/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1092.2860 - val_loss: 1416.6396\n",
            "Epoch 2015/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1092.2659 - val_loss: 1416.6038\n",
            "Epoch 2016/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1092.2196 - val_loss: 1416.5018\n",
            "Epoch 2017/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1092.1764 - val_loss: 1416.2535\n",
            "Epoch 2018/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1092.1359 - val_loss: 1416.0814\n",
            "Epoch 2019/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1092.0871 - val_loss: 1416.0044\n",
            "Epoch 2020/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1092.0560 - val_loss: 1415.9980\n",
            "Epoch 2021/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1092.0121 - val_loss: 1415.8005\n",
            "Epoch 2022/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.9519 - val_loss: 1415.7135\n",
            "Epoch 2023/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1091.9363 - val_loss: 1415.5967\n",
            "Epoch 2024/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.8759 - val_loss: 1415.6436\n",
            "Epoch 2025/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1091.8419 - val_loss: 1415.5854\n",
            "Epoch 2026/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.8926 - val_loss: 1415.7704\n",
            "Epoch 2027/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1091.7839 - val_loss: 1415.5875\n",
            "Epoch 2028/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1091.7321 - val_loss: 1415.5757\n",
            "Epoch 2029/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1091.6770 - val_loss: 1415.8160\n",
            "Epoch 2030/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.6395 - val_loss: 1415.8881\n",
            "Epoch 2031/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1091.5935 - val_loss: 1415.9614\n",
            "Epoch 2032/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.5984 - val_loss: 1416.0977\n",
            "Epoch 2033/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1091.5117 - val_loss: 1416.0732\n",
            "Epoch 2034/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1091.5368 - val_loss: 1415.7637\n",
            "Epoch 2035/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.4042 - val_loss: 1415.7356\n",
            "Epoch 2036/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.4140 - val_loss: 1415.8071\n",
            "Epoch 2037/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.3611 - val_loss: 1415.5667\n",
            "Epoch 2038/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.2905 - val_loss: 1415.5468\n",
            "Epoch 2039/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.2463 - val_loss: 1415.4246\n",
            "Epoch 2040/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 862.432 - 0s 30us/sample - loss: 1091.2166 - val_loss: 1415.3025\n",
            "Epoch 2041/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1091.2022 - val_loss: 1415.2789\n",
            "Epoch 2042/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1091.1285 - val_loss: 1415.0692\n",
            "Epoch 2043/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1091.1354 - val_loss: 1414.9371\n",
            "Epoch 2044/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1091.0325 - val_loss: 1414.6887\n",
            "Epoch 2045/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.9964 - val_loss: 1414.3656\n",
            "Epoch 2046/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1090.9480 - val_loss: 1414.1521\n",
            "Epoch 2047/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1090.9127 - val_loss: 1413.8386\n",
            "Epoch 2048/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1090.9299 - val_loss: 1413.4813\n",
            "Epoch 2049/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.8630 - val_loss: 1413.3124\n",
            "Epoch 2050/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1090.8139 - val_loss: 1413.3650\n",
            "Epoch 2051/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1090.7647 - val_loss: 1413.3143\n",
            "Epoch 2052/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.7712 - val_loss: 1413.4457\n",
            "Epoch 2053/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.7516 - val_loss: 1413.5480\n",
            "Epoch 2054/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1090.6593 - val_loss: 1413.3101\n",
            "Epoch 2055/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1090.6542 - val_loss: 1413.2820\n",
            "Epoch 2056/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.5831 - val_loss: 1413.1910\n",
            "Epoch 2057/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.5216 - val_loss: 1412.9633\n",
            "Epoch 2058/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1090.6459 - val_loss: 1412.6034\n",
            "Epoch 2059/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1090.5453 - val_loss: 1412.5929\n",
            "Epoch 2060/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1090.4109 - val_loss: 1412.6066\n",
            "Epoch 2061/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 947.614 - 0s 27us/sample - loss: 1090.3919 - val_loss: 1412.5718\n",
            "Epoch 2062/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1090.3394 - val_loss: 1412.6577\n",
            "Epoch 2063/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1090.2826 - val_loss: 1412.6013\n",
            "Epoch 2064/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1090.3473 - val_loss: 1412.7297\n",
            "Epoch 2065/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1090.2038 - val_loss: 1412.5685\n",
            "Epoch 2066/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1090.1964 - val_loss: 1412.3147\n",
            "Epoch 2067/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 688.330 - 0s 29us/sample - loss: 1090.3048 - val_loss: 1411.9961\n",
            "Epoch 2068/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1090.1647 - val_loss: 1412.2731\n",
            "Epoch 2069/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1090.1183 - val_loss: 1412.4424\n",
            "Epoch 2070/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1090.0096 - val_loss: 1412.3334\n",
            "Epoch 2071/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1089.9772 - val_loss: 1412.2611\n",
            "Epoch 2072/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.9426 - val_loss: 1412.3062\n",
            "Epoch 2073/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1089.9102 - val_loss: 1412.4019\n",
            "Epoch 2074/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1089.8715 - val_loss: 1412.2924\n",
            "Epoch 2075/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.8357 - val_loss: 1412.2349\n",
            "Epoch 2076/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1089.8032 - val_loss: 1412.2325\n",
            "Epoch 2077/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1089.7665 - val_loss: 1412.2314\n",
            "Epoch 2078/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1089.7179 - val_loss: 1412.2458\n",
            "Epoch 2079/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1089.7340 - val_loss: 1412.5614\n",
            "Epoch 2080/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1089.6228 - val_loss: 1412.5847\n",
            "Epoch 2081/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.6572 - val_loss: 1412.7410\n",
            "Epoch 2082/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.5736 - val_loss: 1412.5420\n",
            "Epoch 2083/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1089.5518 - val_loss: 1412.5006\n",
            "Epoch 2084/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.5637 - val_loss: 1412.1432\n",
            "Epoch 2085/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.4467 - val_loss: 1412.1525\n",
            "Epoch 2086/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1089.4315 - val_loss: 1411.9823\n",
            "Epoch 2087/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.4192 - val_loss: 1412.1084\n",
            "Epoch 2088/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 26us/sample - loss: 1089.3886 - val_loss: 1412.1337\n",
            "Epoch 2089/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.3175 - val_loss: 1411.8358\n",
            "Epoch 2090/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1089.2765 - val_loss: 1411.7358\n",
            "Epoch 2091/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.2107 - val_loss: 1411.5338\n",
            "Epoch 2092/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.2478 - val_loss: 1411.2529\n",
            "Epoch 2093/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.1851 - val_loss: 1411.2168\n",
            "Epoch 2094/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1089.2761 - val_loss: 1410.8947\n",
            "Epoch 2095/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1089.0880 - val_loss: 1411.0868\n",
            "Epoch 2096/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1089.0511 - val_loss: 1411.0372\n",
            "Epoch 2097/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1089.0104 - val_loss: 1411.0328\n",
            "Epoch 2098/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1089.0298 - val_loss: 1411.3617\n",
            "Epoch 2099/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1088.9408 - val_loss: 1411.3042\n",
            "Epoch 2100/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1089.1056 - val_loss: 1411.6876\n",
            "Epoch 2101/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1088.9535 - val_loss: 1411.4611\n",
            "Epoch 2102/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1088.8756 - val_loss: 1411.1676\n",
            "Epoch 2103/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1088.8091 - val_loss: 1411.2386\n",
            "Epoch 2104/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1088.7493 - val_loss: 1411.1615\n",
            "Epoch 2105/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1088.7144 - val_loss: 1411.1196\n",
            "Epoch 2106/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1088.6950 - val_loss: 1411.1636\n",
            "Epoch 2107/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.6306 - val_loss: 1410.9224\n",
            "Epoch 2108/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1088.6030 - val_loss: 1410.7273\n",
            "Epoch 2109/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1088.5647 - val_loss: 1410.6931\n",
            "Epoch 2110/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1088.5605 - val_loss: 1410.6050\n",
            "Epoch 2111/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.5057 - val_loss: 1410.5100\n",
            "Epoch 2112/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.4372 - val_loss: 1410.2714\n",
            "Epoch 2113/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1088.4039 - val_loss: 1410.0264\n",
            "Epoch 2114/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1088.3891 - val_loss: 1409.6600\n",
            "Epoch 2115/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1088.3841 - val_loss: 1409.5671\n",
            "Epoch 2116/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1088.4750 - val_loss: 1409.0613\n",
            "Epoch 2117/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.3145 - val_loss: 1408.9148\n",
            "Epoch 2118/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.3227 - val_loss: 1409.0664\n",
            "Epoch 2119/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.2720 - val_loss: 1409.1287\n",
            "Epoch 2120/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1088.1998 - val_loss: 1408.9703\n",
            "Epoch 2121/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1088.1647 - val_loss: 1408.7610\n",
            "Epoch 2122/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1088.1370 - val_loss: 1408.7876\n",
            "Epoch 2123/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1088.1076 - val_loss: 1408.8154\n",
            "Epoch 2124/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1088.0577 - val_loss: 1408.7838\n",
            "Epoch 2125/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1088.0906 - val_loss: 1408.5648\n",
            "Epoch 2126/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1088.2841 - val_loss: 1408.9818\n",
            "Epoch 2127/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1087.9752 - val_loss: 1408.7375\n",
            "Epoch 2128/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.9268 - val_loss: 1408.6592\n",
            "Epoch 2129/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.9161 - val_loss: 1408.6731\n",
            "Epoch 2130/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.8381 - val_loss: 1408.5961\n",
            "Epoch 2131/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1087.8626 - val_loss: 1408.6222\n",
            "Epoch 2132/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.8772 - val_loss: 1408.4285\n",
            "Epoch 2133/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1087.7554 - val_loss: 1408.5759\n",
            "Epoch 2134/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.7279 - val_loss: 1408.6118\n",
            "Epoch 2135/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1087.6750 - val_loss: 1408.6354\n",
            "Epoch 2136/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.6744 - val_loss: 1408.7236\n",
            "Epoch 2137/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.6282 - val_loss: 1408.4894\n",
            "Epoch 2138/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1087.5989 - val_loss: 1408.3610\n",
            "Epoch 2139/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1087.6035 - val_loss: 1408.5195\n",
            "Epoch 2140/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.5211 - val_loss: 1408.3090\n",
            "Epoch 2141/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.5398 - val_loss: 1408.4220\n",
            "Epoch 2142/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1087.4504 - val_loss: 1408.1748\n",
            "Epoch 2143/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.4064 - val_loss: 1408.0485\n",
            "Epoch 2144/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1087.3814 - val_loss: 1408.0325\n",
            "Epoch 2145/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1087.3481 - val_loss: 1408.1023\n",
            "Epoch 2146/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1087.2989 - val_loss: 1408.0970\n",
            "Epoch 2147/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1087.2668 - val_loss: 1408.0132\n",
            "Epoch 2148/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.2364 - val_loss: 1407.9900\n",
            "Epoch 2149/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.2025 - val_loss: 1407.9755\n",
            "Epoch 2150/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.2033 - val_loss: 1407.8259\n",
            "Epoch 2151/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1087.1412 - val_loss: 1407.8347\n",
            "Epoch 2152/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1087.1333 - val_loss: 1407.8783\n",
            "Epoch 2153/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1087.0612 - val_loss: 1407.8197\n",
            "Epoch 2154/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1087.0362 - val_loss: 1407.6680\n",
            "Epoch 2155/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1087.0186 - val_loss: 1407.4801\n",
            "Epoch 2156/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1086.9717 - val_loss: 1407.3948\n",
            "Epoch 2157/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1086.9517 - val_loss: 1407.3173\n",
            "Epoch 2158/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.8961 - val_loss: 1407.3434\n",
            "Epoch 2159/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1086.8776 - val_loss: 1407.3479\n",
            "Epoch 2160/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 27us/sample - loss: 1086.8922 - val_loss: 1407.3303\n",
            "Epoch 2161/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.8223 - val_loss: 1407.4702\n",
            "Epoch 2162/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.7950 - val_loss: 1407.3741\n",
            "Epoch 2163/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.7968 - val_loss: 1407.6299\n",
            "Epoch 2164/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.7119 - val_loss: 1407.6182\n",
            "Epoch 2165/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.7165 - val_loss: 1407.4604\n",
            "Epoch 2166/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.6206 - val_loss: 1407.5613\n",
            "Epoch 2167/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.6001 - val_loss: 1407.6433\n",
            "Epoch 2168/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.5754 - val_loss: 1407.7114\n",
            "Epoch 2169/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1086.5422 - val_loss: 1407.7529\n",
            "Epoch 2170/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.5008 - val_loss: 1407.8009\n",
            "Epoch 2171/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1086.5340 - val_loss: 1407.8344\n",
            "Epoch 2172/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.4551 - val_loss: 1407.8704\n",
            "Epoch 2173/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.4181 - val_loss: 1407.8477\n",
            "Epoch 2174/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1086.4078 - val_loss: 1407.9279\n",
            "Epoch 2175/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.3607 - val_loss: 1407.9044\n",
            "Epoch 2176/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1086.3404 - val_loss: 1407.7523\n",
            "Epoch 2177/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.3428 - val_loss: 1407.7775\n",
            "Epoch 2178/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1086.2632 - val_loss: 1407.4745\n",
            "Epoch 2179/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1086.2329 - val_loss: 1407.2628\n",
            "Epoch 2180/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1086.2735 - val_loss: 1407.2501\n",
            "Epoch 2181/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1086.1585 - val_loss: 1406.8320\n",
            "Epoch 2182/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1086.1917 - val_loss: 1406.3555\n",
            "Epoch 2183/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1086.0917 - val_loss: 1406.1108\n",
            "Epoch 2184/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1086.0762 - val_loss: 1405.9386\n",
            "Epoch 2185/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1086.0415 - val_loss: 1405.9398\n",
            "Epoch 2186/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1085.9825 - val_loss: 1405.9031\n",
            "Epoch 2187/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1085.9618 - val_loss: 1405.9550\n",
            "Epoch 2188/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.9368 - val_loss: 1405.9097\n",
            "Epoch 2189/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1086.0137 - val_loss: 1405.5675\n",
            "Epoch 2190/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1085.8908 - val_loss: 1405.6849\n",
            "Epoch 2191/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1085.8619 - val_loss: 1405.8027\n",
            "Epoch 2192/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.8024 - val_loss: 1405.8086\n",
            "Epoch 2193/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1085.7647 - val_loss: 1405.7690\n",
            "Epoch 2194/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1085.8024 - val_loss: 1405.8676\n",
            "Epoch 2195/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1085.7166 - val_loss: 1405.7781\n",
            "Epoch 2196/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.7646 - val_loss: 1405.5316\n",
            "Epoch 2197/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.6688 - val_loss: 1405.5936\n",
            "Epoch 2198/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.6668 - val_loss: 1405.7721\n",
            "Epoch 2199/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.6133 - val_loss: 1405.5813\n",
            "Epoch 2200/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1085.5392 - val_loss: 1405.5020\n",
            "Epoch 2201/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1085.5321 - val_loss: 1405.3004\n",
            "Epoch 2202/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.5231 - val_loss: 1405.3763\n",
            "Epoch 2203/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1085.6511 - val_loss: 1405.5685\n",
            "Epoch 2204/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.4333 - val_loss: 1405.1559\n",
            "Epoch 2205/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.5888 - val_loss: 1404.6472\n",
            "Epoch 2206/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.4122 - val_loss: 1404.5034\n",
            "Epoch 2207/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1085.3330 - val_loss: 1404.6747\n",
            "Epoch 2208/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1085.3277 - val_loss: 1404.6208\n",
            "Epoch 2209/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.2624 - val_loss: 1404.7712\n",
            "Epoch 2210/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1085.3026 - val_loss: 1405.0729\n",
            "Epoch 2211/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.2316 - val_loss: 1405.1998\n",
            "Epoch 2212/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1085.2013 - val_loss: 1405.0627\n",
            "Epoch 2213/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1085.1506 - val_loss: 1405.1277\n",
            "Epoch 2214/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1085.1915 - val_loss: 1404.9182\n",
            "Epoch 2215/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1085.0909 - val_loss: 1404.9900\n",
            "Epoch 2216/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1085.0910 - val_loss: 1405.1788\n",
            "Epoch 2217/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1085.0538 - val_loss: 1405.0413\n",
            "Epoch 2218/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1085.0404 - val_loss: 1405.2319\n",
            "Epoch 2219/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1084.9965 - val_loss: 1405.1240\n",
            "Epoch 2220/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1084.9421 - val_loss: 1405.0280\n",
            "Epoch 2221/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1084.9470 - val_loss: 1404.6981\n",
            "Epoch 2222/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1084.8916 - val_loss: 1404.4855\n",
            "Epoch 2223/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1084.9593 - val_loss: 1404.6251\n",
            "Epoch 2224/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1084.8310 - val_loss: 1404.3945\n",
            "Epoch 2225/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1084.7959 - val_loss: 1404.2683\n",
            "Epoch 2226/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1084.7683 - val_loss: 1404.0723\n",
            "Epoch 2227/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.7219 - val_loss: 1404.0552\n",
            "Epoch 2228/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.7231 - val_loss: 1403.8666\n",
            "Epoch 2229/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1084.7104 - val_loss: 1403.7686\n",
            "Epoch 2230/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1084.6438 - val_loss: 1403.8258\n",
            "Epoch 2231/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.6955 - val_loss: 1404.0309\n",
            "Epoch 2232/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.5864 - val_loss: 1403.8928\n",
            "Epoch 2233/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.6455 - val_loss: 1403.5763\n",
            "Epoch 2234/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1084.5728 - val_loss: 1403.6056\n",
            "Epoch 2235/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.5315 - val_loss: 1403.6436\n",
            "Epoch 2236/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1084.4819 - val_loss: 1403.4974\n",
            "Epoch 2237/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1084.5184 - val_loss: 1403.1228\n",
            "Epoch 2238/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.4177 - val_loss: 1403.0674\n",
            "Epoch 2239/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1084.4039 - val_loss: 1403.1248\n",
            "Epoch 2240/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1084.3780 - val_loss: 1403.1469\n",
            "Epoch 2241/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1084.3474 - val_loss: 1403.0144\n",
            "Epoch 2242/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1084.3539 - val_loss: 1403.1274\n",
            "Epoch 2243/5000\n",
            "353/353 [==============================] - 0s 43us/sample - loss: 1084.3443 - val_loss: 1403.1912\n",
            "Epoch 2244/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1084.2930 - val_loss: 1402.8191\n",
            "Epoch 2245/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1084.2391 - val_loss: 1402.6348\n",
            "Epoch 2246/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1084.2104 - val_loss: 1402.6411\n",
            "Epoch 2247/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1084.1807 - val_loss: 1402.6489\n",
            "Epoch 2248/5000\n",
            "353/353 [==============================] - 0s 43us/sample - loss: 1084.1479 - val_loss: 1402.4883\n",
            "Epoch 2249/5000\n",
            "353/353 [==============================] - 0s 44us/sample - loss: 1084.1182 - val_loss: 1402.4862\n",
            "Epoch 2250/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1084.1475 - val_loss: 1402.5659\n",
            "Epoch 2251/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1084.1140 - val_loss: 1402.3715\n",
            "Epoch 2252/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1084.0955 - val_loss: 1402.3164\n",
            "Epoch 2253/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1084.0007 - val_loss: 1401.9031\n",
            "Epoch 2254/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1084.1873 - val_loss: 1401.4011\n",
            "Epoch 2255/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1084.0297 - val_loss: 1401.4824\n",
            "Epoch 2256/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1083.9389 - val_loss: 1401.3348\n",
            "Epoch 2257/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1083.9600 - val_loss: 1401.2145\n",
            "Epoch 2258/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1083.8920 - val_loss: 1401.1974\n",
            "Epoch 2259/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1083.8714 - val_loss: 1401.2773\n",
            "Epoch 2260/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1084.0679 - val_loss: 1400.9841\n",
            "Epoch 2261/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1083.8270 - val_loss: 1401.3810\n",
            "Epoch 2262/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1083.8142 - val_loss: 1401.4750\n",
            "Epoch 2263/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1083.7497 - val_loss: 1401.7363\n",
            "Epoch 2264/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.6931 - val_loss: 1401.8480\n",
            "Epoch 2265/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1083.6684 - val_loss: 1401.9891\n",
            "Epoch 2266/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1083.6487 - val_loss: 1402.1901\n",
            "Epoch 2267/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.6832 - val_loss: 1402.4338\n",
            "Epoch 2268/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.6430 - val_loss: 1402.4753\n",
            "Epoch 2269/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.6293 - val_loss: 1402.4552\n",
            "Epoch 2270/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 1083.6139 - val_loss: 1402.0353\n",
            "Epoch 2271/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1083.5619 - val_loss: 1401.7869\n",
            "Epoch 2272/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.4875 - val_loss: 1401.7545\n",
            "Epoch 2273/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1083.4661 - val_loss: 1401.7567\n",
            "Epoch 2274/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1083.4701 - val_loss: 1401.7842\n",
            "Epoch 2275/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1083.4162 - val_loss: 1401.6091\n",
            "Epoch 2276/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1083.4585 - val_loss: 1401.2588\n",
            "Epoch 2277/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1083.4011 - val_loss: 1401.3770\n",
            "Epoch 2278/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1083.4367 - val_loss: 1401.4753\n",
            "Epoch 2279/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1083.3077 - val_loss: 1401.2275\n",
            "Epoch 2280/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1083.3304 - val_loss: 1401.0403\n",
            "Epoch 2281/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.3287 - val_loss: 1401.1000\n",
            "Epoch 2282/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.2283 - val_loss: 1400.8606\n",
            "Epoch 2283/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1083.2194 - val_loss: 1400.7257\n",
            "Epoch 2284/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1083.2503 - val_loss: 1400.4445\n",
            "Epoch 2285/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1083.2501 - val_loss: 1400.2753\n",
            "Epoch 2286/5000\n",
            "353/353 [==============================] - 0s 42us/sample - loss: 1083.1361 - val_loss: 1400.4562\n",
            "Epoch 2287/5000\n",
            "353/353 [==============================] - 0s 44us/sample - loss: 1083.1522 - val_loss: 1400.8203\n",
            "Epoch 2288/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 1083.0758 - val_loss: 1401.0195\n",
            "Epoch 2289/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1083.0941 - val_loss: 1401.2676\n",
            "Epoch 2290/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1083.0372 - val_loss: 1401.3468\n",
            "Epoch 2291/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1083.0757 - val_loss: 1401.5013\n",
            "Epoch 2292/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1082.9698 - val_loss: 1401.3955\n",
            "Epoch 2293/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1082.9439 - val_loss: 1401.2211\n",
            "Epoch 2294/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1082.9809 - val_loss: 1400.8566\n",
            "Epoch 2295/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1082.9228 - val_loss: 1400.6725\n",
            "Epoch 2296/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1082.8738 - val_loss: 1400.7305\n",
            "Epoch 2297/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1082.8820 - val_loss: 1400.8789\n",
            "Epoch 2298/5000\n",
            "353/353 [==============================] - 0s 43us/sample - loss: 1082.8267 - val_loss: 1400.7709\n",
            "Epoch 2299/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 1082.8741 - val_loss: 1400.8257\n",
            "Epoch 2300/5000\n",
            "353/353 [==============================] - 0s 60us/sample - loss: 1082.7814 - val_loss: 1400.6743\n",
            "Epoch 2301/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 1082.7839 - val_loss: 1400.1346\n",
            "Epoch 2302/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 1082.7163 - val_loss: 1399.9209\n",
            "Epoch 2303/5000\n",
            "353/353 [==============================] - 0s 44us/sample - loss: 1082.7009 - val_loss: 1399.6676\n",
            "Epoch 2304/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 39us/sample - loss: 1082.6757 - val_loss: 1399.5878\n",
            "Epoch 2305/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1082.6596 - val_loss: 1399.5626\n",
            "Epoch 2306/5000\n",
            "353/353 [==============================] - 0s 47us/sample - loss: 1082.6240 - val_loss: 1399.5917\n",
            "Epoch 2307/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1082.6026 - val_loss: 1399.6704\n",
            "Epoch 2308/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1082.6138 - val_loss: 1399.5443\n",
            "Epoch 2309/5000\n",
            "353/353 [==============================] - 0s 41us/sample - loss: 1082.5534 - val_loss: 1399.5664\n",
            "Epoch 2310/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.5023 - val_loss: 1399.8025\n",
            "Epoch 2311/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 1082.5686 - val_loss: 1399.7820\n",
            "Epoch 2312/5000\n",
            "353/353 [==============================] - 0s 43us/sample - loss: 1082.5341 - val_loss: 1400.2634\n",
            "Epoch 2313/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1082.4263 - val_loss: 1400.3572\n",
            "Epoch 2314/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.4327 - val_loss: 1400.4216\n",
            "Epoch 2315/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1082.4288 - val_loss: 1400.8159\n",
            "Epoch 2316/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1082.4379 - val_loss: 1401.0872\n",
            "Epoch 2317/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1082.3481 - val_loss: 1401.2136\n",
            "Epoch 2318/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1082.3367 - val_loss: 1401.0969\n",
            "Epoch 2319/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.3097 - val_loss: 1401.1987\n",
            "Epoch 2320/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1082.2871 - val_loss: 1401.0449\n",
            "Epoch 2321/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1082.3123 - val_loss: 1400.8134\n",
            "Epoch 2322/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1082.4021 - val_loss: 1401.1476\n",
            "Epoch 2323/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.2080 - val_loss: 1400.8462\n",
            "Epoch 2324/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.2055 - val_loss: 1400.7227\n",
            "Epoch 2325/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1082.2025 - val_loss: 1400.4126\n",
            "Epoch 2326/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1082.1688 - val_loss: 1400.5270\n",
            "Epoch 2327/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1082.1637 - val_loss: 1400.1716\n",
            "Epoch 2328/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1082.0669 - val_loss: 1400.1456\n",
            "Epoch 2329/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1082.0376 - val_loss: 1400.1318\n",
            "Epoch 2330/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1082.1168 - val_loss: 1400.3018\n",
            "Epoch 2331/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.9873 - val_loss: 1400.0853\n",
            "Epoch 2332/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1081.9611 - val_loss: 1399.8181\n",
            "Epoch 2333/5000\n",
            "353/353 [==============================] - 0s 39us/sample - loss: 1082.0318 - val_loss: 1399.4775\n",
            "Epoch 2334/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1081.9585 - val_loss: 1399.2411\n",
            "Epoch 2335/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1081.9103 - val_loss: 1399.2899\n",
            "Epoch 2336/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1081.8792 - val_loss: 1399.3494\n",
            "Epoch 2337/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1081.9219 - val_loss: 1399.3593\n",
            "Epoch 2338/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1081.8908 - val_loss: 1399.6184\n",
            "Epoch 2339/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1081.8191 - val_loss: 1399.5175\n",
            "Epoch 2340/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.7780 - val_loss: 1399.4570\n",
            "Epoch 2341/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1081.7816 - val_loss: 1399.4359\n",
            "Epoch 2342/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.7341 - val_loss: 1399.3130\n",
            "Epoch 2343/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.7558 - val_loss: 1399.1782\n",
            "Epoch 2344/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.7545 - val_loss: 1399.4940\n",
            "Epoch 2345/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1081.6953 - val_loss: 1399.3812\n",
            "Epoch 2346/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.6844 - val_loss: 1399.4003\n",
            "Epoch 2347/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1081.6488 - val_loss: 1399.5321\n",
            "Epoch 2348/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1081.6252 - val_loss: 1399.6447\n",
            "Epoch 2349/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1081.5970 - val_loss: 1399.7101\n",
            "Epoch 2350/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1081.5611 - val_loss: 1399.5428\n",
            "Epoch 2351/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1081.5191 - val_loss: 1399.4655\n",
            "Epoch 2352/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.5569 - val_loss: 1399.5199\n",
            "Epoch 2353/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.4933 - val_loss: 1399.1888\n",
            "Epoch 2354/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.6942 - val_loss: 1399.3512\n",
            "Epoch 2355/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.4028 - val_loss: 1398.8873\n",
            "Epoch 2356/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1081.4250 - val_loss: 1398.4403\n",
            "Epoch 2357/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1081.4110 - val_loss: 1398.1768\n",
            "Epoch 2358/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1081.3767 - val_loss: 1397.9911\n",
            "Epoch 2359/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1081.4081 - val_loss: 1398.1243\n",
            "Epoch 2360/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.4199 - val_loss: 1398.2417\n",
            "Epoch 2361/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.2981 - val_loss: 1398.0500\n",
            "Epoch 2362/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.2960 - val_loss: 1397.7380\n",
            "Epoch 2363/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1081.3090 - val_loss: 1397.5004\n",
            "Epoch 2364/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.2554 - val_loss: 1397.5647\n",
            "Epoch 2365/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.2317 - val_loss: 1397.6604\n",
            "Epoch 2366/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.2080 - val_loss: 1397.8119\n",
            "Epoch 2367/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1081.1990 - val_loss: 1397.9656\n",
            "Epoch 2368/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1081.2149 - val_loss: 1398.1044\n",
            "Epoch 2369/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1081.1723 - val_loss: 1397.8136\n",
            "Epoch 2370/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1081.1032 - val_loss: 1397.7653\n",
            "Epoch 2371/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1081.1299 - val_loss: 1397.7501\n",
            "Epoch 2372/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1081.0732 - val_loss: 1397.9180\n",
            "Epoch 2373/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1081.0314 - val_loss: 1398.0190\n",
            "Epoch 2374/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1081.0267 - val_loss: 1398.0754\n",
            "Epoch 2375/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1081.0057 - val_loss: 1398.3018\n",
            "Epoch 2376/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.9756 - val_loss: 1398.3164\n",
            "Epoch 2377/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1080.9496 - val_loss: 1398.5294\n",
            "Epoch 2378/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1080.9498 - val_loss: 1398.7039\n",
            "Epoch 2379/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1080.9385 - val_loss: 1398.6779\n",
            "Epoch 2380/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.8906 - val_loss: 1398.5883\n",
            "Epoch 2381/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.9443 - val_loss: 1398.3086\n",
            "Epoch 2382/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1080.8403 - val_loss: 1398.3159\n",
            "Epoch 2383/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1080.8462 - val_loss: 1398.3298\n",
            "Epoch 2384/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.8547 - val_loss: 1398.4575\n",
            "Epoch 2385/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.7956 - val_loss: 1398.4281\n",
            "Epoch 2386/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.7666 - val_loss: 1398.1949\n",
            "Epoch 2387/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1080.8483 - val_loss: 1397.8392\n",
            "Epoch 2388/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.7226 - val_loss: 1397.7040\n",
            "Epoch 2389/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1080.7126 - val_loss: 1397.5641\n",
            "Epoch 2390/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.7513 - val_loss: 1397.4176\n",
            "Epoch 2391/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.9146 - val_loss: 1397.9309\n",
            "Epoch 2392/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.7345 - val_loss: 1397.6488\n",
            "Epoch 2393/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1080.6373 - val_loss: 1397.7019\n",
            "Epoch 2394/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1080.6429 - val_loss: 1397.6072\n",
            "Epoch 2395/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.5894 - val_loss: 1397.8816\n",
            "Epoch 2396/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1080.5775 - val_loss: 1398.0320\n",
            "Epoch 2397/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.7091 - val_loss: 1397.7343\n",
            "Epoch 2398/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1080.6224 - val_loss: 1398.0748\n",
            "Epoch 2399/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1080.5053 - val_loss: 1398.0929\n",
            "Epoch 2400/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.4931 - val_loss: 1398.0576\n",
            "Epoch 2401/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.4623 - val_loss: 1397.9214\n",
            "Epoch 2402/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.4407 - val_loss: 1397.8021\n",
            "Epoch 2403/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.5280 - val_loss: 1397.9574\n",
            "Epoch 2404/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.3883 - val_loss: 1397.7927\n",
            "Epoch 2405/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.4629 - val_loss: 1397.4435\n",
            "Epoch 2406/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1080.3595 - val_loss: 1397.4108\n",
            "Epoch 2407/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.3559 - val_loss: 1397.5219\n",
            "Epoch 2408/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.3181 - val_loss: 1397.4685\n",
            "Epoch 2409/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1080.3002 - val_loss: 1397.3766\n",
            "Epoch 2410/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1080.2784 - val_loss: 1397.4142\n",
            "Epoch 2411/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1080.2853 - val_loss: 1397.2297\n",
            "Epoch 2412/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.2618 - val_loss: 1397.1510\n",
            "Epoch 2413/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.2060 - val_loss: 1397.3038\n",
            "Epoch 2414/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.2605 - val_loss: 1397.5791\n",
            "Epoch 2415/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1080.1849 - val_loss: 1397.6611\n",
            "Epoch 2416/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.1532 - val_loss: 1397.5811\n",
            "Epoch 2417/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1080.1370 - val_loss: 1397.5095\n",
            "Epoch 2418/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1080.1218 - val_loss: 1397.4043\n",
            "Epoch 2419/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.1692 - val_loss: 1397.1348\n",
            "Epoch 2420/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.0729 - val_loss: 1397.1510\n",
            "Epoch 2421/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.0998 - val_loss: 1397.3671\n",
            "Epoch 2422/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1080.0395 - val_loss: 1397.2428\n",
            "Epoch 2423/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.0180 - val_loss: 1397.1472\n",
            "Epoch 2424/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.9977 - val_loss: 1397.1343\n",
            "Epoch 2425/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.9960 - val_loss: 1397.0327\n",
            "Epoch 2426/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1080.0832 - val_loss: 1397.3722\n",
            "Epoch 2427/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.9635 - val_loss: 1397.1649\n",
            "Epoch 2428/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.9433 - val_loss: 1397.0327\n",
            "Epoch 2429/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.9881 - val_loss: 1396.7872\n",
            "Epoch 2430/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.9036 - val_loss: 1397.0120\n",
            "Epoch 2431/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.8794 - val_loss: 1397.1411\n",
            "Epoch 2432/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1079.8824 - val_loss: 1397.0260\n",
            "Epoch 2433/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.9109 - val_loss: 1396.8704\n",
            "Epoch 2434/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.8228 - val_loss: 1397.1946\n",
            "Epoch 2435/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.7903 - val_loss: 1397.2402\n",
            "Epoch 2436/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.7600 - val_loss: 1397.3567\n",
            "Epoch 2437/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.7623 - val_loss: 1397.5448\n",
            "Epoch 2438/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.7377 - val_loss: 1397.8079\n",
            "Epoch 2439/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.7763 - val_loss: 1398.0330\n",
            "Epoch 2440/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.7228 - val_loss: 1397.9218\n",
            "Epoch 2441/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.7362 - val_loss: 1397.8490\n",
            "Epoch 2442/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.6875 - val_loss: 1397.9181\n",
            "Epoch 2443/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.7074 - val_loss: 1397.7192\n",
            "Epoch 2444/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.6400 - val_loss: 1397.7579\n",
            "Epoch 2445/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.6132 - val_loss: 1397.7369\n",
            "Epoch 2446/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1079.5843 - val_loss: 1397.7347\n",
            "Epoch 2447/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.6062 - val_loss: 1397.8375\n",
            "Epoch 2448/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.5798 - val_loss: 1397.5178\n",
            "Epoch 2449/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.5613 - val_loss: 1397.2711\n",
            "Epoch 2450/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1079.5252 - val_loss: 1397.3245\n",
            "Epoch 2451/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1079.4934 - val_loss: 1397.0574\n",
            "Epoch 2452/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.4810 - val_loss: 1396.8468\n",
            "Epoch 2453/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1079.4508 - val_loss: 1396.7689\n",
            "Epoch 2454/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.4550 - val_loss: 1396.5077\n",
            "Epoch 2455/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.3999 - val_loss: 1396.4347\n",
            "Epoch 2456/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.3976 - val_loss: 1396.3571\n",
            "Epoch 2457/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.4935 - val_loss: 1396.6954\n",
            "Epoch 2458/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.3393 - val_loss: 1396.6445\n",
            "Epoch 2459/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.4183 - val_loss: 1396.3456\n",
            "Epoch 2460/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.3650 - val_loss: 1396.2285\n",
            "Epoch 2461/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.3460 - val_loss: 1396.4092\n",
            "Epoch 2462/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.2963 - val_loss: 1396.5494\n",
            "Epoch 2463/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.3105 - val_loss: 1396.5927\n",
            "Epoch 2464/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.2348 - val_loss: 1396.3429\n",
            "Epoch 2465/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.2066 - val_loss: 1396.2867\n",
            "Epoch 2466/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.1904 - val_loss: 1396.1377\n",
            "Epoch 2467/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.2031 - val_loss: 1396.1654\n",
            "Epoch 2468/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.2352 - val_loss: 1395.7831\n",
            "Epoch 2469/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.1347 - val_loss: 1395.5953\n",
            "Epoch 2470/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.1249 - val_loss: 1395.4341\n",
            "Epoch 2471/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1079.1696 - val_loss: 1395.2996\n",
            "Epoch 2472/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.0995 - val_loss: 1395.3425\n",
            "Epoch 2473/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1079.0761 - val_loss: 1395.6775\n",
            "Epoch 2474/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1079.0508 - val_loss: 1395.8531\n",
            "Epoch 2475/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.0568 - val_loss: 1396.1141\n",
            "Epoch 2476/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1079.0068 - val_loss: 1396.1361\n",
            "Epoch 2477/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1079.0293 - val_loss: 1396.1318\n",
            "Epoch 2478/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.9770 - val_loss: 1396.2252\n",
            "Epoch 2479/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1079.0744 - val_loss: 1396.4695\n",
            "Epoch 2480/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.9927 - val_loss: 1396.4242\n",
            "Epoch 2481/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.9234 - val_loss: 1396.1969\n",
            "Epoch 2482/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.9186 - val_loss: 1395.9824\n",
            "Epoch 2483/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1078.9185 - val_loss: 1395.7356\n",
            "Epoch 2484/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.8818 - val_loss: 1395.6217\n",
            "Epoch 2485/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.8809 - val_loss: 1395.4059\n",
            "Epoch 2486/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.8687 - val_loss: 1395.2128\n",
            "Epoch 2487/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.8275 - val_loss: 1395.1962\n",
            "Epoch 2488/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.8112 - val_loss: 1395.4193\n",
            "Epoch 2489/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.7909 - val_loss: 1395.4117\n",
            "Epoch 2490/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.7822 - val_loss: 1395.4766\n",
            "Epoch 2491/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1078.7617 - val_loss: 1395.4192\n",
            "Epoch 2492/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.7406 - val_loss: 1395.5690\n",
            "Epoch 2493/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.7122 - val_loss: 1395.5359\n",
            "Epoch 2494/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.7052 - val_loss: 1395.6074\n",
            "Epoch 2495/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1078.7417 - val_loss: 1395.4023\n",
            "Epoch 2496/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.7343 - val_loss: 1395.2799\n",
            "Epoch 2497/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.6617 - val_loss: 1395.5562\n",
            "Epoch 2498/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1078.6249 - val_loss: 1395.6514\n",
            "Epoch 2499/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1078.6285 - val_loss: 1395.5902\n",
            "Epoch 2500/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.6624 - val_loss: 1395.8962\n",
            "Epoch 2501/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.6112 - val_loss: 1395.8699\n",
            "Epoch 2502/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.6972 - val_loss: 1395.5309\n",
            "Epoch 2503/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1078.5576 - val_loss: 1395.6823\n",
            "Epoch 2504/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.6491 - val_loss: 1395.4485\n",
            "Epoch 2505/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.5208 - val_loss: 1395.6278\n",
            "Epoch 2506/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1078.5481 - val_loss: 1395.5920\n",
            "Epoch 2507/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.4762 - val_loss: 1395.8478\n",
            "Epoch 2508/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.5066 - val_loss: 1396.1000\n",
            "Epoch 2509/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.4643 - val_loss: 1396.0166\n",
            "Epoch 2510/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.4459 - val_loss: 1396.0115\n",
            "Epoch 2511/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.4910 - val_loss: 1395.8723\n",
            "Epoch 2512/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.5051 - val_loss: 1395.6277\n",
            "Epoch 2513/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.4534 - val_loss: 1395.9294\n",
            "Epoch 2514/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.3763 - val_loss: 1395.8193\n",
            "Epoch 2515/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.3956 - val_loss: 1395.7040\n",
            "Epoch 2516/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.3763 - val_loss: 1395.5466\n",
            "Epoch 2517/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.3196 - val_loss: 1395.6240\n",
            "Epoch 2518/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.3878 - val_loss: 1395.8422\n",
            "Epoch 2519/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.4911 - val_loss: 1395.3953\n",
            "Epoch 2520/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 26us/sample - loss: 1078.3103 - val_loss: 1395.5133\n",
            "Epoch 2521/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.3845 - val_loss: 1395.5496\n",
            "Epoch 2522/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.2719 - val_loss: 1395.3223\n",
            "Epoch 2523/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1078.2355 - val_loss: 1395.2512\n",
            "Epoch 2524/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.1966 - val_loss: 1395.3585\n",
            "Epoch 2525/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1078.1798 - val_loss: 1395.4747\n",
            "Epoch 2526/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.1710 - val_loss: 1395.4611\n",
            "Epoch 2527/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1078.1453 - val_loss: 1395.4900\n",
            "Epoch 2528/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1078.1424 - val_loss: 1395.4753\n",
            "Epoch 2529/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.1411 - val_loss: 1395.3103\n",
            "Epoch 2530/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1078.1140 - val_loss: 1395.3829\n",
            "Epoch 2531/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.1048 - val_loss: 1395.4250\n",
            "Epoch 2532/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.0742 - val_loss: 1395.1993\n",
            "Epoch 2533/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1078.1319 - val_loss: 1394.8422\n",
            "Epoch 2534/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.0916 - val_loss: 1394.9551\n",
            "Epoch 2535/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1078.1060 - val_loss: 1394.6356\n",
            "Epoch 2536/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1078.0033 - val_loss: 1394.6318\n",
            "Epoch 2537/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.0571 - val_loss: 1394.9108\n",
            "Epoch 2538/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.9812 - val_loss: 1394.8905\n",
            "Epoch 2539/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1078.0050 - val_loss: 1394.5956\n",
            "Epoch 2540/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.9972 - val_loss: 1394.7013\n",
            "Epoch 2541/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.9306 - val_loss: 1394.4675\n",
            "Epoch 2542/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.9048 - val_loss: 1394.3387\n",
            "Epoch 2543/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1077.8887 - val_loss: 1394.3364\n",
            "Epoch 2544/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.9395 - val_loss: 1394.1150\n",
            "Epoch 2545/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1078.0084 - val_loss: 1394.3004\n",
            "Epoch 2546/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.9223 - val_loss: 1393.9938\n",
            "Epoch 2547/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1077.8195 - val_loss: 1394.0238\n",
            "Epoch 2548/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.8102 - val_loss: 1394.1357\n",
            "Epoch 2549/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.8230 - val_loss: 1394.2007\n",
            "Epoch 2550/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.7795 - val_loss: 1394.1993\n",
            "Epoch 2551/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.7718 - val_loss: 1394.2288\n",
            "Epoch 2552/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.8096 - val_loss: 1394.0464\n",
            "Epoch 2553/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.7410 - val_loss: 1394.0017\n",
            "Epoch 2554/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.7167 - val_loss: 1393.8591\n",
            "Epoch 2555/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.9235 - val_loss: 1393.4366\n",
            "Epoch 2556/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.7199 - val_loss: 1393.5715\n",
            "Epoch 2557/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.7066 - val_loss: 1393.9098\n",
            "Epoch 2558/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.7282 - val_loss: 1393.8031\n",
            "Epoch 2559/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.7245 - val_loss: 1394.1132\n",
            "Epoch 2560/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.6274 - val_loss: 1394.1409\n",
            "Epoch 2561/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.6461 - val_loss: 1394.0756\n",
            "Epoch 2562/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.6514 - val_loss: 1393.9199\n",
            "Epoch 2563/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1077.6022 - val_loss: 1393.9275\n",
            "Epoch 2564/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.5990 - val_loss: 1394.2701\n",
            "Epoch 2565/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.5735 - val_loss: 1394.3845\n",
            "Epoch 2566/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.6045 - val_loss: 1394.5618\n",
            "Epoch 2567/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.5364 - val_loss: 1394.3408\n",
            "Epoch 2568/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1077.5106 - val_loss: 1394.2404\n",
            "Epoch 2569/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.4949 - val_loss: 1393.9913\n",
            "Epoch 2570/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1077.4777 - val_loss: 1393.7993\n",
            "Epoch 2571/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.5015 - val_loss: 1393.5464\n",
            "Epoch 2572/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.4584 - val_loss: 1393.4717\n",
            "Epoch 2573/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.4515 - val_loss: 1393.3517\n",
            "Epoch 2574/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.4252 - val_loss: 1393.3854\n",
            "Epoch 2575/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.4476 - val_loss: 1393.5404\n",
            "Epoch 2576/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.3919 - val_loss: 1393.5066\n",
            "Epoch 2577/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.4606 - val_loss: 1393.2200\n",
            "Epoch 2578/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.3780 - val_loss: 1393.2318\n",
            "Epoch 2579/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.3756 - val_loss: 1393.5001\n",
            "Epoch 2580/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.3988 - val_loss: 1393.7648\n",
            "Epoch 2581/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.5619 - val_loss: 1394.0547\n",
            "Epoch 2582/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.3069 - val_loss: 1393.7589\n",
            "Epoch 2583/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.4080 - val_loss: 1393.7974\n",
            "Epoch 2584/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.3800 - val_loss: 1393.3066\n",
            "Epoch 2585/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.2900 - val_loss: 1393.2372\n",
            "Epoch 2586/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.2809 - val_loss: 1393.1433\n",
            "Epoch 2587/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.2666 - val_loss: 1393.1379\n",
            "Epoch 2588/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.2180 - val_loss: 1393.4252\n",
            "Epoch 2589/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1077.2201 - val_loss: 1393.4921\n",
            "Epoch 2590/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.2589 - val_loss: 1393.7937\n",
            "Epoch 2591/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.1840 - val_loss: 1393.8732\n",
            "Epoch 2592/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.1668 - val_loss: 1393.8325\n",
            "Epoch 2593/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.1455 - val_loss: 1393.7977\n",
            "Epoch 2594/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1077.1607 - val_loss: 1393.5737\n",
            "Epoch 2595/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.1224 - val_loss: 1393.5261\n",
            "Epoch 2596/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1077.1082 - val_loss: 1393.4292\n",
            "Epoch 2597/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1077.0948 - val_loss: 1393.4580\n",
            "Epoch 2598/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1077.0978 - val_loss: 1393.2852\n",
            "Epoch 2599/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.0647 - val_loss: 1393.1398\n",
            "Epoch 2600/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.0438 - val_loss: 1393.0664\n",
            "Epoch 2601/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1077.0520 - val_loss: 1393.0881\n",
            "Epoch 2602/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1077.0443 - val_loss: 1392.8730\n",
            "Epoch 2603/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1077.0055 - val_loss: 1392.8287\n",
            "Epoch 2604/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1077.0058 - val_loss: 1392.9550\n",
            "Epoch 2605/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1077.0017 - val_loss: 1393.0602\n",
            "Epoch 2606/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1079.05 - 0s 29us/sample - loss: 1077.0494 - val_loss: 1393.0906\n",
            "Epoch 2607/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1076.9714 - val_loss: 1393.0104\n",
            "Epoch 2608/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.9538 - val_loss: 1392.6365\n",
            "Epoch 2609/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.9194 - val_loss: 1392.4783\n",
            "Epoch 2610/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.9331 - val_loss: 1392.3546\n",
            "Epoch 2611/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.9301 - val_loss: 1392.0624\n",
            "Epoch 2612/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1076.9666 - val_loss: 1392.1964\n",
            "Epoch 2613/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1505.05 - 0s 31us/sample - loss: 1076.8917 - val_loss: 1392.0824\n",
            "Epoch 2614/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.8852 - val_loss: 1392.0012\n",
            "Epoch 2615/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.8974 - val_loss: 1392.0481\n",
            "Epoch 2616/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1076.8623 - val_loss: 1392.0024\n",
            "Epoch 2617/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1076.8570 - val_loss: 1392.1404\n",
            "Epoch 2618/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.8251 - val_loss: 1392.2390\n",
            "Epoch 2619/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1076.8537 - val_loss: 1392.1161\n",
            "Epoch 2620/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1076.8316 - val_loss: 1392.4271\n",
            "Epoch 2621/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.7981 - val_loss: 1392.5521\n",
            "Epoch 2622/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.7894 - val_loss: 1392.6500\n",
            "Epoch 2623/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.7591 - val_loss: 1392.5559\n",
            "Epoch 2624/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1076.7618 - val_loss: 1392.6062\n",
            "Epoch 2625/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.7210 - val_loss: 1392.7188\n",
            "Epoch 2626/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.7776 - val_loss: 1392.9081\n",
            "Epoch 2627/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.7992 - val_loss: 1393.1173\n",
            "Epoch 2628/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.6766 - val_loss: 1392.9862\n",
            "Epoch 2629/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.6638 - val_loss: 1392.8550\n",
            "Epoch 2630/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.6711 - val_loss: 1392.8342\n",
            "Epoch 2631/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.6729 - val_loss: 1392.8082\n",
            "Epoch 2632/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.6175 - val_loss: 1392.5759\n",
            "Epoch 2633/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.6117 - val_loss: 1392.4165\n",
            "Epoch 2634/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.6105 - val_loss: 1392.0800\n",
            "Epoch 2635/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.6120 - val_loss: 1392.0294\n",
            "Epoch 2636/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.6517 - val_loss: 1391.6318\n",
            "Epoch 2637/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.6134 - val_loss: 1391.5118\n",
            "Epoch 2638/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.6072 - val_loss: 1391.7526\n",
            "Epoch 2639/5000\n",
            "353/353 [==============================] - 0s 45us/sample - loss: 1076.5786 - val_loss: 1391.7499\n",
            "Epoch 2640/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.5563 - val_loss: 1391.8530\n",
            "Epoch 2641/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.5155 - val_loss: 1392.2421\n",
            "Epoch 2642/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4840 - val_loss: 1392.5563\n",
            "Epoch 2643/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.5401 - val_loss: 1393.0206\n",
            "Epoch 2644/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4649 - val_loss: 1393.0793\n",
            "Epoch 2645/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.4581 - val_loss: 1393.1512\n",
            "Epoch 2646/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.4397 - val_loss: 1393.4482\n",
            "Epoch 2647/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4946 - val_loss: 1393.7900\n",
            "Epoch 2648/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4275 - val_loss: 1393.7266\n",
            "Epoch 2649/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4176 - val_loss: 1393.6377\n",
            "Epoch 2650/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4819 - val_loss: 1393.4919\n",
            "Epoch 2651/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.3729 - val_loss: 1393.6406\n",
            "Epoch 2652/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.3841 - val_loss: 1393.8667\n",
            "Epoch 2653/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4171 - val_loss: 1394.0706\n",
            "Epoch 2654/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.4592 - val_loss: 1393.7826\n",
            "Epoch 2655/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.3572 - val_loss: 1393.9724\n",
            "Epoch 2656/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.3405 - val_loss: 1393.8884\n",
            "Epoch 2657/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.3643 - val_loss: 1393.9447\n",
            "Epoch 2658/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.3126 - val_loss: 1394.0651\n",
            "Epoch 2659/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.3090 - val_loss: 1393.9558\n",
            "Epoch 2660/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.3006 - val_loss: 1394.0287\n",
            "Epoch 2661/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.3185 - val_loss: 1393.9935\n",
            "Epoch 2662/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.3058 - val_loss: 1393.8496\n",
            "Epoch 2663/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 32us/sample - loss: 1076.2487 - val_loss: 1393.5342\n",
            "Epoch 2664/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1076.2509 - val_loss: 1393.4056\n",
            "Epoch 2665/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.2741 - val_loss: 1393.1721\n",
            "Epoch 2666/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.2443 - val_loss: 1393.1857\n",
            "Epoch 2667/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.2032 - val_loss: 1393.0939\n",
            "Epoch 2668/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1076.1665 - val_loss: 1392.8871\n",
            "Epoch 2669/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.1812 - val_loss: 1392.5563\n",
            "Epoch 2670/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.1929 - val_loss: 1392.3335\n",
            "Epoch 2671/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.2537 - val_loss: 1392.0470\n",
            "Epoch 2672/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.2098 - val_loss: 1391.9757\n",
            "Epoch 2673/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.1617 - val_loss: 1391.9817\n",
            "Epoch 2674/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.1422 - val_loss: 1392.4174\n",
            "Epoch 2675/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1076.1628 - val_loss: 1392.3582\n",
            "Epoch 2676/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.1176 - val_loss: 1392.7402\n",
            "Epoch 2677/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.0881 - val_loss: 1392.7290\n",
            "Epoch 2678/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.1397 - val_loss: 1392.8484\n",
            "Epoch 2679/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1076.0141 - val_loss: 1393.1786\n",
            "Epoch 2680/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.0875 - val_loss: 1393.3643\n",
            "Epoch 2681/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1076.0236 - val_loss: 1393.8302\n",
            "Epoch 2682/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.1982 - val_loss: 1394.3456\n",
            "Epoch 2683/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1076.0608 - val_loss: 1394.3424\n",
            "Epoch 2684/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1076.0367 - val_loss: 1394.2488\n",
            "Epoch 2685/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1076.0342 - val_loss: 1393.8909\n",
            "Epoch 2686/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.9917 - val_loss: 1393.4670\n",
            "Epoch 2687/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.9585 - val_loss: 1393.2023\n",
            "Epoch 2688/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.9375 - val_loss: 1393.0297\n",
            "Epoch 2689/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.9291 - val_loss: 1392.6725\n",
            "Epoch 2690/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.9527 - val_loss: 1392.3970\n",
            "Epoch 2691/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.9280 - val_loss: 1392.1536\n",
            "Epoch 2692/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.9036 - val_loss: 1392.2043\n",
            "Epoch 2693/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.9063 - val_loss: 1391.9822\n",
            "Epoch 2694/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.8767 - val_loss: 1392.0239\n",
            "Epoch 2695/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.8746 - val_loss: 1391.9712\n",
            "Epoch 2696/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.9736 - val_loss: 1391.6840\n",
            "Epoch 2697/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.8762 - val_loss: 1391.6793\n",
            "Epoch 2698/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.8289 - val_loss: 1391.7172\n",
            "Epoch 2699/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.8177 - val_loss: 1391.9534\n",
            "Epoch 2700/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.8113 - val_loss: 1392.1167\n",
            "Epoch 2701/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.8840 - val_loss: 1392.0128\n",
            "Epoch 2702/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.7691 - val_loss: 1392.3383\n",
            "Epoch 2703/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.7653 - val_loss: 1392.4971\n",
            "Epoch 2704/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.7456 - val_loss: 1392.6785\n",
            "Epoch 2705/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.7512 - val_loss: 1392.8156\n",
            "Epoch 2706/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.7439 - val_loss: 1392.8589\n",
            "Epoch 2707/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.7523 - val_loss: 1393.0454\n",
            "Epoch 2708/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.7159 - val_loss: 1393.0509\n",
            "Epoch 2709/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.7728 - val_loss: 1392.7516\n",
            "Epoch 2710/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.6968 - val_loss: 1392.7034\n",
            "Epoch 2711/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.6783 - val_loss: 1392.8055\n",
            "Epoch 2712/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.6656 - val_loss: 1392.8378\n",
            "Epoch 2713/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.7170 - val_loss: 1393.0549\n",
            "Epoch 2714/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.6636 - val_loss: 1393.0347\n",
            "Epoch 2715/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1075.6558 - val_loss: 1392.9639\n",
            "Epoch 2716/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.6186 - val_loss: 1392.6785\n",
            "Epoch 2717/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.6332 - val_loss: 1392.3685\n",
            "Epoch 2718/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1075.6167 - val_loss: 1392.2706\n",
            "Epoch 2719/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.5715 - val_loss: 1392.0511\n",
            "Epoch 2720/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.6331 - val_loss: 1391.6838\n",
            "Epoch 2721/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.5906 - val_loss: 1391.5487\n",
            "Epoch 2722/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.7129 - val_loss: 1391.9205\n",
            "Epoch 2723/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.5563 - val_loss: 1391.7429\n",
            "Epoch 2724/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.5394 - val_loss: 1391.7982\n",
            "Epoch 2725/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.5199 - val_loss: 1391.8600\n",
            "Epoch 2726/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.5152 - val_loss: 1391.8296\n",
            "Epoch 2727/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.5065 - val_loss: 1391.7373\n",
            "Epoch 2728/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.5404 - val_loss: 1391.5817\n",
            "Epoch 2729/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.4898 - val_loss: 1391.7827\n",
            "Epoch 2730/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.4958 - val_loss: 1391.8295\n",
            "Epoch 2731/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.4451 - val_loss: 1392.0271\n",
            "Epoch 2732/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.4679 - val_loss: 1392.5083\n",
            "Epoch 2733/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.4412 - val_loss: 1392.9175\n",
            "Epoch 2734/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.4340 - val_loss: 1393.1581\n",
            "Epoch 2735/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.4254 - val_loss: 1393.1700\n",
            "Epoch 2736/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.4214 - val_loss: 1393.3650\n",
            "Epoch 2737/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.4666 - val_loss: 1393.4662\n",
            "Epoch 2738/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.3940 - val_loss: 1393.2339\n",
            "Epoch 2739/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.3996 - val_loss: 1392.9469\n",
            "Epoch 2740/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.3699 - val_loss: 1392.7771\n",
            "Epoch 2741/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.6366 - val_loss: 1392.2438\n",
            "Epoch 2742/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.3589 - val_loss: 1392.3024\n",
            "Epoch 2743/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.3330 - val_loss: 1392.5850\n",
            "Epoch 2744/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.3223 - val_loss: 1392.8795\n",
            "Epoch 2745/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.3439 - val_loss: 1392.8610\n",
            "Epoch 2746/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.3046 - val_loss: 1393.0104\n",
            "Epoch 2747/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.2855 - val_loss: 1393.2123\n",
            "Epoch 2748/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.4011 - val_loss: 1393.0747\n",
            "Epoch 2749/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1075.3394 - val_loss: 1393.4513\n",
            "Epoch 2750/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1075.3415 - val_loss: 1393.6982\n",
            "Epoch 2751/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.2767 - val_loss: 1393.6376\n",
            "Epoch 2752/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1075.3808 - val_loss: 1393.1625\n",
            "Epoch 2753/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1075.2746 - val_loss: 1393.0615\n",
            "Epoch 2754/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1075.2571 - val_loss: 1393.2224\n",
            "Epoch 2755/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.2378 - val_loss: 1393.0446\n",
            "Epoch 2756/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.2081 - val_loss: 1392.9403\n",
            "Epoch 2757/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.2249 - val_loss: 1392.7892\n",
            "Epoch 2758/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.1740 - val_loss: 1392.8433\n",
            "Epoch 2759/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.2188 - val_loss: 1392.6820\n",
            "Epoch 2760/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.1561 - val_loss: 1392.8209\n",
            "Epoch 2761/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.1558 - val_loss: 1392.8645\n",
            "Epoch 2762/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.1796 - val_loss: 1393.0479\n",
            "Epoch 2763/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1075.1314 - val_loss: 1392.9399\n",
            "Epoch 2764/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.1237 - val_loss: 1392.7135\n",
            "Epoch 2765/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.1183 - val_loss: 1392.6266\n",
            "Epoch 2766/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.1074 - val_loss: 1392.4004\n",
            "Epoch 2767/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.0959 - val_loss: 1392.2172\n",
            "Epoch 2768/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1075.2041 - val_loss: 1392.4359\n",
            "Epoch 2769/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1075.0642 - val_loss: 1392.1072\n",
            "Epoch 2770/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.1745 - val_loss: 1391.6089\n",
            "Epoch 2771/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.0566 - val_loss: 1391.6383\n",
            "Epoch 2772/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.0309 - val_loss: 1391.6014\n",
            "Epoch 2773/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.0283 - val_loss: 1391.5341\n",
            "Epoch 2774/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1075.1299 - val_loss: 1391.8633\n",
            "Epoch 2775/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1075.0056 - val_loss: 1391.7524\n",
            "Epoch 2776/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.0131 - val_loss: 1391.8293\n",
            "Epoch 2777/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.0066 - val_loss: 1391.8334\n",
            "Epoch 2778/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1075.0350 - val_loss: 1391.5117\n",
            "Epoch 2779/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.9879 - val_loss: 1391.5632\n",
            "Epoch 2780/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1074.9604 - val_loss: 1391.6169\n",
            "Epoch 2781/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.9414 - val_loss: 1391.6084\n",
            "Epoch 2782/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.9311 - val_loss: 1391.5430\n",
            "Epoch 2783/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.9283 - val_loss: 1391.5043\n",
            "Epoch 2784/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.9204 - val_loss: 1391.4614\n",
            "Epoch 2785/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.9561 - val_loss: 1391.7261\n",
            "Epoch 2786/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.9536 - val_loss: 1391.8185\n",
            "Epoch 2787/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.9018 - val_loss: 1391.9154\n",
            "Epoch 2788/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.8888 - val_loss: 1392.0099\n",
            "Epoch 2789/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.8771 - val_loss: 1391.9471\n",
            "Epoch 2790/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.8697 - val_loss: 1391.9237\n",
            "Epoch 2791/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.8651 - val_loss: 1392.0380\n",
            "Epoch 2792/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.8900 - val_loss: 1392.3015\n",
            "Epoch 2793/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.8667 - val_loss: 1392.4446\n",
            "Epoch 2794/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1075.0261 - val_loss: 1391.9611\n",
            "Epoch 2795/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.8092 - val_loss: 1392.0039\n",
            "Epoch 2796/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.9792 - val_loss: 1392.4647\n",
            "Epoch 2797/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1074.8011 - val_loss: 1392.3354\n",
            "Epoch 2798/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.8271 - val_loss: 1392.0687\n",
            "Epoch 2799/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.7836 - val_loss: 1391.9553\n",
            "Epoch 2800/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.8213 - val_loss: 1392.2030\n",
            "Epoch 2801/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.7505 - val_loss: 1392.1687\n",
            "Epoch 2802/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.7662 - val_loss: 1391.9508\n",
            "Epoch 2803/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.7436 - val_loss: 1391.9506\n",
            "Epoch 2804/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1074.7410 - val_loss: 1391.7704\n",
            "Epoch 2805/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.7897 - val_loss: 1391.4626\n",
            "Epoch 2806/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.7071 - val_loss: 1391.4561\n",
            "Epoch 2807/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.6889 - val_loss: 1391.5979\n",
            "Epoch 2808/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.6862 - val_loss: 1391.6350\n",
            "Epoch 2809/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.6923 - val_loss: 1391.8728\n",
            "Epoch 2810/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.6919 - val_loss: 1392.0869\n",
            "Epoch 2811/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1074.6650 - val_loss: 1391.9534\n",
            "Epoch 2812/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.6546 - val_loss: 1391.8810\n",
            "Epoch 2813/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.6811 - val_loss: 1392.1145\n",
            "Epoch 2814/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.7230 - val_loss: 1391.8556\n",
            "Epoch 2815/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.6094 - val_loss: 1391.8883\n",
            "Epoch 2816/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.6333 - val_loss: 1392.0609\n",
            "Epoch 2817/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.6237 - val_loss: 1391.8726\n",
            "Epoch 2818/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.6054 - val_loss: 1392.0035\n",
            "Epoch 2819/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.5779 - val_loss: 1391.9105\n",
            "Epoch 2820/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.5977 - val_loss: 1391.9950\n",
            "Epoch 2821/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.6557 - val_loss: 1391.5969\n",
            "Epoch 2822/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.6057 - val_loss: 1391.7743\n",
            "Epoch 2823/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.5350 - val_loss: 1391.6548\n",
            "Epoch 2824/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.5696 - val_loss: 1391.3948\n",
            "Epoch 2825/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.5182 - val_loss: 1391.4095\n",
            "Epoch 2826/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.5282 - val_loss: 1391.3035\n",
            "Epoch 2827/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.5331 - val_loss: 1391.2172\n",
            "Epoch 2828/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.4946 - val_loss: 1391.3768\n",
            "Epoch 2829/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.5179 - val_loss: 1391.6688\n",
            "Epoch 2830/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.4915 - val_loss: 1391.8455\n",
            "Epoch 2831/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4957 - val_loss: 1391.8254\n",
            "Epoch 2832/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4664 - val_loss: 1391.8234\n",
            "Epoch 2833/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4507 - val_loss: 1391.6583\n",
            "Epoch 2834/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.5215 - val_loss: 1391.8069\n",
            "Epoch 2835/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.4281 - val_loss: 1391.5952\n",
            "Epoch 2836/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.5886 - val_loss: 1391.2852\n",
            "Epoch 2837/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.4543 - val_loss: 1391.2844\n",
            "Epoch 2838/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.4001 - val_loss: 1391.1010\n",
            "Epoch 2839/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.5088 - val_loss: 1390.7734\n",
            "Epoch 2840/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4050 - val_loss: 1390.8480\n",
            "Epoch 2841/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.4555 - val_loss: 1391.1760\n",
            "Epoch 2842/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4426 - val_loss: 1390.9706\n",
            "Epoch 2843/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.4275 - val_loss: 1391.1748\n",
            "Epoch 2844/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1074.3711 - val_loss: 1391.2251\n",
            "Epoch 2845/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.3875 - val_loss: 1390.9816\n",
            "Epoch 2846/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.3579 - val_loss: 1391.0582\n",
            "Epoch 2847/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.3654 - val_loss: 1390.8854\n",
            "Epoch 2848/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.3418 - val_loss: 1390.8677\n",
            "Epoch 2849/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.3216 - val_loss: 1391.0530\n",
            "Epoch 2850/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.3212 - val_loss: 1391.3726\n",
            "Epoch 2851/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.3289 - val_loss: 1391.5610\n",
            "Epoch 2852/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.3005 - val_loss: 1391.6995\n",
            "Epoch 2853/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1074.2999 - val_loss: 1391.6886\n",
            "Epoch 2854/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.3766 - val_loss: 1391.8788\n",
            "Epoch 2855/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.3078 - val_loss: 1391.9296\n",
            "Epoch 2856/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.2619 - val_loss: 1391.7706\n",
            "Epoch 2857/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.2591 - val_loss: 1391.6506\n",
            "Epoch 2858/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.2439 - val_loss: 1391.7434\n",
            "Epoch 2859/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.2367 - val_loss: 1391.6318\n",
            "Epoch 2860/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.2194 - val_loss: 1391.6167\n",
            "Epoch 2861/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.2085 - val_loss: 1391.6617\n",
            "Epoch 2862/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.2170 - val_loss: 1391.7769\n",
            "Epoch 2863/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.1985 - val_loss: 1391.7168\n",
            "Epoch 2864/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.2023 - val_loss: 1391.6390\n",
            "Epoch 2865/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.2145 - val_loss: 1391.7305\n",
            "Epoch 2866/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1074.1805 - val_loss: 1391.6547\n",
            "Epoch 2867/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.1917 - val_loss: 1391.5284\n",
            "Epoch 2868/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.1726 - val_loss: 1391.6691\n",
            "Epoch 2869/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.2014 - val_loss: 1391.8737\n",
            "Epoch 2870/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1074.2311 - val_loss: 1392.1426\n",
            "Epoch 2871/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.1465 - val_loss: 1391.9761\n",
            "Epoch 2872/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.2247 - val_loss: 1392.1743\n",
            "Epoch 2873/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.1797 - val_loss: 1391.7727\n",
            "Epoch 2874/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.1146 - val_loss: 1391.6985\n",
            "Epoch 2875/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.1140 - val_loss: 1391.5679\n",
            "Epoch 2876/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.1279 - val_loss: 1391.7122\n",
            "Epoch 2877/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.1499 - val_loss: 1391.8339\n",
            "Epoch 2878/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.1086 - val_loss: 1391.4893\n",
            "Epoch 2879/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.0847 - val_loss: 1391.4666\n",
            "Epoch 2880/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1074.1102 - val_loss: 1391.5267\n",
            "Epoch 2881/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.0522 - val_loss: 1391.3883\n",
            "Epoch 2882/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1074.0705 - val_loss: 1391.1168\n",
            "Epoch 2883/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.0466 - val_loss: 1391.1056\n",
            "Epoch 2884/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1074.0464 - val_loss: 1391.0919\n",
            "Epoch 2885/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.0494 - val_loss: 1390.8920\n",
            "Epoch 2886/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1074.1086 - val_loss: 1391.1023\n",
            "Epoch 2887/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.0823 - val_loss: 1391.1167\n",
            "Epoch 2888/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1073.9990 - val_loss: 1390.7782\n",
            "Epoch 2889/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.9985 - val_loss: 1390.5573\n",
            "Epoch 2890/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.0802 - val_loss: 1390.1434\n",
            "Epoch 2891/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1074.0417 - val_loss: 1390.1084\n",
            "Epoch 2892/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1074.0554 - val_loss: 1390.3898\n",
            "Epoch 2893/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1074.0584 - val_loss: 1390.2007\n",
            "Epoch 2894/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.9776 - val_loss: 1390.3481\n",
            "Epoch 2895/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.9874 - val_loss: 1390.3854\n",
            "Epoch 2896/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.9541 - val_loss: 1390.5800\n",
            "Epoch 2897/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.9511 - val_loss: 1390.8143\n",
            "Epoch 2898/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1074.0297 - val_loss: 1391.1786\n",
            "Epoch 2899/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.9595 - val_loss: 1391.2960\n",
            "Epoch 2900/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.9406 - val_loss: 1391.2751\n",
            "Epoch 2901/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.9191 - val_loss: 1391.2949\n",
            "Epoch 2902/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.9499 - val_loss: 1391.0817\n",
            "Epoch 2903/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.9546 - val_loss: 1391.3102\n",
            "Epoch 2904/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.8969 - val_loss: 1391.1718\n",
            "Epoch 2905/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.8876 - val_loss: 1391.1163\n",
            "Epoch 2906/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.8844 - val_loss: 1391.1501\n",
            "Epoch 2907/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.8865 - val_loss: 1391.2889\n",
            "Epoch 2908/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.8658 - val_loss: 1391.3181\n",
            "Epoch 2909/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.9262 - val_loss: 1391.2703\n",
            "Epoch 2910/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.8623 - val_loss: 1391.0521\n",
            "Epoch 2911/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.8684 - val_loss: 1391.0863\n",
            "Epoch 2912/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.8637 - val_loss: 1390.7926\n",
            "Epoch 2913/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.8486 - val_loss: 1390.6542\n",
            "Epoch 2914/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.9009 - val_loss: 1390.9122\n",
            "Epoch 2915/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.8383 - val_loss: 1391.0182\n",
            "Epoch 2916/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.8311 - val_loss: 1391.0306\n",
            "Epoch 2917/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.8346 - val_loss: 1391.1046\n",
            "Epoch 2918/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.7818 - val_loss: 1390.9716\n",
            "Epoch 2919/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.8711 - val_loss: 1390.7180\n",
            "Epoch 2920/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.7814 - val_loss: 1390.4886\n",
            "Epoch 2921/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.8017 - val_loss: 1390.4922\n",
            "Epoch 2922/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.7706 - val_loss: 1390.2991\n",
            "Epoch 2923/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.8476 - val_loss: 1390.0311\n",
            "Epoch 2924/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.9576 - val_loss: 1390.4180\n",
            "Epoch 2925/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.7718 - val_loss: 1390.2362\n",
            "Epoch 2926/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.7725 - val_loss: 1390.1836\n",
            "Epoch 2927/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.8169 - val_loss: 1390.4465\n",
            "Epoch 2928/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1226.66 - 0s 28us/sample - loss: 1073.7376 - val_loss: 1390.4934\n",
            "Epoch 2929/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.7279 - val_loss: 1390.5333\n",
            "Epoch 2930/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.7239 - val_loss: 1390.5314\n",
            "Epoch 2931/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.7100 - val_loss: 1390.4148\n",
            "Epoch 2932/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.7519 - val_loss: 1390.2454\n",
            "Epoch 2933/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.7115 - val_loss: 1390.4329\n",
            "Epoch 2934/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.7417 - val_loss: 1390.6902\n",
            "Epoch 2935/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1124.00 - 0s 29us/sample - loss: 1073.7665 - val_loss: 1390.4926\n",
            "Epoch 2936/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.7353 - val_loss: 1390.5035\n",
            "Epoch 2937/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.7796 - val_loss: 1391.0990\n",
            "Epoch 2938/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.6463 - val_loss: 1391.2723\n",
            "Epoch 2939/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.6375 - val_loss: 1391.3707\n",
            "Epoch 2940/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.6471 - val_loss: 1391.3960\n",
            "Epoch 2941/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.6603 - val_loss: 1391.6982\n",
            "Epoch 2942/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.8645 - val_loss: 1391.4604\n",
            "Epoch 2943/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.6696 - val_loss: 1391.9652\n",
            "Epoch 2944/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.7356 - val_loss: 1392.3683\n",
            "Epoch 2945/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.6400 - val_loss: 1392.1997\n",
            "Epoch 2946/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.6227 - val_loss: 1392.3037\n",
            "Epoch 2947/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.6184 - val_loss: 1392.2101\n",
            "Epoch 2948/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.6410 - val_loss: 1391.9784\n",
            "Epoch 2949/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.5954 - val_loss: 1392.0907\n",
            "Epoch 2950/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.6391 - val_loss: 1391.9189\n",
            "Epoch 2951/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.5838 - val_loss: 1391.6714\n",
            "Epoch 2952/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.6031 - val_loss: 1391.3562\n",
            "Epoch 2953/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.6395 - val_loss: 1391.5585\n",
            "Epoch 2954/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.6128 - val_loss: 1391.6879\n",
            "Epoch 2955/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.5269 - val_loss: 1391.4478\n",
            "Epoch 2956/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1073.5297 - val_loss: 1391.3595\n",
            "Epoch 2957/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.5477 - val_loss: 1391.0973\n",
            "Epoch 2958/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.5619 - val_loss: 1391.0287\n",
            "Epoch 2959/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.6256 - val_loss: 1390.5182\n",
            "Epoch 2960/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.6523 - val_loss: 1390.6472\n",
            "Epoch 2961/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.5470 - val_loss: 1390.1486\n",
            "Epoch 2962/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.5266 - val_loss: 1389.8853\n",
            "Epoch 2963/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.5693 - val_loss: 1389.6890\n",
            "Epoch 2964/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.5489 - val_loss: 1389.8076\n",
            "Epoch 2965/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1073.5175 - val_loss: 1389.7380\n",
            "Epoch 2966/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.5004 - val_loss: 1389.8812\n",
            "Epoch 2967/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.4975 - val_loss: 1389.9060\n",
            "Epoch 2968/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.5311 - val_loss: 1389.9250\n",
            "Epoch 2969/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1073.4660 - val_loss: 1390.1788\n",
            "Epoch 2970/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.7961 - val_loss: 1390.8008\n",
            "Epoch 2971/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.4651 - val_loss: 1390.6434\n",
            "Epoch 2972/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1073.4259 - val_loss: 1390.7063\n",
            "Epoch 2973/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.4336 - val_loss: 1390.8074\n",
            "Epoch 2974/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.4625 - val_loss: 1391.0403\n",
            "Epoch 2975/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.4586 - val_loss: 1391.2130\n",
            "Epoch 2976/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.3949 - val_loss: 1391.1205\n",
            "Epoch 2977/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.4489 - val_loss: 1391.2357\n",
            "Epoch 2978/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.4150 - val_loss: 1390.9305\n",
            "Epoch 2979/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.3801 - val_loss: 1390.8004\n",
            "Epoch 2980/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.3934 - val_loss: 1390.7717\n",
            "Epoch 2981/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.3924 - val_loss: 1390.7444\n",
            "Epoch 2982/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.3758 - val_loss: 1390.6971\n",
            "Epoch 2983/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.3609 - val_loss: 1390.6449\n",
            "Epoch 2984/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.4535 - val_loss: 1390.4270\n",
            "Epoch 2985/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.3590 - val_loss: 1390.6687\n",
            "Epoch 2986/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.3655 - val_loss: 1390.9811\n",
            "Epoch 2987/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.3327 - val_loss: 1391.1572\n",
            "Epoch 2988/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.3336 - val_loss: 1391.2512\n",
            "Epoch 2989/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.3204 - val_loss: 1391.5402\n",
            "Epoch 2990/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.4166 - val_loss: 1391.9006\n",
            "Epoch 2991/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.4041 - val_loss: 1392.1204\n",
            "Epoch 2992/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1073.3013 - val_loss: 1391.9852\n",
            "Epoch 2993/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.4287 - val_loss: 1391.4160\n",
            "Epoch 2994/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.3464 - val_loss: 1391.4701\n",
            "Epoch 2995/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.2770 - val_loss: 1391.4543\n",
            "Epoch 2996/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.2656 - val_loss: 1391.4366\n",
            "Epoch 2997/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.2728 - val_loss: 1391.3141\n",
            "Epoch 2998/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.2820 - val_loss: 1391.3916\n",
            "Epoch 2999/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2848 - val_loss: 1391.3030\n",
            "Epoch 3000/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.2462 - val_loss: 1391.3412\n",
            "Epoch 3001/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2349 - val_loss: 1391.4525\n",
            "Epoch 3002/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.3166 - val_loss: 1391.7639\n",
            "Epoch 3003/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.2267 - val_loss: 1391.7098\n",
            "Epoch 3004/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.2477 - val_loss: 1391.6022\n",
            "Epoch 3005/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1073.2356 - val_loss: 1391.4655\n",
            "Epoch 3006/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.2115 - val_loss: 1391.5334\n",
            "Epoch 3007/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.2452 - val_loss: 1391.3833\n",
            "Epoch 3008/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.2497 - val_loss: 1391.3497\n",
            "Epoch 3009/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2438 - val_loss: 1391.5199\n",
            "Epoch 3010/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.2682 - val_loss: 1392.1116\n",
            "Epoch 3011/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2976 - val_loss: 1392.0349\n",
            "Epoch 3012/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2209 - val_loss: 1392.3499\n",
            "Epoch 3013/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1073.1456 - val_loss: 1392.8192\n",
            "Epoch 3014/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.3552 - val_loss: 1393.4526\n",
            "Epoch 3015/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2438 - val_loss: 1393.5448\n",
            "Epoch 3016/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.2427 - val_loss: 1393.6449\n",
            "Epoch 3017/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.2146 - val_loss: 1393.7454\n",
            "Epoch 3018/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.2235 - val_loss: 1393.6901\n",
            "Epoch 3019/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1954 - val_loss: 1393.7434\n",
            "Epoch 3020/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1073.2866 - val_loss: 1393.8715\n",
            "Epoch 3021/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1073.2170 - val_loss: 1393.3629\n",
            "Epoch 3022/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1619 - val_loss: 1393.0264\n",
            "Epoch 3023/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.1469 - val_loss: 1392.7312\n",
            "Epoch 3024/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.1189 - val_loss: 1392.3918\n",
            "Epoch 3025/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.1443 - val_loss: 1392.0076\n",
            "Epoch 3026/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1252 - val_loss: 1391.7396\n",
            "Epoch 3027/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.1354 - val_loss: 1391.5859\n",
            "Epoch 3028/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1107 - val_loss: 1391.5492\n",
            "Epoch 3029/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.0945 - val_loss: 1391.5991\n",
            "Epoch 3030/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1107 - val_loss: 1391.9540\n",
            "Epoch 3031/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1073.0934 - val_loss: 1392.1238\n",
            "Epoch 3032/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.1466 - val_loss: 1392.2993\n",
            "Epoch 3033/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.1837 - val_loss: 1391.8210\n",
            "Epoch 3034/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.1350 - val_loss: 1391.4968\n",
            "Epoch 3035/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.0522 - val_loss: 1391.6123\n",
            "Epoch 3036/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.0896 - val_loss: 1391.5729\n",
            "Epoch 3037/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.0475 - val_loss: 1391.5942\n",
            "Epoch 3038/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.0356 - val_loss: 1391.6051\n",
            "Epoch 3039/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1073.0238 - val_loss: 1391.8276\n",
            "Epoch 3040/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.0321 - val_loss: 1391.9041\n",
            "Epoch 3041/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.0105 - val_loss: 1392.0558\n",
            "Epoch 3042/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.0559 - val_loss: 1392.2501\n",
            "Epoch 3043/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1073.0416 - val_loss: 1391.9988\n",
            "Epoch 3044/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.0095 - val_loss: 1391.9399\n",
            "Epoch 3045/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1073.0303 - val_loss: 1391.7129\n",
            "Epoch 3046/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1073.0041 - val_loss: 1391.7991\n",
            "Epoch 3047/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1073.0442 - val_loss: 1391.6971\n",
            "Epoch 3048/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.0053 - val_loss: 1392.0344\n",
            "Epoch 3049/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.9744 - val_loss: 1392.0728\n",
            "Epoch 3050/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.9662 - val_loss: 1392.1953\n",
            "Epoch 3051/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.9710 - val_loss: 1392.3372\n",
            "Epoch 3052/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.9610 - val_loss: 1392.3585\n",
            "Epoch 3053/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.0674 - val_loss: 1391.9584\n",
            "Epoch 3054/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1073.0479 - val_loss: 1392.2333\n",
            "Epoch 3055/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.9991 - val_loss: 1391.8810\n",
            "Epoch 3056/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1073.0177 - val_loss: 1391.9756\n",
            "Epoch 3057/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.9112 - val_loss: 1391.8145\n",
            "Epoch 3058/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.9250 - val_loss: 1391.5244\n",
            "Epoch 3059/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.9074 - val_loss: 1391.3917\n",
            "Epoch 3060/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.8920 - val_loss: 1391.1471\n",
            "Epoch 3061/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.9008 - val_loss: 1391.0720\n",
            "Epoch 3062/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.8942 - val_loss: 1390.8182\n",
            "Epoch 3063/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.9038 - val_loss: 1390.6102\n",
            "Epoch 3064/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.8884 - val_loss: 1390.6227\n",
            "Epoch 3065/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.8900 - val_loss: 1390.6251\n",
            "Epoch 3066/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.8989 - val_loss: 1390.4624\n",
            "Epoch 3067/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.9419 - val_loss: 1390.6709\n",
            "Epoch 3068/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.9468 - val_loss: 1390.7406\n",
            "Epoch 3069/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.8528 - val_loss: 1390.5382\n",
            "Epoch 3070/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.8498 - val_loss: 1390.3719\n",
            "Epoch 3071/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.9011 - val_loss: 1390.0908\n",
            "Epoch 3072/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.8679 - val_loss: 1389.9933\n",
            "Epoch 3073/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.8642 - val_loss: 1389.9213\n",
            "Epoch 3074/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.8661 - val_loss: 1389.8820\n",
            "Epoch 3075/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.8747 - val_loss: 1389.9232\n",
            "Epoch 3076/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.8789 - val_loss: 1389.9332\n",
            "Epoch 3077/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.8830 - val_loss: 1390.3517\n",
            "Epoch 3078/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.8649 - val_loss: 1390.5226\n",
            "Epoch 3079/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.8298 - val_loss: 1390.7208\n",
            "Epoch 3080/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.8283 - val_loss: 1390.6609\n",
            "Epoch 3081/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.8844 - val_loss: 1390.8994\n",
            "Epoch 3082/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.7883 - val_loss: 1391.0144\n",
            "Epoch 3083/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1072.8488 - val_loss: 1391.3289\n",
            "Epoch 3084/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.7837 - val_loss: 1391.4065\n",
            "Epoch 3085/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.7682 - val_loss: 1391.3735\n",
            "Epoch 3086/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.7861 - val_loss: 1391.5084\n",
            "Epoch 3087/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.7566 - val_loss: 1391.4852\n",
            "Epoch 3088/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.7620 - val_loss: 1391.4196\n",
            "Epoch 3089/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.7584 - val_loss: 1391.4261\n",
            "Epoch 3090/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.7494 - val_loss: 1391.2726\n",
            "Epoch 3091/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.8122 - val_loss: 1391.0409\n",
            "Epoch 3092/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.7599 - val_loss: 1391.2803\n",
            "Epoch 3093/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.7331 - val_loss: 1391.2930\n",
            "Epoch 3094/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.7349 - val_loss: 1391.3129\n",
            "Epoch 3095/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.7398 - val_loss: 1391.3162\n",
            "Epoch 3096/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.7110 - val_loss: 1391.4534\n",
            "Epoch 3097/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.7075 - val_loss: 1391.7584\n",
            "Epoch 3098/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.7596 - val_loss: 1392.1099\n",
            "Epoch 3099/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.7021 - val_loss: 1392.0959\n",
            "Epoch 3100/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.7326 - val_loss: 1391.9299\n",
            "Epoch 3101/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.7216 - val_loss: 1392.0891\n",
            "Epoch 3102/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.6843 - val_loss: 1392.0627\n",
            "Epoch 3103/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6982 - val_loss: 1392.0327\n",
            "Epoch 3104/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.7357 - val_loss: 1392.1880\n",
            "Epoch 3105/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.7260 - val_loss: 1392.1866\n",
            "Epoch 3106/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.7138 - val_loss: 1391.6710\n",
            "Epoch 3107/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6966 - val_loss: 1391.3282\n",
            "Epoch 3108/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.6752 - val_loss: 1391.3669\n",
            "Epoch 3109/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6963 - val_loss: 1391.3990\n",
            "Epoch 3110/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.7825 - val_loss: 1390.8884\n",
            "Epoch 3111/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.6979 - val_loss: 1391.0083\n",
            "Epoch 3112/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.6450 - val_loss: 1390.8093\n",
            "Epoch 3113/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.6904 - val_loss: 1390.6917\n",
            "Epoch 3114/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.7019 - val_loss: 1390.7067\n",
            "Epoch 3115/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.6472 - val_loss: 1390.7712\n",
            "Epoch 3116/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.6314 - val_loss: 1390.8252\n",
            "Epoch 3117/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.6695 - val_loss: 1390.6335\n",
            "Epoch 3118/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.6845 - val_loss: 1390.5466\n",
            "Epoch 3119/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.7813 - val_loss: 1391.0864\n",
            "Epoch 3120/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.6446 - val_loss: 1391.1196\n",
            "Epoch 3121/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.7353 - val_loss: 1391.5813\n",
            "Epoch 3122/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.5875 - val_loss: 1391.5360\n",
            "Epoch 3123/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.6140 - val_loss: 1391.4302\n",
            "Epoch 3124/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.6194 - val_loss: 1391.6266\n",
            "Epoch 3125/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.6872 - val_loss: 1391.9775\n",
            "Epoch 3126/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.6032 - val_loss: 1391.7919\n",
            "Epoch 3127/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.6610 - val_loss: 1391.7994\n",
            "Epoch 3128/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.5715 - val_loss: 1391.5785\n",
            "Epoch 3129/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.5649 - val_loss: 1391.4906\n",
            "Epoch 3130/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6035 - val_loss: 1391.5474\n",
            "Epoch 3131/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6087 - val_loss: 1391.1685\n",
            "Epoch 3132/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.5525 - val_loss: 1391.2301\n",
            "Epoch 3133/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.5889 - val_loss: 1390.9962\n",
            "Epoch 3134/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.5460 - val_loss: 1391.0098\n",
            "Epoch 3135/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.5978 - val_loss: 1391.3707\n",
            "Epoch 3136/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.5177 - val_loss: 1391.4135\n",
            "Epoch 3137/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.5177 - val_loss: 1391.4340\n",
            "Epoch 3138/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.5483 - val_loss: 1391.6368\n",
            "Epoch 3139/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.5094 - val_loss: 1391.6494\n",
            "Epoch 3140/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.5594 - val_loss: 1391.4518\n",
            "Epoch 3141/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.5503 - val_loss: 1391.6763\n",
            "Epoch 3142/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4922 - val_loss: 1391.8595\n",
            "Epoch 3143/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.4951 - val_loss: 1392.0585\n",
            "Epoch 3144/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.4880 - val_loss: 1392.1058\n",
            "Epoch 3145/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4801 - val_loss: 1392.1696\n",
            "Epoch 3146/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.5035 - val_loss: 1392.0314\n",
            "Epoch 3147/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.5146 - val_loss: 1391.8706\n",
            "Epoch 3148/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4587 - val_loss: 1391.9639\n",
            "Epoch 3149/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.5027 - val_loss: 1391.9388\n",
            "Epoch 3150/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.4535 - val_loss: 1392.1733\n",
            "Epoch 3151/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.4811 - val_loss: 1392.4868\n",
            "Epoch 3152/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1072.4694 - val_loss: 1392.5032\n",
            "Epoch 3153/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1072.4429 - val_loss: 1392.6064\n",
            "Epoch 3154/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1072.4603 - val_loss: 1392.6168\n",
            "Epoch 3155/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.4558 - val_loss: 1392.4363\n",
            "Epoch 3156/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.4410 - val_loss: 1392.5062\n",
            "Epoch 3157/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4176 - val_loss: 1392.4225\n",
            "Epoch 3158/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4279 - val_loss: 1392.3302\n",
            "Epoch 3159/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.4228 - val_loss: 1392.1689\n",
            "Epoch 3160/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4014 - val_loss: 1392.0664\n",
            "Epoch 3161/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.4251 - val_loss: 1391.8357\n",
            "Epoch 3162/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.4003 - val_loss: 1391.6782\n",
            "Epoch 3163/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.4701 - val_loss: 1391.4707\n",
            "Epoch 3164/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.3921 - val_loss: 1391.5641\n",
            "Epoch 3165/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.4033 - val_loss: 1391.9025\n",
            "Epoch 3166/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.4193 - val_loss: 1392.2705\n",
            "Epoch 3167/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.3759 - val_loss: 1392.3292\n",
            "Epoch 3168/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.3742 - val_loss: 1392.2780\n",
            "Epoch 3169/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3639 - val_loss: 1392.3170\n",
            "Epoch 3170/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.5146 - val_loss: 1392.0624\n",
            "Epoch 3171/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.3485 - val_loss: 1392.2130\n",
            "Epoch 3172/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.3926 - val_loss: 1392.6661\n",
            "Epoch 3173/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.3958 - val_loss: 1392.8733\n",
            "Epoch 3174/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.6431 - val_loss: 1392.3792\n",
            "Epoch 3175/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.3348 - val_loss: 1392.6136\n",
            "Epoch 3176/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3609 - val_loss: 1392.8344\n",
            "Epoch 3177/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3620 - val_loss: 1392.7040\n",
            "Epoch 3178/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3384 - val_loss: 1392.8076\n",
            "Epoch 3179/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.3978 - val_loss: 1392.9496\n",
            "Epoch 3180/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.3260 - val_loss: 1392.8026\n",
            "Epoch 3181/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.3343 - val_loss: 1392.6715\n",
            "Epoch 3182/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.3132 - val_loss: 1392.5018\n",
            "Epoch 3183/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3169 - val_loss: 1392.5518\n",
            "Epoch 3184/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.2996 - val_loss: 1392.3323\n",
            "Epoch 3185/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.2882 - val_loss: 1392.1793\n",
            "Epoch 3186/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.3349 - val_loss: 1391.8541\n",
            "Epoch 3187/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2913 - val_loss: 1391.7617\n",
            "Epoch 3188/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.2936 - val_loss: 1391.6249\n",
            "Epoch 3189/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.2786 - val_loss: 1391.7301\n",
            "Epoch 3190/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.3120 - val_loss: 1391.8534\n",
            "Epoch 3191/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.3195 - val_loss: 1391.6177\n",
            "Epoch 3192/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2717 - val_loss: 1391.7155\n",
            "Epoch 3193/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.2808 - val_loss: 1391.8596\n",
            "Epoch 3194/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2523 - val_loss: 1391.8099\n",
            "Epoch 3195/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2535 - val_loss: 1391.7521\n",
            "Epoch 3196/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1072.2771 - val_loss: 1391.8484\n",
            "Epoch 3197/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1072.2440 - val_loss: 1391.8239\n",
            "Epoch 3198/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2411 - val_loss: 1391.7593\n",
            "Epoch 3199/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2599 - val_loss: 1391.7733\n",
            "Epoch 3200/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.2438 - val_loss: 1391.5889\n",
            "Epoch 3201/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.2607 - val_loss: 1391.4679\n",
            "Epoch 3202/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.2482 - val_loss: 1391.0713\n",
            "Epoch 3203/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.2259 - val_loss: 1390.9227\n",
            "Epoch 3204/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.2284 - val_loss: 1390.9241\n",
            "Epoch 3205/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.3019 - val_loss: 1390.6472\n",
            "Epoch 3206/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1072.2413 - val_loss: 1390.7833\n",
            "Epoch 3207/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.2566 - val_loss: 1390.7584\n",
            "Epoch 3208/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1072.2087 - val_loss: 1391.0603\n",
            "Epoch 3209/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.2071 - val_loss: 1391.4662\n",
            "Epoch 3210/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1072.2890 - val_loss: 1391.8654\n",
            "Epoch 3211/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1072.1816 - val_loss: 1391.9440\n",
            "Epoch 3212/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.3242 - val_loss: 1391.7755\n",
            "Epoch 3213/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1870 - val_loss: 1391.8734\n",
            "Epoch 3214/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1680 - val_loss: 1391.8234\n",
            "Epoch 3215/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.1688 - val_loss: 1391.7657\n",
            "Epoch 3216/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.1681 - val_loss: 1391.7344\n",
            "Epoch 3217/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.1714 - val_loss: 1391.8290\n",
            "Epoch 3218/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1966 - val_loss: 1392.0891\n",
            "Epoch 3219/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.2123 - val_loss: 1392.0127\n",
            "Epoch 3220/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.3163 - val_loss: 1391.7012\n",
            "Epoch 3221/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.1685 - val_loss: 1392.0358\n",
            "Epoch 3222/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.1792 - val_loss: 1392.0571\n",
            "Epoch 3223/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.1512 - val_loss: 1392.1874\n",
            "Epoch 3224/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1072.1318 - val_loss: 1392.1920\n",
            "Epoch 3225/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1471 - val_loss: 1392.0878\n",
            "Epoch 3226/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.1420 - val_loss: 1392.0442\n",
            "Epoch 3227/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.2647 - val_loss: 1392.4410\n",
            "Epoch 3228/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.1391 - val_loss: 1392.4102\n",
            "Epoch 3229/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1175 - val_loss: 1392.1943\n",
            "Epoch 3230/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.1223 - val_loss: 1391.9557\n",
            "Epoch 3231/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.1660 - val_loss: 1392.0723\n",
            "Epoch 3232/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.1027 - val_loss: 1391.8383\n",
            "Epoch 3233/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.4204 - val_loss: 1391.2943\n",
            "Epoch 3234/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.1123 - val_loss: 1391.3840\n",
            "Epoch 3235/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0863 - val_loss: 1391.7125\n",
            "Epoch 3236/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.1567 - val_loss: 1392.1735\n",
            "Epoch 3237/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1002 - val_loss: 1392.2844\n",
            "Epoch 3238/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.0874 - val_loss: 1392.2268\n",
            "Epoch 3239/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1072.1414 - val_loss: 1392.3992\n",
            "Epoch 3240/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0814 - val_loss: 1392.1796\n",
            "Epoch 3241/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0710 - val_loss: 1392.1200\n",
            "Epoch 3242/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0610 - val_loss: 1391.9791\n",
            "Epoch 3243/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1072.1909 - val_loss: 1392.0769\n",
            "Epoch 3244/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0622 - val_loss: 1391.8806\n",
            "Epoch 3245/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.1216 - val_loss: 1391.8827\n",
            "Epoch 3246/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1072.0592 - val_loss: 1391.6196\n",
            "Epoch 3247/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0495 - val_loss: 1391.4536\n",
            "Epoch 3248/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1072.0519 - val_loss: 1391.4576\n",
            "Epoch 3249/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.1112 - val_loss: 1391.5884\n",
            "Epoch 3250/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0557 - val_loss: 1391.4810\n",
            "Epoch 3251/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0401 - val_loss: 1391.4399\n",
            "Epoch 3252/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0678 - val_loss: 1391.6271\n",
            "Epoch 3253/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0694 - val_loss: 1391.4358\n",
            "Epoch 3254/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0425 - val_loss: 1391.4196\n",
            "Epoch 3255/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0872 - val_loss: 1391.4658\n",
            "Epoch 3256/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0206 - val_loss: 1391.5876\n",
            "Epoch 3257/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0262 - val_loss: 1391.7454\n",
            "Epoch 3258/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0791 - val_loss: 1391.9733\n",
            "Epoch 3259/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0584 - val_loss: 1392.0946\n",
            "Epoch 3260/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0353 - val_loss: 1391.7968\n",
            "Epoch 3261/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0220 - val_loss: 1391.5503\n",
            "Epoch 3262/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0622 - val_loss: 1391.3671\n",
            "Epoch 3263/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0238 - val_loss: 1391.5093\n",
            "Epoch 3264/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.9802 - val_loss: 1391.6718\n",
            "Epoch 3265/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9953 - val_loss: 1391.9509\n",
            "Epoch 3266/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0120 - val_loss: 1391.9171\n",
            "Epoch 3267/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.9713 - val_loss: 1392.1156\n",
            "Epoch 3268/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9573 - val_loss: 1392.2665\n",
            "Epoch 3269/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1072.0733 - val_loss: 1392.6609\n",
            "Epoch 3270/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1071.9671 - val_loss: 1392.6802\n",
            "Epoch 3271/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.9797 - val_loss: 1392.9373\n",
            "Epoch 3272/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9585 - val_loss: 1392.9594\n",
            "Epoch 3273/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1072.0069 - val_loss: 1392.8304\n",
            "Epoch 3274/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9471 - val_loss: 1392.9109\n",
            "Epoch 3275/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.9974 - val_loss: 1393.1624\n",
            "Epoch 3276/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.9778 - val_loss: 1393.0009\n",
            "Epoch 3277/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.9976 - val_loss: 1392.8578\n",
            "Epoch 3278/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.9356 - val_loss: 1393.0446\n",
            "Epoch 3279/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9483 - val_loss: 1393.2493\n",
            "Epoch 3280/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.9596 - val_loss: 1393.3018\n",
            "Epoch 3281/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.9991 - val_loss: 1393.4315\n",
            "Epoch 3282/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.9410 - val_loss: 1393.1150\n",
            "Epoch 3283/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.9250 - val_loss: 1392.8792\n",
            "Epoch 3284/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.9881 - val_loss: 1392.9250\n",
            "Epoch 3285/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.9254 - val_loss: 1392.5703\n",
            "Epoch 3286/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.9961 - val_loss: 1392.5980\n",
            "Epoch 3287/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.2684 - val_loss: 1391.8215\n",
            "Epoch 3288/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.9516 - val_loss: 1391.6787\n",
            "Epoch 3289/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9188 - val_loss: 1391.6836\n",
            "Epoch 3290/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.9091 - val_loss: 1391.9147\n",
            "Epoch 3291/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9348 - val_loss: 1392.2292\n",
            "Epoch 3292/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.8860 - val_loss: 1392.2432\n",
            "Epoch 3293/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1072.0195 - val_loss: 1392.5787\n",
            "Epoch 3294/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.9244 - val_loss: 1392.3689\n",
            "Epoch 3295/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.8749 - val_loss: 1392.3934\n",
            "Epoch 3296/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.8882 - val_loss: 1392.6174\n",
            "Epoch 3297/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8784 - val_loss: 1392.7478\n",
            "Epoch 3298/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.9057 - val_loss: 1392.8937\n",
            "Epoch 3299/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.8791 - val_loss: 1392.8347\n",
            "Epoch 3300/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.8863 - val_loss: 1392.7981\n",
            "Epoch 3301/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8705 - val_loss: 1392.7169\n",
            "Epoch 3302/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8716 - val_loss: 1392.5669\n",
            "Epoch 3303/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8508 - val_loss: 1392.6006\n",
            "Epoch 3304/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.8688 - val_loss: 1392.4226\n",
            "Epoch 3305/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8567 - val_loss: 1392.3464\n",
            "Epoch 3306/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.9179 - val_loss: 1392.2794\n",
            "Epoch 3307/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8239 - val_loss: 1392.4786\n",
            "Epoch 3308/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8602 - val_loss: 1392.9189\n",
            "Epoch 3309/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.8195 - val_loss: 1393.0630\n",
            "Epoch 3310/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8326 - val_loss: 1393.1115\n",
            "Epoch 3311/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.8856 - val_loss: 1393.3945\n",
            "Epoch 3312/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.8381 - val_loss: 1393.3073\n",
            "Epoch 3313/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8806 - val_loss: 1393.4607\n",
            "Epoch 3314/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8273 - val_loss: 1393.2078\n",
            "Epoch 3315/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8800 - val_loss: 1392.8881\n",
            "Epoch 3316/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8300 - val_loss: 1393.0166\n",
            "Epoch 3317/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.8362 - val_loss: 1392.7786\n",
            "Epoch 3318/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8124 - val_loss: 1392.8770\n",
            "Epoch 3319/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8120 - val_loss: 1392.6699\n",
            "Epoch 3320/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8540 - val_loss: 1392.8011\n",
            "Epoch 3321/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.7854 - val_loss: 1392.7651\n",
            "Epoch 3322/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8003 - val_loss: 1392.7084\n",
            "Epoch 3323/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8056 - val_loss: 1392.6256\n",
            "Epoch 3324/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.7778 - val_loss: 1392.1572\n",
            "Epoch 3325/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.8187 - val_loss: 1391.8112\n",
            "Epoch 3326/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.8366 - val_loss: 1391.8499\n",
            "Epoch 3327/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.8920 - val_loss: 1391.5754\n",
            "Epoch 3328/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7843 - val_loss: 1391.8623\n",
            "Epoch 3329/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7712 - val_loss: 1392.0266\n",
            "Epoch 3330/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.8621 - val_loss: 1391.8904\n",
            "Epoch 3331/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.8433 - val_loss: 1392.3942\n",
            "Epoch 3332/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.7669 - val_loss: 1392.5864\n",
            "Epoch 3333/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.8149 - val_loss: 1392.4730\n",
            "Epoch 3334/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.7328 - val_loss: 1392.6897\n",
            "Epoch 3335/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7420 - val_loss: 1392.9447\n",
            "Epoch 3336/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7352 - val_loss: 1393.0464\n",
            "Epoch 3337/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7377 - val_loss: 1393.0936\n",
            "Epoch 3338/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7756 - val_loss: 1393.3184\n",
            "Epoch 3339/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.7334 - val_loss: 1393.3529\n",
            "Epoch 3340/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.7687 - val_loss: 1393.3069\n",
            "Epoch 3341/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7580 - val_loss: 1393.2468\n",
            "Epoch 3342/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.8120 - val_loss: 1393.6287\n",
            "Epoch 3343/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.7539 - val_loss: 1393.4796\n",
            "Epoch 3344/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.7743 - val_loss: 1393.6007\n",
            "Epoch 3345/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7869 - val_loss: 1393.3285\n",
            "Epoch 3346/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.8721 - val_loss: 1392.7419\n",
            "Epoch 3347/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7110 - val_loss: 1392.7190\n",
            "Epoch 3348/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6960 - val_loss: 1392.5704\n",
            "Epoch 3349/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7184 - val_loss: 1392.3500\n",
            "Epoch 3350/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.7004 - val_loss: 1392.3065\n",
            "Epoch 3351/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.7550 - val_loss: 1392.4745\n",
            "Epoch 3352/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.7430 - val_loss: 1392.2289\n",
            "Epoch 3353/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7709 - val_loss: 1392.5100\n",
            "Epoch 3354/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.7023 - val_loss: 1392.3190\n",
            "Epoch 3355/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.7310 - val_loss: 1392.4193\n",
            "Epoch 3356/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.7716 - val_loss: 1392.6414\n",
            "Epoch 3357/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6957 - val_loss: 1392.6189\n",
            "Epoch 3358/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.7230 - val_loss: 1392.2145\n",
            "Epoch 3359/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.6890 - val_loss: 1391.9814\n",
            "Epoch 3360/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6762 - val_loss: 1391.9568\n",
            "Epoch 3361/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.6659 - val_loss: 1392.0138\n",
            "Epoch 3362/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.6641 - val_loss: 1392.0073\n",
            "Epoch 3363/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.6627 - val_loss: 1392.1160\n",
            "Epoch 3364/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6786 - val_loss: 1392.1116\n",
            "Epoch 3365/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6741 - val_loss: 1392.2587\n",
            "Epoch 3366/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.6981 - val_loss: 1392.5714\n",
            "Epoch 3367/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6512 - val_loss: 1392.5400\n",
            "Epoch 3368/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.7005 - val_loss: 1392.7690\n",
            "Epoch 3369/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6489 - val_loss: 1392.6631\n",
            "Epoch 3370/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.6405 - val_loss: 1392.6394\n",
            "Epoch 3371/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6630 - val_loss: 1392.7646\n",
            "Epoch 3372/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6318 - val_loss: 1392.6798\n",
            "Epoch 3373/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6385 - val_loss: 1392.7882\n",
            "Epoch 3374/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 867.656 - 0s 29us/sample - loss: 1071.6574 - val_loss: 1392.8168\n",
            "Epoch 3375/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6301 - val_loss: 1392.5531\n",
            "Epoch 3376/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.6483 - val_loss: 1392.3586\n",
            "Epoch 3377/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6588 - val_loss: 1392.1671\n",
            "Epoch 3378/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6136 - val_loss: 1392.1871\n",
            "Epoch 3379/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6134 - val_loss: 1392.2958\n",
            "Epoch 3380/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6085 - val_loss: 1392.3970\n",
            "Epoch 3381/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.7164 - val_loss: 1392.6908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3382/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.5979 - val_loss: 1392.5240\n",
            "Epoch 3383/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6263 - val_loss: 1392.2937\n",
            "Epoch 3384/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6146 - val_loss: 1392.2042\n",
            "Epoch 3385/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.6081 - val_loss: 1392.2136\n",
            "Epoch 3386/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.6118 - val_loss: 1392.4957\n",
            "Epoch 3387/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6042 - val_loss: 1392.5643\n",
            "Epoch 3388/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.6072 - val_loss: 1392.6172\n",
            "Epoch 3389/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.5842 - val_loss: 1392.5291\n",
            "Epoch 3390/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6270 - val_loss: 1392.6320\n",
            "Epoch 3391/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.6303 - val_loss: 1392.4886\n",
            "Epoch 3392/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5736 - val_loss: 1392.4523\n",
            "Epoch 3393/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.5731 - val_loss: 1392.4530\n",
            "Epoch 3394/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.5958 - val_loss: 1392.6504\n",
            "Epoch 3395/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5679 - val_loss: 1392.6849\n",
            "Epoch 3396/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.5904 - val_loss: 1392.7622\n",
            "Epoch 3397/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6485 - val_loss: 1392.5236\n",
            "Epoch 3398/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5671 - val_loss: 1392.4374\n",
            "Epoch 3399/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6914 - val_loss: 1392.7314\n",
            "Epoch 3400/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.6920 - val_loss: 1392.8846\n",
            "Epoch 3401/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1382.97 - 0s 30us/sample - loss: 1071.5423 - val_loss: 1392.7261\n",
            "Epoch 3402/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.5266 - val_loss: 1392.4894\n",
            "Epoch 3403/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5321 - val_loss: 1392.2440\n",
            "Epoch 3404/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.6275 - val_loss: 1391.8887\n",
            "Epoch 3405/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.5436 - val_loss: 1391.8136\n",
            "Epoch 3406/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.6516 - val_loss: 1391.5908\n",
            "Epoch 3407/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5529 - val_loss: 1391.7926\n",
            "Epoch 3408/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.5593 - val_loss: 1391.8530\n",
            "Epoch 3409/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5312 - val_loss: 1392.0421\n",
            "Epoch 3410/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.6945 - val_loss: 1392.7028\n",
            "Epoch 3411/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.5858 - val_loss: 1392.6499\n",
            "Epoch 3412/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.5735 - val_loss: 1393.0382\n",
            "Epoch 3413/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5285 - val_loss: 1393.1992\n",
            "Epoch 3414/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.5434 - val_loss: 1393.0752\n",
            "Epoch 3415/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5510 - val_loss: 1393.2759\n",
            "Epoch 3416/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.5306 - val_loss: 1393.3812\n",
            "Epoch 3417/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5577 - val_loss: 1393.2456\n",
            "Epoch 3418/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1071.5172 - val_loss: 1393.3678\n",
            "Epoch 3419/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5028 - val_loss: 1393.3745\n",
            "Epoch 3420/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.5013 - val_loss: 1393.4543\n",
            "Epoch 3421/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.5239 - val_loss: 1393.5361\n",
            "Epoch 3422/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4938 - val_loss: 1393.3434\n",
            "Epoch 3423/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.6366 - val_loss: 1392.9608\n",
            "Epoch 3424/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4828 - val_loss: 1393.1147\n",
            "Epoch 3425/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5326 - val_loss: 1393.2842\n",
            "Epoch 3426/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.4907 - val_loss: 1393.1450\n",
            "Epoch 3427/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5029 - val_loss: 1393.0372\n",
            "Epoch 3428/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4834 - val_loss: 1393.0793\n",
            "Epoch 3429/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4869 - val_loss: 1393.0471\n",
            "Epoch 3430/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.5490 - val_loss: 1393.1123\n",
            "Epoch 3431/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.5120 - val_loss: 1393.1414\n",
            "Epoch 3432/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.4498 - val_loss: 1392.8870\n",
            "Epoch 3433/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4804 - val_loss: 1392.5144\n",
            "Epoch 3434/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.5972 - val_loss: 1392.0424\n",
            "Epoch 3435/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.4979 - val_loss: 1391.9407\n",
            "Epoch 3436/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4834 - val_loss: 1391.8931\n",
            "Epoch 3437/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4593 - val_loss: 1391.9993\n",
            "Epoch 3438/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5361 - val_loss: 1392.2611\n",
            "Epoch 3439/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.4822 - val_loss: 1392.3815\n",
            "Epoch 3440/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4539 - val_loss: 1392.4590\n",
            "Epoch 3441/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4744 - val_loss: 1392.5630\n",
            "Epoch 3442/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4945 - val_loss: 1392.3348\n",
            "Epoch 3443/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.4723 - val_loss: 1392.2773\n",
            "Epoch 3444/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4508 - val_loss: 1392.4471\n",
            "Epoch 3445/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4583 - val_loss: 1392.4072\n",
            "Epoch 3446/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.4533 - val_loss: 1392.4771\n",
            "Epoch 3447/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4245 - val_loss: 1392.5956\n",
            "Epoch 3448/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4301 - val_loss: 1392.6068\n",
            "Epoch 3449/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4205 - val_loss: 1392.6655\n",
            "Epoch 3450/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4469 - val_loss: 1392.8184\n",
            "Epoch 3451/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.4227 - val_loss: 1393.0704\n",
            "Epoch 3452/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.4669 - val_loss: 1393.3622\n",
            "Epoch 3453/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.5297 - val_loss: 1393.0906\n",
            "Epoch 3454/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.4024 - val_loss: 1393.1492\n",
            "Epoch 3455/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4337 - val_loss: 1393.1345\n",
            "Epoch 3456/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4284 - val_loss: 1393.3828\n",
            "Epoch 3457/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.5028 - val_loss: 1393.1951\n",
            "Epoch 3458/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.4093 - val_loss: 1393.5032\n",
            "Epoch 3459/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.4009 - val_loss: 1393.6511\n",
            "Epoch 3460/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.4916 - val_loss: 1393.8490\n",
            "Epoch 3461/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3913 - val_loss: 1393.7517\n",
            "Epoch 3462/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3879 - val_loss: 1393.6085\n",
            "Epoch 3463/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4233 - val_loss: 1393.3954\n",
            "Epoch 3464/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4182 - val_loss: 1393.3822\n",
            "Epoch 3465/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3906 - val_loss: 1393.5066\n",
            "Epoch 3466/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3774 - val_loss: 1393.4535\n",
            "Epoch 3467/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.3736 - val_loss: 1393.3639\n",
            "Epoch 3468/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3665 - val_loss: 1393.2417\n",
            "Epoch 3469/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3670 - val_loss: 1393.1750\n",
            "Epoch 3470/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3619 - val_loss: 1393.1332\n",
            "Epoch 3471/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3667 - val_loss: 1392.9578\n",
            "Epoch 3472/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.4205 - val_loss: 1392.8623\n",
            "Epoch 3473/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3622 - val_loss: 1392.6974\n",
            "Epoch 3474/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3638 - val_loss: 1392.6033\n",
            "Epoch 3475/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3529 - val_loss: 1392.6000\n",
            "Epoch 3476/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.4700 - val_loss: 1392.8575\n",
            "Epoch 3477/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3642 - val_loss: 1392.6747\n",
            "Epoch 3478/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3582 - val_loss: 1392.7465\n",
            "Epoch 3479/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3513 - val_loss: 1392.6414\n",
            "Epoch 3480/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3420 - val_loss: 1392.6234\n",
            "Epoch 3481/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3571 - val_loss: 1392.5956\n",
            "Epoch 3482/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3446 - val_loss: 1392.5935\n",
            "Epoch 3483/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3554 - val_loss: 1392.8361\n",
            "Epoch 3484/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3739 - val_loss: 1392.7457\n",
            "Epoch 3485/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.5290 - val_loss: 1393.2430\n",
            "Epoch 3486/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3265 - val_loss: 1393.1938\n",
            "Epoch 3487/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.4567 - val_loss: 1392.7994\n",
            "Epoch 3488/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3415 - val_loss: 1392.8843\n",
            "Epoch 3489/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3785 - val_loss: 1392.7682\n",
            "Epoch 3490/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3076 - val_loss: 1392.9623\n",
            "Epoch 3491/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3752 - val_loss: 1393.3043\n",
            "Epoch 3492/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3601 - val_loss: 1393.4454\n",
            "Epoch 3493/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.4529 - val_loss: 1393.7368\n",
            "Epoch 3494/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.3042 - val_loss: 1393.5773\n",
            "Epoch 3495/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3451 - val_loss: 1393.3489\n",
            "Epoch 3496/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3474 - val_loss: 1393.1506\n",
            "Epoch 3497/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.2946 - val_loss: 1393.2313\n",
            "Epoch 3498/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.4851 - val_loss: 1393.5878\n",
            "Epoch 3499/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3310 - val_loss: 1393.2328\n",
            "Epoch 3500/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.3394 - val_loss: 1393.3317\n",
            "Epoch 3501/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3068 - val_loss: 1393.0719\n",
            "Epoch 3502/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.3222 - val_loss: 1392.9088\n",
            "Epoch 3503/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2889 - val_loss: 1392.9462\n",
            "Epoch 3504/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.3683 - val_loss: 1393.0603\n",
            "Epoch 3505/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2827 - val_loss: 1392.9999\n",
            "Epoch 3506/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3204 - val_loss: 1392.7627\n",
            "Epoch 3507/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3262 - val_loss: 1392.7155\n",
            "Epoch 3508/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2999 - val_loss: 1393.0081\n",
            "Epoch 3509/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2817 - val_loss: 1393.1693\n",
            "Epoch 3510/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.2930 - val_loss: 1393.2251\n",
            "Epoch 3511/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3226 - val_loss: 1393.1116\n",
            "Epoch 3512/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2696 - val_loss: 1393.1558\n",
            "Epoch 3513/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2936 - val_loss: 1393.2059\n",
            "Epoch 3514/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2532 - val_loss: 1393.2042\n",
            "Epoch 3515/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3021 - val_loss: 1393.1029\n",
            "Epoch 3516/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3392 - val_loss: 1393.4932\n",
            "Epoch 3517/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2611 - val_loss: 1393.5885\n",
            "Epoch 3518/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3101 - val_loss: 1393.5925\n",
            "Epoch 3519/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3160 - val_loss: 1393.5056\n",
            "Epoch 3520/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2405 - val_loss: 1393.6516\n",
            "Epoch 3521/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3889 - val_loss: 1394.0803\n",
            "Epoch 3522/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.2895 - val_loss: 1393.8265\n",
            "Epoch 3523/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2371 - val_loss: 1393.7871\n",
            "Epoch 3524/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2470 - val_loss: 1393.7681\n",
            "Epoch 3525/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2838 - val_loss: 1393.7520\n",
            "Epoch 3526/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2292 - val_loss: 1393.7709\n",
            "Epoch 3527/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.2377 - val_loss: 1393.8966\n",
            "Epoch 3528/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3241 - val_loss: 1393.7957\n",
            "Epoch 3529/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2104 - val_loss: 1394.0674\n",
            "Epoch 3530/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2154 - val_loss: 1394.2346\n",
            "Epoch 3531/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3213 - val_loss: 1394.6815\n",
            "Epoch 3532/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.3071 - val_loss: 1394.4496\n",
            "Epoch 3533/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.2333 - val_loss: 1394.5535\n",
            "Epoch 3534/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.3002 - val_loss: 1394.3076\n",
            "Epoch 3535/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2123 - val_loss: 1394.3690\n",
            "Epoch 3536/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2662 - val_loss: 1394.5580\n",
            "Epoch 3537/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.2182 - val_loss: 1394.4303\n",
            "Epoch 3538/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2087 - val_loss: 1394.2880\n",
            "Epoch 3539/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2330 - val_loss: 1394.2150\n",
            "Epoch 3540/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.2962 - val_loss: 1393.8799\n",
            "Epoch 3541/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.2572 - val_loss: 1393.8323\n",
            "Epoch 3542/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.3174 - val_loss: 1394.1788\n",
            "Epoch 3543/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.3375 - val_loss: 1394.3788\n",
            "Epoch 3544/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2169 - val_loss: 1393.9211\n",
            "Epoch 3545/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2635 - val_loss: 1393.4990\n",
            "Epoch 3546/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.1757 - val_loss: 1393.4475\n",
            "Epoch 3547/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.3179 - val_loss: 1393.6901\n",
            "Epoch 3548/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.2439 - val_loss: 1393.2666\n",
            "Epoch 3549/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2504 - val_loss: 1392.9869\n",
            "Epoch 3550/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2033 - val_loss: 1393.0739\n",
            "Epoch 3551/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.2107 - val_loss: 1392.8188\n",
            "Epoch 3552/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.2228 - val_loss: 1392.7074\n",
            "Epoch 3553/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1930 - val_loss: 1392.7312\n",
            "Epoch 3554/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1988 - val_loss: 1392.6055\n",
            "Epoch 3555/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1811 - val_loss: 1392.6108\n",
            "Epoch 3556/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2803 - val_loss: 1393.0067\n",
            "Epoch 3557/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1779 - val_loss: 1393.0110\n",
            "Epoch 3558/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1815 - val_loss: 1392.8412\n",
            "Epoch 3559/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.1642 - val_loss: 1392.9028\n",
            "Epoch 3560/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1518 - val_loss: 1392.9233\n",
            "Epoch 3561/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1493 - val_loss: 1392.9397\n",
            "Epoch 3562/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1839 - val_loss: 1393.1216\n",
            "Epoch 3563/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1518 - val_loss: 1393.1072\n",
            "Epoch 3564/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1554 - val_loss: 1393.2094\n",
            "Epoch 3565/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.2604 - val_loss: 1393.4680\n",
            "Epoch 3566/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1498 - val_loss: 1393.2386\n",
            "Epoch 3567/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1641 - val_loss: 1392.9884\n",
            "Epoch 3568/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1801 - val_loss: 1392.8292\n",
            "Epoch 3569/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1398 - val_loss: 1392.9048\n",
            "Epoch 3570/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1536 - val_loss: 1393.1228\n",
            "Epoch 3571/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.1278 - val_loss: 1393.1656\n",
            "Epoch 3572/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.2049 - val_loss: 1393.3197\n",
            "Epoch 3573/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.2003 - val_loss: 1393.1969\n",
            "Epoch 3574/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.2169 - val_loss: 1393.6454\n",
            "Epoch 3575/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.1300 - val_loss: 1393.6200\n",
            "Epoch 3576/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1504 - val_loss: 1393.6344\n",
            "Epoch 3577/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1487 - val_loss: 1393.6802\n",
            "Epoch 3578/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.1125 - val_loss: 1393.6877\n",
            "Epoch 3579/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1410 - val_loss: 1393.8110\n",
            "Epoch 3580/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1191 - val_loss: 1393.6283\n",
            "Epoch 3581/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.2171 - val_loss: 1393.3776\n",
            "Epoch 3582/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.1743 - val_loss: 1393.7266\n",
            "Epoch 3583/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.1398 - val_loss: 1393.8578\n",
            "Epoch 3584/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.0974 - val_loss: 1393.7731\n",
            "Epoch 3585/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.2118 - val_loss: 1393.9093\n",
            "Epoch 3586/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1920 - val_loss: 1393.4415\n",
            "Epoch 3587/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1229 - val_loss: 1393.3635\n",
            "Epoch 3588/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1277 - val_loss: 1393.2620\n",
            "Epoch 3589/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.2811 - val_loss: 1393.6809\n",
            "Epoch 3590/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.0913 - val_loss: 1393.6196\n",
            "Epoch 3591/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0997 - val_loss: 1393.7040\n",
            "Epoch 3592/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0867 - val_loss: 1393.8413\n",
            "Epoch 3593/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0886 - val_loss: 1393.8563\n",
            "Epoch 3594/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0818 - val_loss: 1393.9650\n",
            "Epoch 3595/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0861 - val_loss: 1393.9387\n",
            "Epoch 3596/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.1900 - val_loss: 1394.2645\n",
            "Epoch 3597/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1270 - val_loss: 1393.9659\n",
            "Epoch 3598/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1117 - val_loss: 1393.9031\n",
            "Epoch 3599/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.0673 - val_loss: 1393.9786\n",
            "Epoch 3600/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1213 - val_loss: 1394.0889\n",
            "Epoch 3601/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0877 - val_loss: 1393.9741\n",
            "Epoch 3602/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1688 - val_loss: 1393.7887\n",
            "Epoch 3603/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0767 - val_loss: 1393.6814\n",
            "Epoch 3604/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1350 - val_loss: 1393.8999\n",
            "Epoch 3605/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.1044 - val_loss: 1393.7733\n",
            "Epoch 3606/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0816 - val_loss: 1393.4380\n",
            "Epoch 3607/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0565 - val_loss: 1393.4573\n",
            "Epoch 3608/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0453 - val_loss: 1393.3698\n",
            "Epoch 3609/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1368 - val_loss: 1393.3497\n",
            "Epoch 3610/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0541 - val_loss: 1393.4462\n",
            "Epoch 3611/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0447 - val_loss: 1393.4623\n",
            "Epoch 3612/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.1513 - val_loss: 1393.7863\n",
            "Epoch 3613/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.1503 - val_loss: 1393.4596\n",
            "Epoch 3614/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0331 - val_loss: 1393.5266\n",
            "Epoch 3615/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0604 - val_loss: 1393.7401\n",
            "Epoch 3616/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0339 - val_loss: 1393.7609\n",
            "Epoch 3617/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.1007 - val_loss: 1393.9996\n",
            "Epoch 3618/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0231 - val_loss: 1393.9399\n",
            "Epoch 3619/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0486 - val_loss: 1393.8416\n",
            "Epoch 3620/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0283 - val_loss: 1393.5802\n",
            "Epoch 3621/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0323 - val_loss: 1393.2855\n",
            "Epoch 3622/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0465 - val_loss: 1393.2651\n",
            "Epoch 3623/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0074 - val_loss: 1393.0641\n",
            "Epoch 3624/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1071.1645 - val_loss: 1392.6832\n",
            "Epoch 3625/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.0291 - val_loss: 1392.6238\n",
            "Epoch 3626/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1013 - val_loss: 1392.7678\n",
            "Epoch 3627/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0215 - val_loss: 1392.5780\n",
            "Epoch 3628/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1071.0585 - val_loss: 1392.3845\n",
            "Epoch 3629/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.0407 - val_loss: 1392.3951\n",
            "Epoch 3630/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1071.0302 - val_loss: 1392.4913\n",
            "Epoch 3631/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.1729 - val_loss: 1392.9307\n",
            "Epoch 3632/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0156 - val_loss: 1392.8959\n",
            "Epoch 3633/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.0179 - val_loss: 1392.8457\n",
            "Epoch 3634/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0549 - val_loss: 1392.7939\n",
            "Epoch 3635/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0170 - val_loss: 1393.0851\n",
            "Epoch 3636/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0363 - val_loss: 1393.3389\n",
            "Epoch 3637/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0243 - val_loss: 1393.3534\n",
            "Epoch 3638/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0277 - val_loss: 1393.2448\n",
            "Epoch 3639/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0738 - val_loss: 1393.5330\n",
            "Epoch 3640/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.9932 - val_loss: 1393.6780\n",
            "Epoch 3641/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9933 - val_loss: 1393.7655\n",
            "Epoch 3642/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.9910 - val_loss: 1393.8910\n",
            "Epoch 3643/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.0066 - val_loss: 1393.9984\n",
            "Epoch 3644/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0659 - val_loss: 1393.7017\n",
            "Epoch 3645/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1071.1282 - val_loss: 1394.0632\n",
            "Epoch 3646/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.1018 - val_loss: 1394.2223\n",
            "Epoch 3647/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9885 - val_loss: 1393.9166\n",
            "Epoch 3648/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1071.0286 - val_loss: 1393.5737\n",
            "Epoch 3649/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1071.2236 - val_loss: 1393.2101\n",
            "Epoch 3650/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.1465 - val_loss: 1393.7015\n",
            "Epoch 3651/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9795 - val_loss: 1393.7262\n",
            "Epoch 3652/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1071.0144 - val_loss: 1393.8611\n",
            "Epoch 3653/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9629 - val_loss: 1393.8391\n",
            "Epoch 3654/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0424 - val_loss: 1394.0466\n",
            "Epoch 3655/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0082 - val_loss: 1393.7974\n",
            "Epoch 3656/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9950 - val_loss: 1393.9548\n",
            "Epoch 3657/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9736 - val_loss: 1393.9084\n",
            "Epoch 3658/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9563 - val_loss: 1394.0148\n",
            "Epoch 3659/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.9550 - val_loss: 1394.1794\n",
            "Epoch 3660/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9637 - val_loss: 1394.3527\n",
            "Epoch 3661/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9625 - val_loss: 1394.4244\n",
            "Epoch 3662/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.9727 - val_loss: 1394.5781\n",
            "Epoch 3663/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.9983 - val_loss: 1394.3605\n",
            "Epoch 3664/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0134 - val_loss: 1394.2617\n",
            "Epoch 3665/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.9568 - val_loss: 1394.5430\n",
            "Epoch 3666/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.9482 - val_loss: 1394.5433\n",
            "Epoch 3667/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9507 - val_loss: 1394.7444\n",
            "Epoch 3668/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9386 - val_loss: 1394.7690\n",
            "Epoch 3669/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9518 - val_loss: 1394.6945\n",
            "Epoch 3670/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9925 - val_loss: 1394.8335\n",
            "Epoch 3671/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9445 - val_loss: 1394.7659\n",
            "Epoch 3672/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9415 - val_loss: 1394.4147\n",
            "Epoch 3673/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9144 - val_loss: 1394.2454\n",
            "Epoch 3674/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9413 - val_loss: 1393.9006\n",
            "Epoch 3675/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9732 - val_loss: 1393.9249\n",
            "Epoch 3676/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.9503 - val_loss: 1393.6666\n",
            "Epoch 3677/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9130 - val_loss: 1393.3175\n",
            "Epoch 3678/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0076 - val_loss: 1392.9801\n",
            "Epoch 3679/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.9632 - val_loss: 1392.7648\n",
            "Epoch 3680/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9416 - val_loss: 1392.8608\n",
            "Epoch 3681/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9341 - val_loss: 1392.9656\n",
            "Epoch 3682/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.9383 - val_loss: 1393.0446\n",
            "Epoch 3683/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0080 - val_loss: 1393.2865\n",
            "Epoch 3684/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9368 - val_loss: 1393.2998\n",
            "Epoch 3685/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9085 - val_loss: 1393.3320\n",
            "Epoch 3686/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.0057 - val_loss: 1393.6805\n",
            "Epoch 3687/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.9197 - val_loss: 1393.6888\n",
            "Epoch 3688/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.9019 - val_loss: 1393.6116\n",
            "Epoch 3689/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.8973 - val_loss: 1393.6777\n",
            "Epoch 3690/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.8971 - val_loss: 1393.7275\n",
            "Epoch 3691/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8959 - val_loss: 1393.7667\n",
            "Epoch 3692/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9354 - val_loss: 1393.7976\n",
            "Epoch 3693/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.9290 - val_loss: 1394.0005\n",
            "Epoch 3694/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0386 - val_loss: 1394.2218\n",
            "Epoch 3695/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9474 - val_loss: 1393.8671\n",
            "Epoch 3696/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.0091 - val_loss: 1393.5925\n",
            "Epoch 3697/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.8891 - val_loss: 1393.7578\n",
            "Epoch 3698/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8776 - val_loss: 1393.9016\n",
            "Epoch 3699/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.8919 - val_loss: 1394.0175\n",
            "Epoch 3700/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8802 - val_loss: 1394.1880\n",
            "Epoch 3701/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8764 - val_loss: 1394.6500\n",
            "Epoch 3702/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.9610 - val_loss: 1395.0878\n",
            "Epoch 3703/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8793 - val_loss: 1395.0579\n",
            "Epoch 3704/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9009 - val_loss: 1395.0092\n",
            "Epoch 3705/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.9527 - val_loss: 1394.8120\n",
            "Epoch 3706/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8721 - val_loss: 1394.8867\n",
            "Epoch 3707/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9270 - val_loss: 1394.8351\n",
            "Epoch 3708/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9193 - val_loss: 1394.9663\n",
            "Epoch 3709/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9262 - val_loss: 1394.9255\n",
            "Epoch 3710/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8659 - val_loss: 1395.1750\n",
            "Epoch 3711/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.8614 - val_loss: 1395.2223\n",
            "Epoch 3712/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8675 - val_loss: 1395.2716\n",
            "Epoch 3713/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.8713 - val_loss: 1395.4020\n",
            "Epoch 3714/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9281 - val_loss: 1395.3005\n",
            "Epoch 3715/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9036 - val_loss: 1395.5397\n",
            "Epoch 3716/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.8609 - val_loss: 1395.4639\n",
            "Epoch 3717/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.9024 - val_loss: 1395.5396\n",
            "Epoch 3718/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8973 - val_loss: 1395.1559\n",
            "Epoch 3719/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8419 - val_loss: 1395.0360\n",
            "Epoch 3720/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8814 - val_loss: 1394.8226\n",
            "Epoch 3721/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8321 - val_loss: 1394.7104\n",
            "Epoch 3722/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9307 - val_loss: 1394.3748\n",
            "Epoch 3723/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8599 - val_loss: 1394.1396\n",
            "Epoch 3724/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0168 - val_loss: 1394.4281\n",
            "Epoch 3725/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8355 - val_loss: 1394.3540\n",
            "Epoch 3726/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.8571 - val_loss: 1394.0837\n",
            "Epoch 3727/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.8875 - val_loss: 1394.1619\n",
            "Epoch 3728/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8183 - val_loss: 1393.8871\n",
            "Epoch 3729/5000\n",
            "353/353 [==============================] - 0s 24us/sample - loss: 1070.8150 - val_loss: 1393.7239\n",
            "Epoch 3730/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8444 - val_loss: 1393.4413\n",
            "Epoch 3731/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8766 - val_loss: 1393.2616\n",
            "Epoch 3732/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1071.0036 - val_loss: 1393.6375\n",
            "Epoch 3733/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8200 - val_loss: 1393.6921\n",
            "Epoch 3734/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8188 - val_loss: 1393.6702\n",
            "Epoch 3735/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8338 - val_loss: 1393.7074\n",
            "Epoch 3736/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8113 - val_loss: 1393.6898\n",
            "Epoch 3737/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8223 - val_loss: 1393.6442\n",
            "Epoch 3738/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8439 - val_loss: 1393.5990\n",
            "Epoch 3739/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1071.0743 - val_loss: 1394.1417\n",
            "Epoch 3740/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8029 - val_loss: 1394.1283\n",
            "Epoch 3741/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.7987 - val_loss: 1394.0015\n",
            "Epoch 3742/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8603 - val_loss: 1393.7867\n",
            "Epoch 3743/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.8133 - val_loss: 1393.8622\n",
            "Epoch 3744/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.8009 - val_loss: 1393.9337\n",
            "Epoch 3745/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9124 - val_loss: 1393.8707\n",
            "Epoch 3746/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.9303 - val_loss: 1394.4230\n",
            "Epoch 3747/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7829 - val_loss: 1394.5900\n",
            "Epoch 3748/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8122 - val_loss: 1394.6395\n",
            "Epoch 3749/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8769 - val_loss: 1395.0579\n",
            "Epoch 3750/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.8273 - val_loss: 1395.1947\n",
            "Epoch 3751/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8126 - val_loss: 1395.0687\n",
            "Epoch 3752/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.9458 - val_loss: 1395.2134\n",
            "Epoch 3753/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8625 - val_loss: 1395.1329\n",
            "Epoch 3754/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7875 - val_loss: 1395.4003\n",
            "Epoch 3755/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.7891 - val_loss: 1395.5402\n",
            "Epoch 3756/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8390 - val_loss: 1395.7627\n",
            "Epoch 3757/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8148 - val_loss: 1395.7529\n",
            "Epoch 3758/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8434 - val_loss: 1395.7178\n",
            "Epoch 3759/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1071.0148 - val_loss: 1395.0613\n",
            "Epoch 3760/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7893 - val_loss: 1395.1278\n",
            "Epoch 3761/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7912 - val_loss: 1395.1248\n",
            "Epoch 3762/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.9031 - val_loss: 1394.9753\n",
            "Epoch 3763/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7938 - val_loss: 1395.0001\n",
            "Epoch 3764/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.7530 - val_loss: 1394.7825\n",
            "Epoch 3765/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7975 - val_loss: 1394.7614\n",
            "Epoch 3766/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7363 - val_loss: 1394.4703\n",
            "Epoch 3767/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.7908 - val_loss: 1394.1602\n",
            "Epoch 3768/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.7847 - val_loss: 1393.9016\n",
            "Epoch 3769/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7617 - val_loss: 1393.8143\n",
            "Epoch 3770/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7888 - val_loss: 1393.7916\n",
            "Epoch 3771/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7826 - val_loss: 1393.9742\n",
            "Epoch 3772/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1071.0289 - val_loss: 1394.3380\n",
            "Epoch 3773/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.8093 - val_loss: 1394.0129\n",
            "Epoch 3774/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8221 - val_loss: 1393.6129\n",
            "Epoch 3775/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.7876 - val_loss: 1393.3871\n",
            "Epoch 3776/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8295 - val_loss: 1393.2614\n",
            "Epoch 3777/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.8329 - val_loss: 1393.2130\n",
            "Epoch 3778/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.8181 - val_loss: 1393.6494\n",
            "Epoch 3779/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7486 - val_loss: 1393.8010\n",
            "Epoch 3780/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.8428 - val_loss: 1394.2371\n",
            "Epoch 3781/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7304 - val_loss: 1394.3079\n",
            "Epoch 3782/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.7789 - val_loss: 1394.3319\n",
            "Epoch 3783/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.7852 - val_loss: 1394.3688\n",
            "Epoch 3784/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.8009 - val_loss: 1394.8942\n",
            "Epoch 3785/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.8042 - val_loss: 1395.2085\n",
            "Epoch 3786/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.8480 - val_loss: 1394.9146\n",
            "Epoch 3787/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.7206 - val_loss: 1394.9518\n",
            "Epoch 3788/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7157 - val_loss: 1395.0511\n",
            "Epoch 3789/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.7302 - val_loss: 1395.0508\n",
            "Epoch 3790/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8269 - val_loss: 1395.4019\n",
            "Epoch 3791/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7295 - val_loss: 1395.3278\n",
            "Epoch 3792/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.7441 - val_loss: 1395.3876\n",
            "Epoch 3793/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7660 - val_loss: 1395.2968\n",
            "Epoch 3794/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7151 - val_loss: 1395.2660\n",
            "Epoch 3795/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7195 - val_loss: 1395.2568\n",
            "Epoch 3796/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7558 - val_loss: 1394.9862\n",
            "Epoch 3797/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1070.7092 - val_loss: 1394.8770\n",
            "Epoch 3798/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7472 - val_loss: 1394.6910\n",
            "Epoch 3799/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7408 - val_loss: 1394.7802\n",
            "Epoch 3800/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7503 - val_loss: 1394.5197\n",
            "Epoch 3801/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.7220 - val_loss: 1394.5581\n",
            "Epoch 3802/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7279 - val_loss: 1394.3610\n",
            "Epoch 3803/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7234 - val_loss: 1394.4506\n",
            "Epoch 3804/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6986 - val_loss: 1394.4692\n",
            "Epoch 3805/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.7016 - val_loss: 1394.4845\n",
            "Epoch 3806/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6906 - val_loss: 1394.4149\n",
            "Epoch 3807/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6978 - val_loss: 1394.4299\n",
            "Epoch 3808/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1070.7237 - val_loss: 1394.4934\n",
            "Epoch 3809/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.8565 - val_loss: 1394.1971\n",
            "Epoch 3810/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.7268 - val_loss: 1394.4985\n",
            "Epoch 3811/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6923 - val_loss: 1394.6118\n",
            "Epoch 3812/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7110 - val_loss: 1394.7878\n",
            "Epoch 3813/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7134 - val_loss: 1394.9254\n",
            "Epoch 3814/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6976 - val_loss: 1394.8213\n",
            "Epoch 3815/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7098 - val_loss: 1394.8280\n",
            "Epoch 3816/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7187 - val_loss: 1394.6078\n",
            "Epoch 3817/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.6930 - val_loss: 1394.6079\n",
            "Epoch 3818/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6933 - val_loss: 1394.6627\n",
            "Epoch 3819/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6900 - val_loss: 1394.5786\n",
            "Epoch 3820/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6884 - val_loss: 1394.4603\n",
            "Epoch 3821/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1070.6720 - val_loss: 1394.4766\n",
            "Epoch 3822/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6691 - val_loss: 1394.5687\n",
            "Epoch 3823/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6709 - val_loss: 1394.6235\n",
            "Epoch 3824/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6872 - val_loss: 1394.6422\n",
            "Epoch 3825/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6905 - val_loss: 1394.6946\n",
            "Epoch 3826/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6984 - val_loss: 1394.8516\n",
            "Epoch 3827/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.7051 - val_loss: 1394.8650\n",
            "Epoch 3828/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6572 - val_loss: 1394.7546\n",
            "Epoch 3829/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6540 - val_loss: 1394.6453\n",
            "Epoch 3830/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6972 - val_loss: 1394.3953\n",
            "Epoch 3831/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.6666 - val_loss: 1394.4199\n",
            "Epoch 3832/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6566 - val_loss: 1394.4519\n",
            "Epoch 3833/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6902 - val_loss: 1394.6735\n",
            "Epoch 3834/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1070.6583 - val_loss: 1394.6677\n",
            "Epoch 3835/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6895 - val_loss: 1394.5951\n",
            "Epoch 3836/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6769 - val_loss: 1394.4900\n",
            "Epoch 3837/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6561 - val_loss: 1394.5929\n",
            "Epoch 3838/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.7625 - val_loss: 1394.5012\n",
            "Epoch 3839/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6397 - val_loss: 1394.8141\n",
            "Epoch 3840/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7162 - val_loss: 1395.1223\n",
            "Epoch 3841/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6565 - val_loss: 1395.2317\n",
            "Epoch 3842/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6775 - val_loss: 1395.2606\n",
            "Epoch 3843/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.7664 - val_loss: 1395.0181\n",
            "Epoch 3844/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6621 - val_loss: 1395.2212\n",
            "Epoch 3845/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.7074 - val_loss: 1395.0914\n",
            "Epoch 3846/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6871 - val_loss: 1395.4023\n",
            "Epoch 3847/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6517 - val_loss: 1395.4440\n",
            "Epoch 3848/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6327 - val_loss: 1395.4015\n",
            "Epoch 3849/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6471 - val_loss: 1395.2356\n",
            "Epoch 3850/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.6664 - val_loss: 1395.3229\n",
            "Epoch 3851/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1070.6246 - val_loss: 1395.1417\n",
            "Epoch 3852/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.6236 - val_loss: 1395.0338\n",
            "Epoch 3853/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6581 - val_loss: 1394.9904\n",
            "Epoch 3854/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6377 - val_loss: 1394.7578\n",
            "Epoch 3855/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6557 - val_loss: 1394.8047\n",
            "Epoch 3856/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6614 - val_loss: 1394.9021\n",
            "Epoch 3857/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6281 - val_loss: 1394.8190\n",
            "Epoch 3858/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6135 - val_loss: 1394.6382\n",
            "Epoch 3859/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6246 - val_loss: 1394.5521\n",
            "Epoch 3860/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6331 - val_loss: 1394.3075\n",
            "Epoch 3861/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6216 - val_loss: 1394.2495\n",
            "Epoch 3862/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6113 - val_loss: 1394.1196\n",
            "Epoch 3863/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6310 - val_loss: 1394.1023\n",
            "Epoch 3864/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6979 - val_loss: 1393.7783\n",
            "Epoch 3865/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6315 - val_loss: 1393.8774\n",
            "Epoch 3866/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.7207 - val_loss: 1394.1301\n",
            "Epoch 3867/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.7289 - val_loss: 1393.8154\n",
            "Epoch 3868/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.6741 - val_loss: 1393.8256\n",
            "Epoch 3869/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.6719 - val_loss: 1393.8092\n",
            "Epoch 3870/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.9820 - val_loss: 1394.4713\n",
            "Epoch 3871/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6128 - val_loss: 1394.5377\n",
            "Epoch 3872/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6612 - val_loss: 1394.3680\n",
            "Epoch 3873/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6348 - val_loss: 1394.4867\n",
            "Epoch 3874/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.6307 - val_loss: 1394.5786\n",
            "Epoch 3875/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6263 - val_loss: 1394.8617\n",
            "Epoch 3876/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6899 - val_loss: 1394.9070\n",
            "Epoch 3877/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5885 - val_loss: 1395.1079\n",
            "Epoch 3878/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5897 - val_loss: 1395.1953\n",
            "Epoch 3879/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5826 - val_loss: 1395.2937\n",
            "Epoch 3880/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7961 - val_loss: 1395.6736\n",
            "Epoch 3881/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6114 - val_loss: 1395.3698\n",
            "Epoch 3882/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6602 - val_loss: 1395.0830\n",
            "Epoch 3883/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5937 - val_loss: 1395.0295\n",
            "Epoch 3884/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5770 - val_loss: 1395.1448\n",
            "Epoch 3885/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5858 - val_loss: 1395.1398\n",
            "Epoch 3886/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5804 - val_loss: 1395.2692\n",
            "Epoch 3887/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.5766 - val_loss: 1395.3069\n",
            "Epoch 3888/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.7086 - val_loss: 1395.6053\n",
            "Epoch 3889/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5784 - val_loss: 1395.5247\n",
            "Epoch 3890/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5648 - val_loss: 1395.3806\n",
            "Epoch 3891/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5704 - val_loss: 1395.1157\n",
            "Epoch 3892/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5774 - val_loss: 1394.9901\n",
            "Epoch 3893/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5663 - val_loss: 1394.7861\n",
            "Epoch 3894/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.5675 - val_loss: 1394.7053\n",
            "Epoch 3895/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.6646 - val_loss: 1394.3147\n",
            "Epoch 3896/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.6266 - val_loss: 1394.1455\n",
            "Epoch 3897/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.6215 - val_loss: 1394.1707\n",
            "Epoch 3898/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5935 - val_loss: 1394.2175\n",
            "Epoch 3899/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6442 - val_loss: 1394.0626\n",
            "Epoch 3900/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5737 - val_loss: 1394.1285\n",
            "Epoch 3901/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5876 - val_loss: 1394.2604\n",
            "Epoch 3902/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5660 - val_loss: 1394.4546\n",
            "Epoch 3903/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6091 - val_loss: 1394.7047\n",
            "Epoch 3904/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5572 - val_loss: 1394.7230\n",
            "Epoch 3905/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.6516 - val_loss: 1394.9462\n",
            "Epoch 3906/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5660 - val_loss: 1394.9266\n",
            "Epoch 3907/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6265 - val_loss: 1394.5847\n",
            "Epoch 3908/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5518 - val_loss: 1394.4669\n",
            "Epoch 3909/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5540 - val_loss: 1394.4031\n",
            "Epoch 3910/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.6023 - val_loss: 1394.2937\n",
            "Epoch 3911/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.5757 - val_loss: 1394.5792\n",
            "Epoch 3912/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5418 - val_loss: 1394.6727\n",
            "Epoch 3913/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5533 - val_loss: 1394.7505\n",
            "Epoch 3914/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6443 - val_loss: 1394.6962\n",
            "Epoch 3915/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.5509 - val_loss: 1394.8280\n",
            "Epoch 3916/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5664 - val_loss: 1394.9407\n",
            "Epoch 3917/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5533 - val_loss: 1394.9860\n",
            "Epoch 3918/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.5547 - val_loss: 1394.8418\n",
            "Epoch 3919/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.5681 - val_loss: 1394.7811\n",
            "Epoch 3920/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.5647 - val_loss: 1394.6281\n",
            "Epoch 3921/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5927 - val_loss: 1394.5376\n",
            "Epoch 3922/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5429 - val_loss: 1394.7245\n",
            "Epoch 3923/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5511 - val_loss: 1395.0845\n",
            "Epoch 3924/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5435 - val_loss: 1395.3168\n",
            "Epoch 3925/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5220 - val_loss: 1395.3795\n",
            "Epoch 3926/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5438 - val_loss: 1395.4967\n",
            "Epoch 3927/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5546 - val_loss: 1395.5347\n",
            "Epoch 3928/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5423 - val_loss: 1395.5275\n",
            "Epoch 3929/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5232 - val_loss: 1395.3737\n",
            "Epoch 3930/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5324 - val_loss: 1395.2217\n",
            "Epoch 3931/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5560 - val_loss: 1395.2983\n",
            "Epoch 3932/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5138 - val_loss: 1395.3087\n",
            "Epoch 3933/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5516 - val_loss: 1395.3961\n",
            "Epoch 3934/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5222 - val_loss: 1395.4904\n",
            "Epoch 3935/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5108 - val_loss: 1395.4996\n",
            "Epoch 3936/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5224 - val_loss: 1395.5280\n",
            "Epoch 3937/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5107 - val_loss: 1395.4427\n",
            "Epoch 3938/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5192 - val_loss: 1395.2922\n",
            "Epoch 3939/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5108 - val_loss: 1395.3555\n",
            "Epoch 3940/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5498 - val_loss: 1395.2946\n",
            "Epoch 3941/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.5056 - val_loss: 1395.2458\n",
            "Epoch 3942/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5085 - val_loss: 1395.0411\n",
            "Epoch 3943/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.5102 - val_loss: 1394.9407\n",
            "Epoch 3944/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.5277 - val_loss: 1394.8734\n",
            "Epoch 3945/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4964 - val_loss: 1394.7256\n",
            "Epoch 3946/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5287 - val_loss: 1394.5652\n",
            "Epoch 3947/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4968 - val_loss: 1394.3961\n",
            "Epoch 3948/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5483 - val_loss: 1394.4321\n",
            "Epoch 3949/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.5188 - val_loss: 1394.1875\n",
            "Epoch 3950/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5146 - val_loss: 1393.9932\n",
            "Epoch 3951/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5178 - val_loss: 1393.9243\n",
            "Epoch 3952/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5182 - val_loss: 1393.8679\n",
            "Epoch 3953/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1070.5821 - val_loss: 1394.0769\n",
            "Epoch 3954/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5197 - val_loss: 1394.0430\n",
            "Epoch 3955/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5513 - val_loss: 1393.9369\n",
            "Epoch 3956/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.5119 - val_loss: 1394.1053\n",
            "Epoch 3957/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5033 - val_loss: 1394.2917\n",
            "Epoch 3958/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4952 - val_loss: 1394.3976\n",
            "Epoch 3959/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5101 - val_loss: 1394.6697\n",
            "Epoch 3960/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5072 - val_loss: 1394.8171\n",
            "Epoch 3961/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5901 - val_loss: 1394.6304\n",
            "Epoch 3962/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4961 - val_loss: 1394.8719\n",
            "Epoch 3963/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4745 - val_loss: 1394.9611\n",
            "Epoch 3964/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4899 - val_loss: 1395.1813\n",
            "Epoch 3965/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.6227 - val_loss: 1395.5117\n",
            "Epoch 3966/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4671 - val_loss: 1395.4088\n",
            "Epoch 3967/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.5405 - val_loss: 1395.4886\n",
            "Epoch 3968/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4620 - val_loss: 1395.2396\n",
            "Epoch 3969/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4969 - val_loss: 1395.0808\n",
            "Epoch 3970/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4767 - val_loss: 1394.8173\n",
            "Epoch 3971/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4923 - val_loss: 1394.6296\n",
            "Epoch 3972/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4734 - val_loss: 1394.6483\n",
            "Epoch 3973/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4876 - val_loss: 1394.6414\n",
            "Epoch 3974/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4876 - val_loss: 1394.8419\n",
            "Epoch 3975/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4855 - val_loss: 1394.7886\n",
            "Epoch 3976/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4595 - val_loss: 1394.9093\n",
            "Epoch 3977/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4716 - val_loss: 1395.1434\n",
            "Epoch 3978/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.4760 - val_loss: 1395.3029\n",
            "Epoch 3979/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4868 - val_loss: 1395.4403\n",
            "Epoch 3980/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4587 - val_loss: 1395.3035\n",
            "Epoch 3981/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.5381 - val_loss: 1395.0524\n",
            "Epoch 3982/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4560 - val_loss: 1395.1038\n",
            "Epoch 3983/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5395 - val_loss: 1395.3813\n",
            "Epoch 3984/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4733 - val_loss: 1395.2390\n",
            "Epoch 3985/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4868 - val_loss: 1395.4120\n",
            "Epoch 3986/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4729 - val_loss: 1395.3313\n",
            "Epoch 3987/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4857 - val_loss: 1395.1211\n",
            "Epoch 3988/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4464 - val_loss: 1395.1102\n",
            "Epoch 3989/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4481 - val_loss: 1395.1403\n",
            "Epoch 3990/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4630 - val_loss: 1395.0096\n",
            "Epoch 3991/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4743 - val_loss: 1394.9418\n",
            "Epoch 3992/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4531 - val_loss: 1395.1847\n",
            "Epoch 3993/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4460 - val_loss: 1395.2146\n",
            "Epoch 3994/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5268 - val_loss: 1395.5479\n",
            "Epoch 3995/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.5522 - val_loss: 1395.2783\n",
            "Epoch 3996/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1070.4646 - val_loss: 1395.5150\n",
            "Epoch 3997/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 559.786 - 0s 30us/sample - loss: 1070.5232 - val_loss: 1395.6251\n",
            "Epoch 3998/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4608 - val_loss: 1395.3640\n",
            "Epoch 3999/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4461 - val_loss: 1395.3727\n",
            "Epoch 4000/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.4537 - val_loss: 1395.3147\n",
            "Epoch 4001/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.5098 - val_loss: 1394.9974\n",
            "Epoch 4002/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1070.4502 - val_loss: 1394.9386\n",
            "Epoch 4003/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.4307 - val_loss: 1395.1310\n",
            "Epoch 4004/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.4387 - val_loss: 1395.2390\n",
            "Epoch 4005/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5087 - val_loss: 1395.0801\n",
            "Epoch 4006/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.5541 - val_loss: 1395.5455\n",
            "Epoch 4007/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4237 - val_loss: 1395.6554\n",
            "Epoch 4008/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4240 - val_loss: 1395.6288\n",
            "Epoch 4009/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4129 - val_loss: 1395.5917\n",
            "Epoch 4010/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4146 - val_loss: 1395.5779\n",
            "Epoch 4011/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4581 - val_loss: 1395.6655\n",
            "Epoch 4012/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4733 - val_loss: 1395.3292\n",
            "Epoch 4013/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4378 - val_loss: 1395.4420\n",
            "Epoch 4014/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4171 - val_loss: 1395.3015\n",
            "Epoch 4015/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4240 - val_loss: 1395.3405\n",
            "Epoch 4016/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4131 - val_loss: 1395.1975\n",
            "Epoch 4017/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4314 - val_loss: 1395.0065\n",
            "Epoch 4018/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4298 - val_loss: 1395.0223\n",
            "Epoch 4019/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4707 - val_loss: 1395.2068\n",
            "Epoch 4020/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4479 - val_loss: 1395.4259\n",
            "Epoch 4021/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4008 - val_loss: 1395.3573\n",
            "Epoch 4022/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.5256 - val_loss: 1395.3613\n",
            "Epoch 4023/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.3944 - val_loss: 1395.2765\n",
            "Epoch 4024/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4004 - val_loss: 1395.1832\n",
            "Epoch 4025/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4319 - val_loss: 1394.9485\n",
            "Epoch 4026/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4125 - val_loss: 1394.9900\n",
            "Epoch 4027/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4312 - val_loss: 1394.8588\n",
            "Epoch 4028/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4091 - val_loss: 1394.8485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4029/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4049 - val_loss: 1395.0009\n",
            "Epoch 4030/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.4941 - val_loss: 1395.2402\n",
            "Epoch 4031/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4513 - val_loss: 1395.4744\n",
            "Epoch 4032/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4030 - val_loss: 1395.4436\n",
            "Epoch 4033/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.5752 - val_loss: 1395.0432\n",
            "Epoch 4034/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1042.79 - 0s 30us/sample - loss: 1070.4292 - val_loss: 1395.0328\n",
            "Epoch 4035/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3913 - val_loss: 1395.3610\n",
            "Epoch 4036/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.4072 - val_loss: 1395.6508\n",
            "Epoch 4037/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4349 - val_loss: 1395.9314\n",
            "Epoch 4038/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3861 - val_loss: 1395.9387\n",
            "Epoch 4039/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3827 - val_loss: 1395.9037\n",
            "Epoch 4040/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3810 - val_loss: 1395.9452\n",
            "Epoch 4041/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4860 - val_loss: 1395.6580\n",
            "Epoch 4042/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4391 - val_loss: 1395.5953\n",
            "Epoch 4043/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4136 - val_loss: 1396.0251\n",
            "Epoch 4044/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3777 - val_loss: 1396.2252\n",
            "Epoch 4045/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3885 - val_loss: 1396.2112\n",
            "Epoch 4046/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3920 - val_loss: 1396.3861\n",
            "Epoch 4047/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3944 - val_loss: 1396.2638\n",
            "Epoch 4048/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3885 - val_loss: 1396.3884\n",
            "Epoch 4049/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3748 - val_loss: 1396.3807\n",
            "Epoch 4050/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3993 - val_loss: 1396.4016\n",
            "Epoch 4051/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4253 - val_loss: 1396.0601\n",
            "Epoch 4052/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3507 - val_loss: 1395.8701\n",
            "Epoch 4053/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4192 - val_loss: 1395.5906\n",
            "Epoch 4054/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.3692 - val_loss: 1395.6707\n",
            "Epoch 4055/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3672 - val_loss: 1395.6150\n",
            "Epoch 4056/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3502 - val_loss: 1395.7023\n",
            "Epoch 4057/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3772 - val_loss: 1395.6582\n",
            "Epoch 4058/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1144.66 - 0s 31us/sample - loss: 1070.4837 - val_loss: 1396.0637\n",
            "Epoch 4059/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3648 - val_loss: 1396.0774\n",
            "Epoch 4060/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3713 - val_loss: 1395.9305\n",
            "Epoch 4061/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3488 - val_loss: 1395.9006\n",
            "Epoch 4062/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3694 - val_loss: 1395.7499\n",
            "Epoch 4063/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3573 - val_loss: 1395.8259\n",
            "Epoch 4064/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4466 - val_loss: 1395.6095\n",
            "Epoch 4065/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3365 - val_loss: 1395.7885\n",
            "Epoch 4066/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3498 - val_loss: 1395.8915\n",
            "Epoch 4067/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.3427 - val_loss: 1396.0704\n",
            "Epoch 4068/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.4136 - val_loss: 1396.3046\n",
            "Epoch 4069/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3911 - val_loss: 1396.1472\n",
            "Epoch 4070/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.3509 - val_loss: 1396.1095\n",
            "Epoch 4071/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.4141 - val_loss: 1395.6853\n",
            "Epoch 4072/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.4607 - val_loss: 1395.3903\n",
            "Epoch 4073/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3931 - val_loss: 1395.3674\n",
            "Epoch 4074/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3237 - val_loss: 1395.5630\n",
            "Epoch 4075/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.3895 - val_loss: 1395.9901\n",
            "Epoch 4076/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.3633 - val_loss: 1396.1135\n",
            "Epoch 4077/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3864 - val_loss: 1395.9310\n",
            "Epoch 4078/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3295 - val_loss: 1396.0769\n",
            "Epoch 4079/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.4240 - val_loss: 1395.8933\n",
            "Epoch 4080/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.5838 - val_loss: 1396.3984\n",
            "Epoch 4081/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3242 - val_loss: 1396.2288\n",
            "Epoch 4082/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3806 - val_loss: 1395.9673\n",
            "Epoch 4083/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3409 - val_loss: 1395.9183\n",
            "Epoch 4084/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3245 - val_loss: 1395.9324\n",
            "Epoch 4085/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3289 - val_loss: 1396.1036\n",
            "Epoch 4086/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3924 - val_loss: 1395.9797\n",
            "Epoch 4087/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3381 - val_loss: 1396.2900\n",
            "Epoch 4088/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4217 - val_loss: 1396.5994\n",
            "Epoch 4089/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3360 - val_loss: 1396.4135\n",
            "Epoch 4090/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3115 - val_loss: 1396.3835\n",
            "Epoch 4091/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3251 - val_loss: 1396.3938\n",
            "Epoch 4092/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3138 - val_loss: 1396.3541\n",
            "Epoch 4093/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3183 - val_loss: 1396.1160\n",
            "Epoch 4094/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3003 - val_loss: 1396.0552\n",
            "Epoch 4095/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3736 - val_loss: 1395.8060\n",
            "Epoch 4096/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3095 - val_loss: 1395.7617\n",
            "Epoch 4097/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.3396 - val_loss: 1395.9587\n",
            "Epoch 4098/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3230 - val_loss: 1395.8461\n",
            "Epoch 4099/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.3201 - val_loss: 1396.0239\n",
            "Epoch 4100/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2989 - val_loss: 1396.0355\n",
            "Epoch 4101/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3004 - val_loss: 1396.0439\n",
            "Epoch 4102/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3068 - val_loss: 1395.9738\n",
            "Epoch 4103/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2907 - val_loss: 1395.8615\n",
            "Epoch 4104/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3315 - val_loss: 1395.6320\n",
            "Epoch 4105/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3655 - val_loss: 1395.6022\n",
            "Epoch 4106/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3191 - val_loss: 1395.6804\n",
            "Epoch 4107/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3560 - val_loss: 1395.4281\n",
            "Epoch 4108/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2932 - val_loss: 1395.4725\n",
            "Epoch 4109/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3327 - val_loss: 1395.6682\n",
            "Epoch 4110/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3168 - val_loss: 1395.8351\n",
            "Epoch 4111/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3500 - val_loss: 1395.9546\n",
            "Epoch 4112/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3996 - val_loss: 1395.5516\n",
            "Epoch 4113/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3489 - val_loss: 1395.6754\n",
            "Epoch 4114/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2839 - val_loss: 1395.5781\n",
            "Epoch 4115/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2865 - val_loss: 1395.4319\n",
            "Epoch 4116/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2925 - val_loss: 1395.3708\n",
            "Epoch 4117/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2795 - val_loss: 1395.2448\n",
            "Epoch 4118/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3778 - val_loss: 1395.3467\n",
            "Epoch 4119/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.4371 - val_loss: 1394.8707\n",
            "Epoch 4120/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.4318 - val_loss: 1394.6738\n",
            "Epoch 4121/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3162 - val_loss: 1395.0212\n",
            "Epoch 4122/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3420 - val_loss: 1395.3342\n",
            "Epoch 4123/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3216 - val_loss: 1395.4916\n",
            "Epoch 4124/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2792 - val_loss: 1395.6348\n",
            "Epoch 4125/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1070.3500 - val_loss: 1395.5283\n",
            "Epoch 4126/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3435 - val_loss: 1395.9454\n",
            "Epoch 4127/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2897 - val_loss: 1396.0027\n",
            "Epoch 4128/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2870 - val_loss: 1396.2622\n",
            "Epoch 4129/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3055 - val_loss: 1396.4604\n",
            "Epoch 4130/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3023 - val_loss: 1396.3270\n",
            "Epoch 4131/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2773 - val_loss: 1396.2375\n",
            "Epoch 4132/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2791 - val_loss: 1396.3248\n",
            "Epoch 4133/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2820 - val_loss: 1396.1793\n",
            "Epoch 4134/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2944 - val_loss: 1396.1118\n",
            "Epoch 4135/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2813 - val_loss: 1396.1531\n",
            "Epoch 4136/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2622 - val_loss: 1396.3407\n",
            "Epoch 4137/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 950.954 - 0s 31us/sample - loss: 1070.2667 - val_loss: 1396.3867\n",
            "Epoch 4138/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.2839 - val_loss: 1396.2698\n",
            "Epoch 4139/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3829 - val_loss: 1396.5853\n",
            "Epoch 4140/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2550 - val_loss: 1396.4102\n",
            "Epoch 4141/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1114.05 - 0s 29us/sample - loss: 1070.2666 - val_loss: 1396.2095\n",
            "Epoch 4142/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.3217 - val_loss: 1396.2526\n",
            "Epoch 4143/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2526 - val_loss: 1396.2561\n",
            "Epoch 4144/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2574 - val_loss: 1396.1454\n",
            "Epoch 4145/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3013 - val_loss: 1396.1337\n",
            "Epoch 4146/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2520 - val_loss: 1395.9504\n",
            "Epoch 4147/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2502 - val_loss: 1395.9052\n",
            "Epoch 4148/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3563 - val_loss: 1395.5715\n",
            "Epoch 4149/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2682 - val_loss: 1395.5011\n",
            "Epoch 4150/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3327 - val_loss: 1395.7461\n",
            "Epoch 4151/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2426 - val_loss: 1395.6823\n",
            "Epoch 4152/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2547 - val_loss: 1395.6837\n",
            "Epoch 4153/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2430 - val_loss: 1395.6304\n",
            "Epoch 4154/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.2985 - val_loss: 1395.3735\n",
            "Epoch 4155/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3010 - val_loss: 1395.1477\n",
            "Epoch 4156/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2663 - val_loss: 1395.1050\n",
            "Epoch 4157/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3035 - val_loss: 1395.3011\n",
            "Epoch 4158/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2805 - val_loss: 1395.4425\n",
            "Epoch 4159/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2520 - val_loss: 1395.5928\n",
            "Epoch 4160/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2552 - val_loss: 1395.7480\n",
            "Epoch 4161/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2456 - val_loss: 1395.8307\n",
            "Epoch 4162/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2289 - val_loss: 1395.8422\n",
            "Epoch 4163/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2502 - val_loss: 1395.8564\n",
            "Epoch 4164/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2911 - val_loss: 1396.1199\n",
            "Epoch 4165/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.2466 - val_loss: 1396.1969\n",
            "Epoch 4166/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2294 - val_loss: 1396.3763\n",
            "Epoch 4167/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3015 - val_loss: 1396.2706\n",
            "Epoch 4168/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2314 - val_loss: 1396.4843\n",
            "Epoch 4169/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2300 - val_loss: 1396.4550\n",
            "Epoch 4170/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2267 - val_loss: 1396.4968\n",
            "Epoch 4171/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2785 - val_loss: 1396.7948\n",
            "Epoch 4172/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2888 - val_loss: 1396.8551\n",
            "Epoch 4173/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2153 - val_loss: 1396.7614\n",
            "Epoch 4174/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.2600 - val_loss: 1396.5389\n",
            "Epoch 4175/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3088 - val_loss: 1396.3252\n",
            "Epoch 4176/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2293 - val_loss: 1396.5569\n",
            "Epoch 4177/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2812 - val_loss: 1396.8615\n",
            "Epoch 4178/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3236 - val_loss: 1396.6327\n",
            "Epoch 4179/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2069 - val_loss: 1396.8126\n",
            "Epoch 4180/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2268 - val_loss: 1396.8492\n",
            "Epoch 4181/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2511 - val_loss: 1396.8441\n",
            "Epoch 4182/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.3018 - val_loss: 1397.2906\n",
            "Epoch 4183/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2617 - val_loss: 1397.2809\n",
            "Epoch 4184/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2129 - val_loss: 1397.2607\n",
            "Epoch 4185/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2662 - val_loss: 1397.0325\n",
            "Epoch 4186/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.3775 - val_loss: 1396.7432\n",
            "Epoch 4187/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2814 - val_loss: 1397.0833\n",
            "Epoch 4188/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2099 - val_loss: 1397.0121\n",
            "Epoch 4189/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2177 - val_loss: 1396.9397\n",
            "Epoch 4190/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2387 - val_loss: 1397.1144\n",
            "Epoch 4191/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.2243 - val_loss: 1397.1257\n",
            "Epoch 4192/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2081 - val_loss: 1396.9664\n",
            "Epoch 4193/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2254 - val_loss: 1396.8892\n",
            "Epoch 4194/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2952 - val_loss: 1396.4349\n",
            "Epoch 4195/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2363 - val_loss: 1396.1626\n",
            "Epoch 4196/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1922 - val_loss: 1396.1608\n",
            "Epoch 4197/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2069 - val_loss: 1396.2328\n",
            "Epoch 4198/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2208 - val_loss: 1396.2563\n",
            "Epoch 4199/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.2091 - val_loss: 1395.9839\n",
            "Epoch 4200/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.2070 - val_loss: 1395.9596\n",
            "Epoch 4201/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2137 - val_loss: 1395.7617\n",
            "Epoch 4202/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2017 - val_loss: 1395.8530\n",
            "Epoch 4203/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1877 - val_loss: 1395.8639\n",
            "Epoch 4204/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2157 - val_loss: 1395.9510\n",
            "Epoch 4205/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.2312 - val_loss: 1396.0939\n",
            "Epoch 4206/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2039 - val_loss: 1395.8961\n",
            "Epoch 4207/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1930 - val_loss: 1395.9575\n",
            "Epoch 4208/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1888 - val_loss: 1395.9791\n",
            "Epoch 4209/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2072 - val_loss: 1395.9376\n",
            "Epoch 4210/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1932 - val_loss: 1395.9357\n",
            "Epoch 4211/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2468 - val_loss: 1395.6412\n",
            "Epoch 4212/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.3260 - val_loss: 1395.4723\n",
            "Epoch 4213/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1897 - val_loss: 1395.7711\n",
            "Epoch 4214/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1633 - val_loss: 1396.1189\n",
            "Epoch 4215/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1701 - val_loss: 1396.4839\n",
            "Epoch 4216/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.2182 - val_loss: 1396.9061\n",
            "Epoch 4217/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1898 - val_loss: 1397.1597\n",
            "Epoch 4218/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.2248 - val_loss: 1397.3789\n",
            "Epoch 4219/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1772 - val_loss: 1397.2872\n",
            "Epoch 4220/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1948 - val_loss: 1397.2985\n",
            "Epoch 4221/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2315 - val_loss: 1396.9539\n",
            "Epoch 4222/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1891 - val_loss: 1396.7268\n",
            "Epoch 4223/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1643 - val_loss: 1396.6312\n",
            "Epoch 4224/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1736 - val_loss: 1396.5389\n",
            "Epoch 4225/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1678 - val_loss: 1396.5350\n",
            "Epoch 4226/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1690 - val_loss: 1396.5148\n",
            "Epoch 4227/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1701 - val_loss: 1396.3988\n",
            "Epoch 4228/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1493 - val_loss: 1396.2229\n",
            "Epoch 4229/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1070.1592 - val_loss: 1396.0487\n",
            "Epoch 4230/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1698 - val_loss: 1396.0151\n",
            "Epoch 4231/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1595 - val_loss: 1395.9196\n",
            "Epoch 4232/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1682 - val_loss: 1395.9686\n",
            "Epoch 4233/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1970 - val_loss: 1395.8828\n",
            "Epoch 4234/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1789 - val_loss: 1396.0348\n",
            "Epoch 4235/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1746 - val_loss: 1396.1893\n",
            "Epoch 4236/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2688 - val_loss: 1395.9886\n",
            "Epoch 4237/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1677 - val_loss: 1396.0503\n",
            "Epoch 4238/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1542 - val_loss: 1396.1456\n",
            "Epoch 4239/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1841 - val_loss: 1396.4203\n",
            "Epoch 4240/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1717 - val_loss: 1396.3575\n",
            "Epoch 4241/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2083 - val_loss: 1396.6182\n",
            "Epoch 4242/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1359 - val_loss: 1396.7355\n",
            "Epoch 4243/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1569 - val_loss: 1396.9235\n",
            "Epoch 4244/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1441 - val_loss: 1397.1091\n",
            "Epoch 4245/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1751 - val_loss: 1397.0563\n",
            "Epoch 4246/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1841 - val_loss: 1397.2500\n",
            "Epoch 4247/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1070.1480 - val_loss: 1397.3730\n",
            "Epoch 4248/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2686 - val_loss: 1397.8204\n",
            "Epoch 4249/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1775 - val_loss: 1397.9839\n",
            "Epoch 4250/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2755 - val_loss: 1398.2279\n",
            "Epoch 4251/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2279 - val_loss: 1398.3827\n",
            "Epoch 4252/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1716 - val_loss: 1398.2621\n",
            "Epoch 4253/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.3182 - val_loss: 1398.2927\n",
            "Epoch 4254/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2938 - val_loss: 1397.7098\n",
            "Epoch 4255/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1446 - val_loss: 1397.5653\n",
            "Epoch 4256/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1070.2989 - val_loss: 1397.1652\n",
            "Epoch 4257/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1618 - val_loss: 1397.3612\n",
            "Epoch 4258/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1402 - val_loss: 1397.2750\n",
            "Epoch 4259/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1526 - val_loss: 1397.1837\n",
            "Epoch 4260/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.1671 - val_loss: 1397.3988\n",
            "Epoch 4261/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1487 - val_loss: 1397.3136\n",
            "Epoch 4262/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1569 - val_loss: 1397.4720\n",
            "Epoch 4263/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1424 - val_loss: 1397.3181\n",
            "Epoch 4264/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1575 - val_loss: 1397.1849\n",
            "Epoch 4265/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1070.1639 - val_loss: 1397.2415\n",
            "Epoch 4266/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1236 - val_loss: 1397.4446\n",
            "Epoch 4267/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.1707 - val_loss: 1397.6334\n",
            "Epoch 4268/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1463 - val_loss: 1397.5491\n",
            "Epoch 4269/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1799 - val_loss: 1397.2703\n",
            "Epoch 4270/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1981 - val_loss: 1397.4177\n",
            "Epoch 4271/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1330 - val_loss: 1397.3518\n",
            "Epoch 4272/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1251 - val_loss: 1397.1594\n",
            "Epoch 4273/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1664 - val_loss: 1397.1177\n",
            "Epoch 4274/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1157 - val_loss: 1396.7065\n",
            "Epoch 4275/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1061 - val_loss: 1396.5321\n",
            "Epoch 4276/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.3364 - val_loss: 1395.9900\n",
            "Epoch 4277/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1947 - val_loss: 1396.0529\n",
            "Epoch 4278/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2036 - val_loss: 1395.8289\n",
            "Epoch 4279/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1419 - val_loss: 1396.0282\n",
            "Epoch 4280/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1335 - val_loss: 1396.1912\n",
            "Epoch 4281/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1551 - val_loss: 1396.5981\n",
            "Epoch 4282/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1115 - val_loss: 1396.8069\n",
            "Epoch 4283/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1253 - val_loss: 1396.9851\n",
            "Epoch 4284/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1312 - val_loss: 1396.9064\n",
            "Epoch 4285/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1178 - val_loss: 1397.0109\n",
            "Epoch 4286/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1167 - val_loss: 1397.0203\n",
            "Epoch 4287/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1563 - val_loss: 1396.9169\n",
            "Epoch 4288/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0992 - val_loss: 1396.8914\n",
            "Epoch 4289/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0971 - val_loss: 1396.8560\n",
            "Epoch 4290/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0997 - val_loss: 1396.8147\n",
            "Epoch 4291/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1200 - val_loss: 1396.7062\n",
            "Epoch 4292/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0917 - val_loss: 1396.8221\n",
            "Epoch 4293/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2242 - val_loss: 1397.1311\n",
            "Epoch 4294/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1021 - val_loss: 1396.9186\n",
            "Epoch 4295/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1234 - val_loss: 1396.7354\n",
            "Epoch 4296/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1120 - val_loss: 1396.8795\n",
            "Epoch 4297/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1893 - val_loss: 1396.6451\n",
            "Epoch 4298/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1147 - val_loss: 1396.7648\n",
            "Epoch 4299/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.1074 - val_loss: 1396.8269\n",
            "Epoch 4300/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1070.0947 - val_loss: 1396.8042\n",
            "Epoch 4301/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.0998 - val_loss: 1396.6091\n",
            "Epoch 4302/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.3012 - val_loss: 1396.2571\n",
            "Epoch 4303/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0831 - val_loss: 1396.3840\n",
            "Epoch 4304/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1266 - val_loss: 1396.7013\n",
            "Epoch 4305/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1368 - val_loss: 1396.8959\n",
            "Epoch 4306/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0949 - val_loss: 1396.7649\n",
            "Epoch 4307/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1589 - val_loss: 1396.9735\n",
            "Epoch 4308/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0838 - val_loss: 1396.9291\n",
            "Epoch 4309/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0814 - val_loss: 1396.8772\n",
            "Epoch 4310/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0909 - val_loss: 1396.8075\n",
            "Epoch 4311/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1413 - val_loss: 1397.0334\n",
            "Epoch 4312/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.0982 - val_loss: 1396.8500\n",
            "Epoch 4313/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1380 - val_loss: 1396.7183\n",
            "Epoch 4314/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0738 - val_loss: 1396.8967\n",
            "Epoch 4315/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.2683 - val_loss: 1397.3546\n",
            "Epoch 4316/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0733 - val_loss: 1397.2910\n",
            "Epoch 4317/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1580 - val_loss: 1397.4424\n",
            "Epoch 4318/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1321 - val_loss: 1397.1349\n",
            "Epoch 4319/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1055 - val_loss: 1397.1432\n",
            "Epoch 4320/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0689 - val_loss: 1397.1952\n",
            "Epoch 4321/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.2212 - val_loss: 1397.5076\n",
            "Epoch 4322/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1226 - val_loss: 1397.3385\n",
            "Epoch 4323/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0806 - val_loss: 1397.2661\n",
            "Epoch 4324/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1183 - val_loss: 1397.0643\n",
            "Epoch 4325/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0900 - val_loss: 1397.1927\n",
            "Epoch 4326/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 720.476 - 0s 28us/sample - loss: 1070.1397 - val_loss: 1396.9703\n",
            "Epoch 4327/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0801 - val_loss: 1396.9999\n",
            "Epoch 4328/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1070.0789 - val_loss: 1397.1890\n",
            "Epoch 4329/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.0603 - val_loss: 1397.1920\n",
            "Epoch 4330/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1678 - val_loss: 1397.3945\n",
            "Epoch 4331/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1670 - val_loss: 1396.9688\n",
            "Epoch 4332/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0866 - val_loss: 1396.9482\n",
            "Epoch 4333/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1427 - val_loss: 1397.1798\n",
            "Epoch 4334/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0539 - val_loss: 1397.0342\n",
            "Epoch 4335/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0629 - val_loss: 1397.0270\n",
            "Epoch 4336/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0927 - val_loss: 1396.7977\n",
            "Epoch 4337/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1908 - val_loss: 1396.4578\n",
            "Epoch 4338/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.0549 - val_loss: 1396.5605\n",
            "Epoch 4339/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0707 - val_loss: 1396.5537\n",
            "Epoch 4340/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0822 - val_loss: 1396.8381\n",
            "Epoch 4341/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.1464 - val_loss: 1397.0906\n",
            "Epoch 4342/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0895 - val_loss: 1396.9628\n",
            "Epoch 4343/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0488 - val_loss: 1396.9849\n",
            "Epoch 4344/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0491 - val_loss: 1396.9539\n",
            "Epoch 4345/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0502 - val_loss: 1396.9614\n",
            "Epoch 4346/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0410 - val_loss: 1396.8464\n",
            "Epoch 4347/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1261 - val_loss: 1396.6028\n",
            "Epoch 4348/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0990 - val_loss: 1396.8594\n",
            "Epoch 4349/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0739 - val_loss: 1396.8671\n",
            "Epoch 4350/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0917 - val_loss: 1396.7123\n",
            "Epoch 4351/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0944 - val_loss: 1396.9797\n",
            "Epoch 4352/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0385 - val_loss: 1396.9917\n",
            "Epoch 4353/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1051.97 - 0s 28us/sample - loss: 1070.0493 - val_loss: 1396.9003\n",
            "Epoch 4354/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0698 - val_loss: 1396.8552\n",
            "Epoch 4355/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0495 - val_loss: 1397.1072\n",
            "Epoch 4356/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0413 - val_loss: 1397.2695\n",
            "Epoch 4357/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0338 - val_loss: 1397.2822\n",
            "Epoch 4358/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0705 - val_loss: 1397.3953\n",
            "Epoch 4359/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0449 - val_loss: 1397.4939\n",
            "Epoch 4360/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1070.0457 - val_loss: 1397.5168\n",
            "Epoch 4361/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0318 - val_loss: 1397.5530\n",
            "Epoch 4362/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0594 - val_loss: 1397.6656\n",
            "Epoch 4363/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.0311 - val_loss: 1397.5731\n",
            "Epoch 4364/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0565 - val_loss: 1397.5697\n",
            "Epoch 4365/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.2203 - val_loss: 1397.0737\n",
            "Epoch 4366/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0264 - val_loss: 1397.0941\n",
            "Epoch 4367/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1371 - val_loss: 1397.3325\n",
            "Epoch 4368/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0369 - val_loss: 1397.0856\n",
            "Epoch 4369/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.0265 - val_loss: 1397.0039\n",
            "Epoch 4370/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0318 - val_loss: 1396.9980\n",
            "Epoch 4371/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0564 - val_loss: 1396.7437\n",
            "Epoch 4372/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0371 - val_loss: 1396.7803\n",
            "Epoch 4373/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0633 - val_loss: 1396.8611\n",
            "Epoch 4374/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0578 - val_loss: 1396.5769\n",
            "Epoch 4375/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1065 - val_loss: 1396.3202\n",
            "Epoch 4376/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0723 - val_loss: 1396.2731\n",
            "Epoch 4377/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.0194 - val_loss: 1396.5033\n",
            "Epoch 4378/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1399 - val_loss: 1396.9819\n",
            "Epoch 4379/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.0362 - val_loss: 1397.0017\n",
            "Epoch 4380/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0773 - val_loss: 1397.2609\n",
            "Epoch 4381/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.1242 - val_loss: 1397.0715\n",
            "Epoch 4382/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.1388 - val_loss: 1397.3884\n",
            "Epoch 4383/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0129 - val_loss: 1397.3256\n",
            "Epoch 4384/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0506 - val_loss: 1397.3311\n",
            "Epoch 4385/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1101 - val_loss: 1396.8962\n",
            "Epoch 4386/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0564 - val_loss: 1396.6971\n",
            "Epoch 4387/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1225.77 - 0s 31us/sample - loss: 1070.2039 - val_loss: 1397.0569\n",
            "Epoch 4388/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0119 - val_loss: 1396.8746\n",
            "Epoch 4389/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.2224 - val_loss: 1396.4407\n",
            "Epoch 4390/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1071 - val_loss: 1396.4363\n",
            "Epoch 4391/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0860 - val_loss: 1396.5323\n",
            "Epoch 4392/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0360 - val_loss: 1396.9271\n",
            "Epoch 4393/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0243 - val_loss: 1397.0072\n",
            "Epoch 4394/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9923 - val_loss: 1397.1947\n",
            "Epoch 4395/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0057 - val_loss: 1397.4768\n",
            "Epoch 4396/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.1045 - val_loss: 1397.3912\n",
            "Epoch 4397/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9812 - val_loss: 1397.6459\n",
            "Epoch 4398/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1232 - val_loss: 1398.1493\n",
            "Epoch 4399/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0080 - val_loss: 1398.3643\n",
            "Epoch 4400/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0313 - val_loss: 1398.5348\n",
            "Epoch 4401/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.1167 - val_loss: 1398.2904\n",
            "Epoch 4402/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0179 - val_loss: 1398.3806\n",
            "Epoch 4403/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.1440 - val_loss: 1398.0724\n",
            "Epoch 4404/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0050 - val_loss: 1398.1768\n",
            "Epoch 4405/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0525 - val_loss: 1398.2512\n",
            "Epoch 4406/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0393 - val_loss: 1398.2222\n",
            "Epoch 4407/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9984 - val_loss: 1398.3860\n",
            "Epoch 4408/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0381 - val_loss: 1398.3427\n",
            "Epoch 4409/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0188 - val_loss: 1398.5912\n",
            "Epoch 4410/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0237 - val_loss: 1398.5885\n",
            "Epoch 4411/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0333 - val_loss: 1398.6605\n",
            "Epoch 4412/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0104 - val_loss: 1398.4663\n",
            "Epoch 4413/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0373 - val_loss: 1398.2277\n",
            "Epoch 4414/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0129 - val_loss: 1398.2815\n",
            "Epoch 4415/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9998 - val_loss: 1398.0912\n",
            "Epoch 4416/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9943 - val_loss: 1397.9230\n",
            "Epoch 4417/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0161 - val_loss: 1397.6729\n",
            "Epoch 4418/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9911 - val_loss: 1397.6333\n",
            "Epoch 4419/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0475 - val_loss: 1397.7466\n",
            "Epoch 4420/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9762 - val_loss: 1397.6053\n",
            "Epoch 4421/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1070.0394 - val_loss: 1397.1833\n",
            "Epoch 4422/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0381 - val_loss: 1397.1530\n",
            "Epoch 4423/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9839 - val_loss: 1396.9497\n",
            "Epoch 4424/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9791 - val_loss: 1396.7920\n",
            "Epoch 4425/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0023 - val_loss: 1396.6177\n",
            "Epoch 4426/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1070.0438 - val_loss: 1396.8221\n",
            "Epoch 4427/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0295 - val_loss: 1396.8605\n",
            "Epoch 4428/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0257 - val_loss: 1396.5460\n",
            "Epoch 4429/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0641 - val_loss: 1396.3473\n",
            "Epoch 4430/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1675 - val_loss: 1396.7789\n",
            "Epoch 4431/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0847 - val_loss: 1396.9573\n",
            "Epoch 4432/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9857 - val_loss: 1396.7344\n",
            "Epoch 4433/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0255 - val_loss: 1396.7833\n",
            "Epoch 4434/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0264 - val_loss: 1396.6768\n",
            "Epoch 4435/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.1213 - val_loss: 1397.0776\n",
            "Epoch 4436/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9762 - val_loss: 1397.0770\n",
            "Epoch 4437/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9977 - val_loss: 1396.9668\n",
            "Epoch 4438/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.9698 - val_loss: 1396.9510\n",
            "Epoch 4439/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9620 - val_loss: 1397.0494\n",
            "Epoch 4440/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0305 - val_loss: 1397.2972\n",
            "Epoch 4441/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1070.0161 - val_loss: 1397.4041\n",
            "Epoch 4442/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9587 - val_loss: 1397.3147\n",
            "Epoch 4443/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0291 - val_loss: 1397.0479\n",
            "Epoch 4444/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9864 - val_loss: 1397.0024\n",
            "Epoch 4445/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0434 - val_loss: 1396.8063\n",
            "Epoch 4446/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9618 - val_loss: 1396.9427\n",
            "Epoch 4447/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9943 - val_loss: 1397.2201\n",
            "Epoch 4448/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 856.142 - 0s 29us/sample - loss: 1069.9598 - val_loss: 1397.2542\n",
            "Epoch 4449/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.9585 - val_loss: 1397.3757\n",
            "Epoch 4450/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9762 - val_loss: 1397.5186\n",
            "Epoch 4451/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9564 - val_loss: 1397.4696\n",
            "Epoch 4452/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0319 - val_loss: 1397.2957\n",
            "Epoch 4453/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0129 - val_loss: 1397.2491\n",
            "Epoch 4454/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9395 - val_loss: 1397.4669\n",
            "Epoch 4455/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9555 - val_loss: 1397.6558\n",
            "Epoch 4456/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 1001.53 - 0s 28us/sample - loss: 1069.9433 - val_loss: 1397.7788\n",
            "Epoch 4457/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9585 - val_loss: 1397.9722\n",
            "Epoch 4458/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9563 - val_loss: 1398.0709\n",
            "Epoch 4459/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9695 - val_loss: 1398.0999\n",
            "Epoch 4460/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0313 - val_loss: 1397.7896\n",
            "Epoch 4461/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9554 - val_loss: 1397.7203\n",
            "Epoch 4462/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9431 - val_loss: 1397.7494\n",
            "Epoch 4463/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9691 - val_loss: 1397.7627\n",
            "Epoch 4464/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9429 - val_loss: 1397.8491\n",
            "Epoch 4465/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9579 - val_loss: 1397.8210\n",
            "Epoch 4466/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0182 - val_loss: 1397.8190\n",
            "Epoch 4467/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9575 - val_loss: 1397.7672\n",
            "Epoch 4468/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9476 - val_loss: 1397.8063\n",
            "Epoch 4469/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9383 - val_loss: 1397.7512\n",
            "Epoch 4470/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9669 - val_loss: 1397.8158\n",
            "Epoch 4471/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9343 - val_loss: 1397.8252\n",
            "Epoch 4472/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9393 - val_loss: 1397.7660\n",
            "Epoch 4473/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9355 - val_loss: 1397.8397\n",
            "Epoch 4474/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9580 - val_loss: 1397.7246\n",
            "Epoch 4475/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.9501 - val_loss: 1397.8712\n",
            "Epoch 4476/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9622 - val_loss: 1397.7686\n",
            "Epoch 4477/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9638 - val_loss: 1397.7075\n",
            "Epoch 4478/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.0167 - val_loss: 1397.4407\n",
            "Epoch 4479/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.9540 - val_loss: 1397.5507\n",
            "Epoch 4480/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.1132 - val_loss: 1397.7461\n",
            "Epoch 4481/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1070.1568 - val_loss: 1397.2579\n",
            "Epoch 4482/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9470 - val_loss: 1397.3900\n",
            "Epoch 4483/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.9494 - val_loss: 1397.4454\n",
            "Epoch 4484/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.9516 - val_loss: 1397.2323\n",
            "Epoch 4485/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9984 - val_loss: 1397.3837\n",
            "Epoch 4486/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9200 - val_loss: 1397.2462\n",
            "Epoch 4487/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9708 - val_loss: 1396.9843\n",
            "Epoch 4488/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1070.0516 - val_loss: 1396.7825\n",
            "Epoch 4489/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9511 - val_loss: 1396.7678\n",
            "Epoch 4490/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9244 - val_loss: 1396.9220\n",
            "Epoch 4491/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9208 - val_loss: 1397.0812\n",
            "Epoch 4492/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0408 - val_loss: 1397.4805\n",
            "Epoch 4493/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9522 - val_loss: 1397.3840\n",
            "Epoch 4494/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9661 - val_loss: 1397.6514\n",
            "Epoch 4495/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.9406 - val_loss: 1397.7104\n",
            "Epoch 4496/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9276 - val_loss: 1397.7844\n",
            "Epoch 4497/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0300 - val_loss: 1397.9781\n",
            "Epoch 4498/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1069.9432 - val_loss: 1397.7369\n",
            "Epoch 4499/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9472 - val_loss: 1397.5977\n",
            "Epoch 4500/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.9226 - val_loss: 1397.5740\n",
            "Epoch 4501/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9534 - val_loss: 1397.6891\n",
            "Epoch 4502/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9843 - val_loss: 1397.4122\n",
            "Epoch 4503/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9177 - val_loss: 1397.2938\n",
            "Epoch 4504/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9517 - val_loss: 1397.3541\n",
            "Epoch 4505/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.9452 - val_loss: 1397.1082\n",
            "Epoch 4506/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9717 - val_loss: 1397.2476\n",
            "Epoch 4507/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9257 - val_loss: 1397.1075\n",
            "Epoch 4508/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9711 - val_loss: 1397.2833\n",
            "Epoch 4509/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9488 - val_loss: 1397.3317\n",
            "Epoch 4510/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9779 - val_loss: 1396.9661\n",
            "Epoch 4511/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9446 - val_loss: 1397.0369\n",
            "Epoch 4512/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9788 - val_loss: 1396.7717\n",
            "Epoch 4513/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.9561 - val_loss: 1396.9507\n",
            "Epoch 4514/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9166 - val_loss: 1396.9517\n",
            "Epoch 4515/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9161 - val_loss: 1396.9607\n",
            "Epoch 4516/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9570 - val_loss: 1396.7610\n",
            "Epoch 4517/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1070.0879 - val_loss: 1396.5660\n",
            "Epoch 4518/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9969 - val_loss: 1397.0518\n",
            "Epoch 4519/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9407 - val_loss: 1397.1891\n",
            "Epoch 4520/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9002 - val_loss: 1397.2352\n",
            "Epoch 4521/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9821 - val_loss: 1397.5386\n",
            "Epoch 4522/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9763 - val_loss: 1397.6830\n",
            "Epoch 4523/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9315 - val_loss: 1397.4608\n",
            "Epoch 4524/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.9476 - val_loss: 1397.2943\n",
            "Epoch 4525/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9019 - val_loss: 1397.3448\n",
            "Epoch 4526/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9419 - val_loss: 1397.6364\n",
            "Epoch 4527/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9209 - val_loss: 1397.8011\n",
            "Epoch 4528/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9680 - val_loss: 1397.9823\n",
            "Epoch 4529/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9651 - val_loss: 1397.6727\n",
            "Epoch 4530/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9166 - val_loss: 1397.5182\n",
            "Epoch 4531/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9191 - val_loss: 1397.6809\n",
            "Epoch 4532/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9111 - val_loss: 1397.5304\n",
            "Epoch 4533/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8836 - val_loss: 1397.5851\n",
            "Epoch 4534/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9029 - val_loss: 1397.6060\n",
            "Epoch 4535/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9214 - val_loss: 1397.5444\n",
            "Epoch 4536/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.9595 - val_loss: 1397.8398\n",
            "Epoch 4537/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8961 - val_loss: 1397.7268\n",
            "Epoch 4538/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.9513 - val_loss: 1397.9231\n",
            "Epoch 4539/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9395 - val_loss: 1397.8031\n",
            "Epoch 4540/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8772 - val_loss: 1397.9341\n",
            "Epoch 4541/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9544 - val_loss: 1398.1686\n",
            "Epoch 4542/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0971 - val_loss: 1397.7737\n",
            "Epoch 4543/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8967 - val_loss: 1398.0160\n",
            "Epoch 4544/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8790 - val_loss: 1398.0219\n",
            "Epoch 4545/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9390 - val_loss: 1397.9163\n",
            "Epoch 4546/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8711 - val_loss: 1398.1143\n",
            "Epoch 4547/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8728 - val_loss: 1398.2703\n",
            "Epoch 4548/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9299 - val_loss: 1398.4557\n",
            "Epoch 4549/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9341 - val_loss: 1398.6543\n",
            "Epoch 4550/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8816 - val_loss: 1398.5222\n",
            "Epoch 4551/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8859 - val_loss: 1398.3801\n",
            "Epoch 4552/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9125 - val_loss: 1398.4689\n",
            "Epoch 4553/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8810 - val_loss: 1398.4169\n",
            "Epoch 4554/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9422 - val_loss: 1398.0583\n",
            "Epoch 4555/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8679 - val_loss: 1397.9943\n",
            "Epoch 4556/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9112 - val_loss: 1397.9453\n",
            "Epoch 4557/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8657 - val_loss: 1397.8500\n",
            "Epoch 4558/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8683 - val_loss: 1397.6276\n",
            "Epoch 4559/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.8861 - val_loss: 1397.4803\n",
            "Epoch 4560/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8844 - val_loss: 1397.5746\n",
            "Epoch 4561/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9216 - val_loss: 1397.6854\n",
            "Epoch 4562/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9361 - val_loss: 1397.3569\n",
            "Epoch 4563/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8800 - val_loss: 1397.2749\n",
            "Epoch 4564/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9835 - val_loss: 1397.5402\n",
            "Epoch 4565/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8875 - val_loss: 1397.5424\n",
            "Epoch 4566/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8666 - val_loss: 1397.3466\n",
            "Epoch 4567/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.8853 - val_loss: 1397.3884\n",
            "Epoch 4568/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9358 - val_loss: 1397.1111\n",
            "Epoch 4569/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.9357 - val_loss: 1397.3329\n",
            "Epoch 4570/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8803 - val_loss: 1397.3499\n",
            "Epoch 4571/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8628 - val_loss: 1397.2148\n",
            "Epoch 4572/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8964 - val_loss: 1397.2297\n",
            "Epoch 4573/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9150 - val_loss: 1397.3695\n",
            "Epoch 4574/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8645 - val_loss: 1397.2789\n",
            "Epoch 4575/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9418 - val_loss: 1397.3945\n",
            "Epoch 4576/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8647 - val_loss: 1397.2019\n",
            "Epoch 4577/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8588 - val_loss: 1396.9414\n",
            "Epoch 4578/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.9028 - val_loss: 1396.7218\n",
            "Epoch 4579/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9654 - val_loss: 1396.5043\n",
            "Epoch 4580/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8908 - val_loss: 1396.6041\n",
            "Epoch 4581/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8825 - val_loss: 1396.7390\n",
            "Epoch 4582/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8843 - val_loss: 1396.7627\n",
            "Epoch 4583/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8759 - val_loss: 1396.8949\n",
            "Epoch 4584/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8892 - val_loss: 1397.1879\n",
            "Epoch 4585/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8802 - val_loss: 1397.4331\n",
            "Epoch 4586/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8809 - val_loss: 1397.4753\n",
            "Epoch 4587/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.8413 - val_loss: 1397.6088\n",
            "Epoch 4588/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9431 - val_loss: 1397.9453\n",
            "Epoch 4589/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8719 - val_loss: 1397.8444\n",
            "Epoch 4590/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8454 - val_loss: 1397.9346\n",
            "Epoch 4591/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8479 - val_loss: 1398.0179\n",
            "Epoch 4592/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8690 - val_loss: 1397.8945\n",
            "Epoch 4593/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8540 - val_loss: 1397.9957\n",
            "Epoch 4594/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8644 - val_loss: 1398.0990\n",
            "Epoch 4595/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.9026 - val_loss: 1398.1628\n",
            "Epoch 4596/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8552 - val_loss: 1398.1976\n",
            "Epoch 4597/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8405 - val_loss: 1398.0729\n",
            "Epoch 4598/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1070.0906 - val_loss: 1397.5858\n",
            "Epoch 4599/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8450 - val_loss: 1397.7557\n",
            "Epoch 4600/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9070 - val_loss: 1397.8850\n",
            "Epoch 4601/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8500 - val_loss: 1397.9822\n",
            "Epoch 4602/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8745 - val_loss: 1397.9937\n",
            "Epoch 4603/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8430 - val_loss: 1398.1469\n",
            "Epoch 4604/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8681 - val_loss: 1398.1113\n",
            "Epoch 4605/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1070.0270 - val_loss: 1398.5673\n",
            "Epoch 4606/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8869 - val_loss: 1398.3306\n",
            "Epoch 4607/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8576 - val_loss: 1398.3605\n",
            "Epoch 4608/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8354 - val_loss: 1398.3885\n",
            "Epoch 4609/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.9158 - val_loss: 1398.4980\n",
            "Epoch 4610/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8649 - val_loss: 1398.5576\n",
            "Epoch 4611/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8335 - val_loss: 1398.5042\n",
            "Epoch 4612/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8429 - val_loss: 1398.4523\n",
            "Epoch 4613/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8960 - val_loss: 1398.0508\n",
            "Epoch 4614/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.9520 - val_loss: 1397.7244\n",
            "Epoch 4615/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8451 - val_loss: 1397.9004\n",
            "Epoch 4616/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8698 - val_loss: 1398.0354\n",
            "Epoch 4617/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8631 - val_loss: 1397.8916\n",
            "Epoch 4618/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.8181 - val_loss: 1397.9561\n",
            "Epoch 4619/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8527 - val_loss: 1398.1320\n",
            "Epoch 4620/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9271 - val_loss: 1398.4236\n",
            "Epoch 4621/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8638 - val_loss: 1398.2089\n",
            "Epoch 4622/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8989 - val_loss: 1398.2729\n",
            "Epoch 4623/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8351 - val_loss: 1398.2375\n",
            "Epoch 4624/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8056 - val_loss: 1398.0708\n",
            "Epoch 4625/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8645 - val_loss: 1397.7278\n",
            "Epoch 4626/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9289 - val_loss: 1397.3925\n",
            "Epoch 4627/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8289 - val_loss: 1397.4821\n",
            "Epoch 4628/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9254 - val_loss: 1397.3708\n",
            "Epoch 4629/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8196 - val_loss: 1397.4258\n",
            "Epoch 4630/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8642 - val_loss: 1397.4132\n",
            "Epoch 4631/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8636 - val_loss: 1397.8146\n",
            "Epoch 4632/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8171 - val_loss: 1398.0428\n",
            "Epoch 4633/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8061 - val_loss: 1398.2795\n",
            "Epoch 4634/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 587.495 - 0s 36us/sample - loss: 1069.8868 - val_loss: 1398.2219\n",
            "Epoch 4635/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8433 - val_loss: 1398.5195\n",
            "Epoch 4636/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8116 - val_loss: 1398.8602\n",
            "Epoch 4637/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8216 - val_loss: 1399.0671\n",
            "Epoch 4638/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8912 - val_loss: 1399.2906\n",
            "Epoch 4639/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8355 - val_loss: 1399.2411\n",
            "Epoch 4640/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8395 - val_loss: 1399.2791\n",
            "Epoch 4641/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8593 - val_loss: 1399.0570\n",
            "Epoch 4642/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8511 - val_loss: 1399.0865\n",
            "Epoch 4643/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.9222 - val_loss: 1398.7040\n",
            "Epoch 4644/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8268 - val_loss: 1398.5164\n",
            "Epoch 4645/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8777 - val_loss: 1398.3228\n",
            "Epoch 4646/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8446 - val_loss: 1398.4984\n",
            "Epoch 4647/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8932 - val_loss: 1398.2920\n",
            "Epoch 4648/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8260 - val_loss: 1398.2997\n",
            "Epoch 4649/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1069.8321 - val_loss: 1398.3323\n",
            "Epoch 4650/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8637 - val_loss: 1398.7437\n",
            "Epoch 4651/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8078 - val_loss: 1398.8677\n",
            "Epoch 4652/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8127 - val_loss: 1398.8083\n",
            "Epoch 4653/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8050 - val_loss: 1398.8494\n",
            "Epoch 4654/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7999 - val_loss: 1398.8035\n",
            "Epoch 4655/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8130 - val_loss: 1398.6716\n",
            "Epoch 4656/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8272 - val_loss: 1398.5375\n",
            "Epoch 4657/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7991 - val_loss: 1398.5630\n",
            "Epoch 4658/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8714 - val_loss: 1398.4987\n",
            "Epoch 4659/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8939 - val_loss: 1398.9561\n",
            "Epoch 4660/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8029 - val_loss: 1399.0593\n",
            "Epoch 4661/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8127 - val_loss: 1399.0687\n",
            "Epoch 4662/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7994 - val_loss: 1398.9298\n",
            "Epoch 4663/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8061 - val_loss: 1398.9192\n",
            "Epoch 4664/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7974 - val_loss: 1398.8066\n",
            "Epoch 4665/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8452 - val_loss: 1398.4910\n",
            "Epoch 4666/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7845 - val_loss: 1398.4479\n",
            "Epoch 4667/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8273 - val_loss: 1398.2297\n",
            "Epoch 4668/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7993 - val_loss: 1398.3156\n",
            "Epoch 4669/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8124 - val_loss: 1398.1276\n",
            "Epoch 4670/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8197 - val_loss: 1398.2727\n",
            "Epoch 4671/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8798 - val_loss: 1398.2939\n",
            "Epoch 4672/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8025 - val_loss: 1398.0256\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4673/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7839 - val_loss: 1397.9846\n",
            "Epoch 4674/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.9161 - val_loss: 1398.0917\n",
            "Epoch 4675/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7834 - val_loss: 1397.6888\n",
            "Epoch 4676/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7879 - val_loss: 1397.4852\n",
            "Epoch 4677/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8259 - val_loss: 1397.4879\n",
            "Epoch 4678/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8130 - val_loss: 1397.3484\n",
            "Epoch 4679/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8269 - val_loss: 1397.0289\n",
            "Epoch 4680/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7999 - val_loss: 1396.8684\n",
            "Epoch 4681/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8152 - val_loss: 1396.8090\n",
            "Epoch 4682/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8214 - val_loss: 1396.8690\n",
            "Epoch 4683/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8169 - val_loss: 1396.9142\n",
            "Epoch 4684/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8863 - val_loss: 1396.7104\n",
            "Epoch 4685/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8156 - val_loss: 1396.7777\n",
            "Epoch 4686/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8112 - val_loss: 1396.9453\n",
            "Epoch 4687/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8242 - val_loss: 1397.2450\n",
            "Epoch 4688/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8078 - val_loss: 1397.4736\n",
            "Epoch 4689/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7894 - val_loss: 1397.6769\n",
            "Epoch 4690/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7729 - val_loss: 1397.7861\n",
            "Epoch 4691/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7767 - val_loss: 1397.9115\n",
            "Epoch 4692/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8159 - val_loss: 1397.7982\n",
            "Epoch 4693/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7695 - val_loss: 1397.8804\n",
            "Epoch 4694/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7639 - val_loss: 1398.0220\n",
            "Epoch 4695/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7600 - val_loss: 1398.1835\n",
            "Epoch 4696/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7665 - val_loss: 1398.4261\n",
            "Epoch 4697/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.8001 - val_loss: 1398.7050\n",
            "Epoch 4698/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7816 - val_loss: 1398.7830\n",
            "Epoch 4699/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7611 - val_loss: 1398.7186\n",
            "Epoch 4700/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7968 - val_loss: 1398.6995\n",
            "Epoch 4701/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7537 - val_loss: 1398.5437\n",
            "Epoch 4702/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7537 - val_loss: 1398.3027\n",
            "Epoch 4703/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8664 - val_loss: 1397.9479\n",
            "Epoch 4704/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7818 - val_loss: 1398.0273\n",
            "Epoch 4705/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7740 - val_loss: 1397.8611\n",
            "Epoch 4706/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7638 - val_loss: 1397.8365\n",
            "Epoch 4707/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7591 - val_loss: 1397.8335\n",
            "Epoch 4708/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7938 - val_loss: 1398.0039\n",
            "Epoch 4709/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8679 - val_loss: 1397.8337\n",
            "Epoch 4710/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7928 - val_loss: 1398.1609\n",
            "Epoch 4711/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7656 - val_loss: 1398.3237\n",
            "Epoch 4712/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7906 - val_loss: 1398.2977\n",
            "Epoch 4713/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7699 - val_loss: 1398.4238\n",
            "Epoch 4714/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7499 - val_loss: 1398.3715\n",
            "Epoch 4715/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7477 - val_loss: 1398.3517\n",
            "Epoch 4716/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7588 - val_loss: 1398.3680\n",
            "Epoch 4717/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7498 - val_loss: 1398.3176\n",
            "Epoch 4718/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7829 - val_loss: 1398.3154\n",
            "Epoch 4719/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8430 - val_loss: 1397.8768\n",
            "Epoch 4720/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8753 - val_loss: 1397.5870\n",
            "Epoch 4721/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7813 - val_loss: 1397.8291\n",
            "Epoch 4722/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7471 - val_loss: 1397.8807\n",
            "Epoch 4723/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.7689 - val_loss: 1397.9993\n",
            "Epoch 4724/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7539 - val_loss: 1397.9818\n",
            "Epoch 4725/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8074 - val_loss: 1397.9093\n",
            "Epoch 4726/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7681 - val_loss: 1397.7808\n",
            "Epoch 4727/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8027 - val_loss: 1398.0012\n",
            "Epoch 4728/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7468 - val_loss: 1397.9264\n",
            "Epoch 4729/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7463 - val_loss: 1397.9014\n",
            "Epoch 4730/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7698 - val_loss: 1398.0686\n",
            "Epoch 4731/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7902 - val_loss: 1398.2526\n",
            "Epoch 4732/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7458 - val_loss: 1398.1483\n",
            "Epoch 4733/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7813 - val_loss: 1398.1516\n",
            "Epoch 4734/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7536 - val_loss: 1398.3429\n",
            "Epoch 4735/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7522 - val_loss: 1398.2996\n",
            "Epoch 4736/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7308 - val_loss: 1398.4089\n",
            "Epoch 4737/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7424 - val_loss: 1398.5754\n",
            "Epoch 4738/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7539 - val_loss: 1398.5134\n",
            "Epoch 4739/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8277 - val_loss: 1398.8304\n",
            "Epoch 4740/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8087 - val_loss: 1398.6063\n",
            "Epoch 4741/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7312 - val_loss: 1398.7000\n",
            "Epoch 4742/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.7516 - val_loss: 1398.8541\n",
            "Epoch 4743/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8045 - val_loss: 1398.7135\n",
            "Epoch 4744/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.8029 - val_loss: 1399.0658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4745/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7535 - val_loss: 1399.0049\n",
            "Epoch 4746/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7325 - val_loss: 1399.0186\n",
            "Epoch 4747/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7432 - val_loss: 1399.1165\n",
            "Epoch 4748/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.8363 - val_loss: 1398.8593\n",
            "Epoch 4749/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7293 - val_loss: 1399.0144\n",
            "Epoch 4750/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8196 - val_loss: 1399.2756\n",
            "Epoch 4751/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7850 - val_loss: 1399.0358\n",
            "Epoch 4752/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7703 - val_loss: 1398.8096\n",
            "Epoch 4753/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.8267 - val_loss: 1398.6278\n",
            "Epoch 4754/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.7817 - val_loss: 1398.9532\n",
            "Epoch 4755/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.8545 - val_loss: 1399.2896\n",
            "Epoch 4756/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7804 - val_loss: 1399.3208\n",
            "Epoch 4757/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.7220 - val_loss: 1399.1489\n",
            "Epoch 4758/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7268 - val_loss: 1399.0344\n",
            "Epoch 4759/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7220 - val_loss: 1398.8002\n",
            "Epoch 4760/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7438 - val_loss: 1398.7349\n",
            "Epoch 4761/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7177 - val_loss: 1398.4153\n",
            "Epoch 4762/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7166 - val_loss: 1398.2749\n",
            "Epoch 4763/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.7508 - val_loss: 1397.8953\n",
            "Epoch 4764/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.9153 - val_loss: 1397.4972\n",
            "Epoch 4765/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7875 - val_loss: 1397.7084\n",
            "Epoch 4766/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7597 - val_loss: 1397.8202\n",
            "Epoch 4767/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7219 - val_loss: 1397.7367\n",
            "Epoch 4768/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7363 - val_loss: 1397.7839\n",
            "Epoch 4769/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7576 - val_loss: 1397.8440\n",
            "Epoch 4770/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7656 - val_loss: 1397.8315\n",
            "Epoch 4771/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8087 - val_loss: 1397.4526\n",
            "Epoch 4772/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7411 - val_loss: 1397.3601\n",
            "Epoch 4773/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7396 - val_loss: 1397.3721\n",
            "Epoch 4774/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7406 - val_loss: 1397.4471\n",
            "Epoch 4775/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7346 - val_loss: 1397.6646\n",
            "Epoch 4776/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7851 - val_loss: 1397.9333\n",
            "Epoch 4777/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7536 - val_loss: 1397.8953\n",
            "Epoch 4778/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7214 - val_loss: 1398.1335\n",
            "Epoch 4779/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7475 - val_loss: 1398.3531\n",
            "Epoch 4780/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7122 - val_loss: 1398.3384\n",
            "Epoch 4781/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7051 - val_loss: 1398.3986\n",
            "Epoch 4782/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7308 - val_loss: 1398.5563\n",
            "Epoch 4783/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7027 - val_loss: 1398.5597\n",
            "Epoch 4784/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7510 - val_loss: 1398.6543\n",
            "Epoch 4785/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7063 - val_loss: 1398.5594\n",
            "Epoch 4786/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7775 - val_loss: 1398.6963\n",
            "Epoch 4787/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7270 - val_loss: 1398.4271\n",
            "Epoch 4788/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7409 - val_loss: 1398.2401\n",
            "Epoch 4789/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7522 - val_loss: 1398.1172\n",
            "Epoch 4790/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.8077 - val_loss: 1398.0249\n",
            "Epoch 4791/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6943 - val_loss: 1398.3345\n",
            "Epoch 4792/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7378 - val_loss: 1398.7180\n",
            "Epoch 4793/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6963 - val_loss: 1398.8739\n",
            "Epoch 4794/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7544 - val_loss: 1399.3330\n",
            "Epoch 4795/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7251 - val_loss: 1399.4889\n",
            "Epoch 4796/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7303 - val_loss: 1399.4930\n",
            "Epoch 4797/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7469 - val_loss: 1399.4731\n",
            "Epoch 4798/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7074 - val_loss: 1399.6727\n",
            "Epoch 4799/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7101 - val_loss: 1399.7939\n",
            "Epoch 4800/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7255 - val_loss: 1399.8629\n",
            "Epoch 4801/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7757 - val_loss: 1400.0703\n",
            "Epoch 4802/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7553 - val_loss: 1399.8763\n",
            "Epoch 4803/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7224 - val_loss: 1399.9191\n",
            "Epoch 4804/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7224 - val_loss: 1399.7175\n",
            "Epoch 4805/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7708 - val_loss: 1399.4377\n",
            "Epoch 4806/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7365 - val_loss: 1399.4271\n",
            "Epoch 4807/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.7157 - val_loss: 1399.3442\n",
            "Epoch 4808/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7695 - val_loss: 1399.1523\n",
            "Epoch 4809/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7092 - val_loss: 1399.2577\n",
            "Epoch 4810/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7632 - val_loss: 1399.0269\n",
            "Epoch 4811/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6960 - val_loss: 1398.9773\n",
            "Epoch 4812/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7763 - val_loss: 1399.1975\n",
            "Epoch 4813/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6965 - val_loss: 1398.9634\n",
            "Epoch 4814/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7071 - val_loss: 1398.8080\n",
            "Epoch 4815/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6908 - val_loss: 1398.7767\n",
            "Epoch 4816/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.7682 - val_loss: 1398.9983\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4817/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6983 - val_loss: 1398.7729\n",
            "Epoch 4818/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7176 - val_loss: 1398.5981\n",
            "Epoch 4819/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6955 - val_loss: 1398.7203\n",
            "Epoch 4820/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6919 - val_loss: 1398.7136\n",
            "Epoch 4821/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7331 - val_loss: 1398.5765\n",
            "Epoch 4822/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7053 - val_loss: 1398.8219\n",
            "Epoch 4823/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7628 - val_loss: 1398.6970\n",
            "Epoch 4824/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6886 - val_loss: 1398.8174\n",
            "Epoch 4825/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7139 - val_loss: 1398.8398\n",
            "Epoch 4826/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7509 - val_loss: 1399.1489\n",
            "Epoch 4827/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6938 - val_loss: 1399.0247\n",
            "Epoch 4828/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7063 - val_loss: 1399.1213\n",
            "Epoch 4829/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7643 - val_loss: 1398.9613\n",
            "Epoch 4830/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7260 - val_loss: 1399.2225\n",
            "Epoch 4831/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6868 - val_loss: 1399.2751\n",
            "Epoch 4832/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7725 - val_loss: 1398.9775\n",
            "Epoch 4833/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6952 - val_loss: 1398.9886\n",
            "Epoch 4834/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.7387 - val_loss: 1399.3228\n",
            "Epoch 4835/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.6803 - val_loss: 1399.3503\n",
            "Epoch 4836/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6899 - val_loss: 1399.3677\n",
            "Epoch 4837/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6879 - val_loss: 1399.2427\n",
            "Epoch 4838/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6714 - val_loss: 1399.2572\n",
            "Epoch 4839/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7300 - val_loss: 1399.1699\n",
            "Epoch 4840/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7168 - val_loss: 1399.4163\n",
            "Epoch 4841/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6910 - val_loss: 1399.4801\n",
            "Epoch 4842/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7804 - val_loss: 1399.1814\n",
            "Epoch 4843/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7089 - val_loss: 1399.2087\n",
            "Epoch 4844/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6920 - val_loss: 1399.1296\n",
            "Epoch 4845/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7037 - val_loss: 1399.3419\n",
            "Epoch 4846/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.6696 - val_loss: 1399.2772\n",
            "Epoch 4847/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.6957 - val_loss: 1399.3885\n",
            "Epoch 4848/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7605 - val_loss: 1399.0898\n",
            "Epoch 4849/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6635 - val_loss: 1399.1255\n",
            "Epoch 4850/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6718 - val_loss: 1399.1969\n",
            "Epoch 4851/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7195 - val_loss: 1399.1343\n",
            "Epoch 4852/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6833 - val_loss: 1399.2710\n",
            "Epoch 4853/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7938 - val_loss: 1398.9689\n",
            "Epoch 4854/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6682 - val_loss: 1398.9680\n",
            "Epoch 4855/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.6618 - val_loss: 1399.0162\n",
            "Epoch 4856/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6835 - val_loss: 1399.2982\n",
            "Epoch 4857/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1069.7360 - val_loss: 1399.5062\n",
            "Epoch 4858/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6716 - val_loss: 1399.4757\n",
            "Epoch 4859/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.7466 - val_loss: 1399.0671\n",
            "Epoch 4860/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6976 - val_loss: 1398.8541\n",
            "Epoch 4861/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6544 - val_loss: 1398.8568\n",
            "Epoch 4862/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.7039 - val_loss: 1398.9447\n",
            "Epoch 4863/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6596 - val_loss: 1398.8242\n",
            "Epoch 4864/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6656 - val_loss: 1398.7476\n",
            "Epoch 4865/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6567 - val_loss: 1398.7821\n",
            "Epoch 4866/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6607 - val_loss: 1398.9243\n",
            "Epoch 4867/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7132 - val_loss: 1398.8870\n",
            "Epoch 4868/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7217 - val_loss: 1398.6726\n",
            "Epoch 4869/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.8375 - val_loss: 1399.0178\n",
            "Epoch 4870/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6565 - val_loss: 1398.9480\n",
            "Epoch 4871/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7318 - val_loss: 1398.5511\n",
            "Epoch 4872/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7744 - val_loss: 1398.2605\n",
            "Epoch 4873/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6636 - val_loss: 1398.4562\n",
            "Epoch 4874/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7181 - val_loss: 1398.7413\n",
            "Epoch 4875/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.6679 - val_loss: 1398.6141\n",
            "Epoch 4876/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6725 - val_loss: 1398.7119\n",
            "Epoch 4877/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6713 - val_loss: 1398.7622\n",
            "Epoch 4878/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6911 - val_loss: 1399.0013\n",
            "Epoch 4879/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.7025 - val_loss: 1398.9669\n",
            "Epoch 4880/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6713 - val_loss: 1399.3093\n",
            "Epoch 4881/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6443 - val_loss: 1399.4587\n",
            "Epoch 4882/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6604 - val_loss: 1399.5358\n",
            "Epoch 4883/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7363 - val_loss: 1399.3809\n",
            "Epoch 4884/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.6592 - val_loss: 1399.5416\n",
            "Epoch 4885/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.6391 - val_loss: 1399.8582\n",
            "Epoch 4886/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7480 - val_loss: 1400.1765\n",
            "Epoch 4887/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6618 - val_loss: 1400.3141\n",
            "Epoch 4888/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6844 - val_loss: 1400.4227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4889/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.8262 - val_loss: 1400.1171\n",
            "Epoch 4890/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.6464 - val_loss: 1400.2003\n",
            "Epoch 4891/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6838 - val_loss: 1400.3851\n",
            "Epoch 4892/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.7592 - val_loss: 1400.0952\n",
            "Epoch 4893/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6663 - val_loss: 1400.0582\n",
            "Epoch 4894/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6429 - val_loss: 1400.1201\n",
            "Epoch 4895/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.6716 - val_loss: 1400.2427\n",
            "Epoch 4896/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6822 - val_loss: 1400.2196\n",
            "Epoch 4897/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6595 - val_loss: 1400.0273\n",
            "Epoch 4898/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6664 - val_loss: 1399.8619\n",
            "Epoch 4899/5000\n",
            "353/353 [==============================] - ETA: 0s - loss: 904.976 - 0s 33us/sample - loss: 1069.6468 - val_loss: 1399.8982\n",
            "Epoch 4900/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6528 - val_loss: 1399.8970\n",
            "Epoch 4901/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6365 - val_loss: 1399.8276\n",
            "Epoch 4902/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.6276 - val_loss: 1399.6620\n",
            "Epoch 4903/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6871 - val_loss: 1399.6996\n",
            "Epoch 4904/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6392 - val_loss: 1399.4741\n",
            "Epoch 4905/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6575 - val_loss: 1399.2301\n",
            "Epoch 4906/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6249 - val_loss: 1399.1473\n",
            "Epoch 4907/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6400 - val_loss: 1399.1918\n",
            "Epoch 4908/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6290 - val_loss: 1399.0281\n",
            "Epoch 4909/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6237 - val_loss: 1399.0210\n",
            "Epoch 4910/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6997 - val_loss: 1398.7827\n",
            "Epoch 4911/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6384 - val_loss: 1398.8555\n",
            "Epoch 4912/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6493 - val_loss: 1399.1442\n",
            "Epoch 4913/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.7369 - val_loss: 1399.0642\n",
            "Epoch 4914/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.7471 - val_loss: 1399.5529\n",
            "Epoch 4915/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6332 - val_loss: 1399.5311\n",
            "Epoch 4916/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6237 - val_loss: 1399.7152\n",
            "Epoch 4917/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6941 - val_loss: 1399.5782\n",
            "Epoch 4918/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6135 - val_loss: 1399.7981\n",
            "Epoch 4919/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6201 - val_loss: 1399.9540\n",
            "Epoch 4920/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6791 - val_loss: 1400.1676\n",
            "Epoch 4921/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6533 - val_loss: 1399.9564\n",
            "Epoch 4922/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.6239 - val_loss: 1399.8608\n",
            "Epoch 4923/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6537 - val_loss: 1399.8832\n",
            "Epoch 4924/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6797 - val_loss: 1399.6842\n",
            "Epoch 4925/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6286 - val_loss: 1399.3162\n",
            "Epoch 4926/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6695 - val_loss: 1399.0071\n",
            "Epoch 4927/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6620 - val_loss: 1399.0889\n",
            "Epoch 4928/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6921 - val_loss: 1398.8062\n",
            "Epoch 4929/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6881 - val_loss: 1398.6716\n",
            "Epoch 4930/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6434 - val_loss: 1398.7360\n",
            "Epoch 4931/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6245 - val_loss: 1399.0942\n",
            "Epoch 4932/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6252 - val_loss: 1399.3622\n",
            "Epoch 4933/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6629 - val_loss: 1399.5952\n",
            "Epoch 4934/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6130 - val_loss: 1399.6251\n",
            "Epoch 4935/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6345 - val_loss: 1399.6587\n",
            "Epoch 4936/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6766 - val_loss: 1399.3738\n",
            "Epoch 4937/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6322 - val_loss: 1399.1710\n",
            "Epoch 4938/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6125 - val_loss: 1399.1927\n",
            "Epoch 4939/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6248 - val_loss: 1399.2651\n",
            "Epoch 4940/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6308 - val_loss: 1399.2192\n",
            "Epoch 4941/5000\n",
            "353/353 [==============================] - 0s 36us/sample - loss: 1069.6196 - val_loss: 1399.2264\n",
            "Epoch 4942/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6295 - val_loss: 1399.1757\n",
            "Epoch 4943/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6799 - val_loss: 1398.9146\n",
            "Epoch 4944/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.6034 - val_loss: 1399.0411\n",
            "Epoch 4945/5000\n",
            "353/353 [==============================] - 0s 35us/sample - loss: 1069.6824 - val_loss: 1399.1138\n",
            "Epoch 4946/5000\n",
            "353/353 [==============================] - 0s 40us/sample - loss: 1069.6030 - val_loss: 1399.1156\n",
            "Epoch 4947/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6308 - val_loss: 1399.1150\n",
            "Epoch 4948/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.6086 - val_loss: 1398.8477\n",
            "Epoch 4949/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6716 - val_loss: 1398.4993\n",
            "Epoch 4950/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6018 - val_loss: 1398.4694\n",
            "Epoch 4951/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6092 - val_loss: 1398.5060\n",
            "Epoch 4952/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6144 - val_loss: 1398.5803\n",
            "Epoch 4953/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6205 - val_loss: 1398.4861\n",
            "Epoch 4954/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6108 - val_loss: 1398.6422\n",
            "Epoch 4955/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6123 - val_loss: 1398.7035\n",
            "Epoch 4956/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.6053 - val_loss: 1398.7404\n",
            "Epoch 4957/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6101 - val_loss: 1399.0131\n",
            "Epoch 4958/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.5924 - val_loss: 1399.1298\n",
            "Epoch 4959/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.5989 - val_loss: 1399.1974\n",
            "Epoch 4960/5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.5977 - val_loss: 1399.2773\n",
            "Epoch 4961/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.5975 - val_loss: 1399.3320\n",
            "Epoch 4962/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.6099 - val_loss: 1399.6368\n",
            "Epoch 4963/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6125 - val_loss: 1399.8303\n",
            "Epoch 4964/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.6293 - val_loss: 1399.9167\n",
            "Epoch 4965/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6001 - val_loss: 1399.7424\n",
            "Epoch 4966/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.5854 - val_loss: 1399.6628\n",
            "Epoch 4967/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.7183 - val_loss: 1399.3491\n",
            "Epoch 4968/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.5953 - val_loss: 1399.4417\n",
            "Epoch 4969/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.6035 - val_loss: 1399.6086\n",
            "Epoch 4970/5000\n",
            "353/353 [==============================] - 0s 26us/sample - loss: 1069.6563 - val_loss: 1399.7731\n",
            "Epoch 4971/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6512 - val_loss: 1399.7986\n",
            "Epoch 4972/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.5810 - val_loss: 1399.4727\n",
            "Epoch 4973/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.5833 - val_loss: 1399.2365\n",
            "Epoch 4974/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.5786 - val_loss: 1399.1228\n",
            "Epoch 4975/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6833 - val_loss: 1398.8212\n",
            "Epoch 4976/5000\n",
            "353/353 [==============================] - 0s 27us/sample - loss: 1069.6934 - val_loss: 1399.1224\n",
            "Epoch 4977/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.5999 - val_loss: 1398.9589\n",
            "Epoch 4978/5000\n",
            "353/353 [==============================] - 0s 37us/sample - loss: 1069.5845 - val_loss: 1398.9961\n",
            "Epoch 4979/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.5861 - val_loss: 1398.9397\n",
            "Epoch 4980/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6241 - val_loss: 1398.7495\n",
            "Epoch 4981/5000\n",
            "353/353 [==============================] - 0s 34us/sample - loss: 1069.5846 - val_loss: 1398.7764\n",
            "Epoch 4982/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.5966 - val_loss: 1398.7994\n",
            "Epoch 4983/5000\n",
            "353/353 [==============================] - 0s 25us/sample - loss: 1069.5848 - val_loss: 1399.0574\n",
            "Epoch 4984/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.5912 - val_loss: 1399.3156\n",
            "Epoch 4985/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.6957 - val_loss: 1399.6884\n",
            "Epoch 4986/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.5759 - val_loss: 1399.6836\n",
            "Epoch 4987/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.5750 - val_loss: 1399.6495\n",
            "Epoch 4988/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6445 - val_loss: 1399.4458\n",
            "Epoch 4989/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.5947 - val_loss: 1399.4441\n",
            "Epoch 4990/5000\n",
            "353/353 [==============================] - 0s 28us/sample - loss: 1069.5788 - val_loss: 1399.6798\n",
            "Epoch 4991/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.6129 - val_loss: 1399.8962\n",
            "Epoch 4992/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.5868 - val_loss: 1399.9329\n",
            "Epoch 4993/5000\n",
            "353/353 [==============================] - 0s 38us/sample - loss: 1069.5827 - val_loss: 1399.8243\n",
            "Epoch 4994/5000\n",
            "353/353 [==============================] - 0s 32us/sample - loss: 1069.6782 - val_loss: 1399.6249\n",
            "Epoch 4995/5000\n",
            "353/353 [==============================] - 0s 33us/sample - loss: 1069.5809 - val_loss: 1399.5641\n",
            "Epoch 4996/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.5749 - val_loss: 1399.4608\n",
            "Epoch 4997/5000\n",
            "353/353 [==============================] - 0s 29us/sample - loss: 1069.6613 - val_loss: 1399.2181\n",
            "Epoch 4998/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.5662 - val_loss: 1399.3759\n",
            "Epoch 4999/5000\n",
            "353/353 [==============================] - 0s 30us/sample - loss: 1069.5696 - val_loss: 1399.4974\n",
            "Epoch 5000/5000\n",
            "353/353 [==============================] - 0s 31us/sample - loss: 1069.5933 - val_loss: 1399.5815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1c76b845c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV2s_mjYux8i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjTG_GYtux8i",
        "outputId": "f59e63dd-c7f4-4c5f-c755-a8afb3bd44d9"
      },
      "source": [
        "#For tuning Hyper-Parameters\n",
        "losses = pd.DataFrame(model.history.history)\n",
        "losses.plot()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c207c68d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjHuPvG4ux8j"
      },
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcDECdQUux8j",
        "outputId": "415a8f39-736a-45a8-a795-adcf95b87b2f"
      },
      "source": [
        "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
        "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
        "print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE: 27.85627894516444\n",
            "MSE: 2012.4765855590167\n",
            "RMSE: 44.86063514440045\n",
            "Explained Variance Score: 0.1106908761684372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kD_5ufIux8j",
        "outputId": "4e66e377-4b09-4504-d9c2-493530a7f202"
      },
      "source": [
        "plt.scatter(y_test,predictions)\n",
        "plt.plot(y_test, y_test, 'r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f1c204ba5f8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cWtf6zSux8k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "ec85a2fd-05cf-457e-f67f-23a22f7ede15"
      },
      "source": [
        "sns.distplot((y_test-predictions),bins=50);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5f2bc21c0ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jIGO0gF2BmX"
      },
      "source": [
        "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}